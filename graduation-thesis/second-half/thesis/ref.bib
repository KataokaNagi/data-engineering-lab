@comment{
  file     ref.bib
  brief    bibliography references
  author   Kataoka Nagi al18036[at]shibaura-it.ac.jp
  date     2021-12-21 22:09:37
  Version: 1.0
  par      History
            add
  Copyright (c) 2021 Kataoka Nagi
}


@article{karimi_news_2018,
  title    = {News recommender systems – {Survey} and roads ahead},
  volume   = {54},
  issn     = {0306-4573},
  url      = {https://www.sciencedirect.com/science/article/pii/S030645731730153X},
  doi      = {10.1016/j.ipm.2018.04.008},
  abstract = {More and more people read the news online, e.g., by visiting the websites of their favorite newspapers or by navigating the sites of news aggregators. However, the abundance of news information that is published online every day through different channels can make it challenging for readers to locate the content they are interested in. The goal of News Recommender Systems (NRS) is to make reading suggestions to users in a personalized way. Due to their practical relevance, a variety of technical approaches to build such systems have been proposed over the last two decades. In this work, we review the state-of-the-art of designing and evaluating news recommender systems over the last ten years. One main goal of the work is to analyze which particular challenges of news recommendation (e.g., short item life times and recency aspects) have been well explored and which areas still require more work. Furthermore, in contrast to previous surveys, the paper specifically discusses methodological questions and today’s academic practice of evaluating and comparing different algorithmic news recommendation approaches based on accuracy measures.},
  language = {en},
  number   = {6},
  urldate  = {2021-06-25},
  journal  = {Information Processing \& Management},
  author   = {Karimi, Mozhgan and Jannach, Dietmar and Jugovac, Michael},
  month    = nov,
  year     = {2018},
  pages    = {1203--1227},
  file     = {Karimi et al_2018_News recommender systems – Survey and roads ahead.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Karimi et al_2018_News recommender systems – Survey and roads ahead.pdf:application/pdf}
}

@misc{pariser_beware_nodate,
  title    = {Beware online "filter bubbles"},
  url      = {https://www.ted.com/talks/eli_pariser_beware_online_filter_bubbles},
  abstract = {As web companies strive to tailor their services (including news and search results) to our personal tastes, there's a dangerous unintended consequence: We get trapped in a "filter bubble" and don't get exposed to information that could challenge or broaden our worldview. Eli Pariser argues powerfully that this will ultimately prove to be bad for us and bad for democracy.},
  language = {en},
  urldate  = {2022-01-06},
  author   = {Pariser, Eli}
}

@article{bruns_filter_2019,
  title    = {Filter bubble},
  volume   = {8},
  issn     = {2197-6775},
  url      = {https://policyreview.info/concepts/filter-bubble},
  abstract = {Concepts such as ‘filter bubble’ enjoy considerable popularity in scholarly as well as mainstream debates, but are rarely defined with any rigour. This has led to highly contradictory research findings. This article provides a critical review of the ‘filter bubble’ idea, and concludes that its persistence has served only to distract scholarly attention from far more critical areas of enquiry.},
  number   = {4},
  urldate  = {2021-05-29},
  journal  = {Internet Policy Review},
  author   = {Bruns, Axel},
  month    = nov,
  year     = {2019},
  file     = {Bruns_2019_Filter bubble.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Bruns_2019_Filter bubble.pdf:application/pdf}
}

@article{nguyen_echo_2020,
  title    = {{ECHO} {CHAMBERS} {AND} {EPISTEMIC} {BUBBLES}},
  volume   = {17},
  issn     = {1742-3600, 1750-0117},
  url      = {https://www.cambridge.org/core/journals/episteme/article/abs/echo-chambers-and-epistemic-bubbles/5D4AC3A808C538E17C50A7C09EC706F0},
  doi      = {10.1017/epi.2018.32},
  abstract = {Discussion of the phenomena of post-truth and fake news often implicates the closed epistemic networks of social media. The recent conversation has, however, blurred two distinct social epistemic phenomena. An epistemic bubble is a social epistemic structure in which other relevant voices have been left out, perhaps accidentally. An echo chamber is a social epistemic structure from which other relevant voices have been actively excluded and discredited. Members of epistemic bubbles lack exposure to relevant information and arguments. Members of echo chambers, on the other hand, have been brought to systematically distrust all outside sources. In epistemic bubbles, other voices are not heard; in echo chambers, other voices are actively undermined. It is crucial to keep these phenomena distinct. First, echo chambers can explain the post-truth phenomena in a way that epistemic bubbles cannot. Second, each type of structure requires a distinct intervention. Mere exposure to evidence can shatter an epistemic bubble, but may actually reinforce an echo chamber. Finally, echo chambers are much harder to escape. Once in their grip, an agent may act with epistemic virtue, but social context will pervert those actions. Escape from an echo chamber may require a radical rebooting of one's belief system.},
  language = {en},
  number   = {2},
  urldate  = {2022-01-06},
  journal  = {Episteme},
  author   = {Nguyen, C. Thi},
  month    = jun,
  year     = {2020},
  note     = {Publisher: Cambridge University Press},
  pages    = {141--161}
}

@misc{inc_echo_nodate,
  title   = {エコー‐チェンバー【echo chamber】},
  url     = {https://japanknowledge.com/lib/display/?lid=2001028535800},
  urldate = {2022-01-06},
  journal = {JapanKnowledge},
  author  = {Inc, NetAdvance}
}

@article{__2020-5,
  title    = {ウェブの功罪},
  volume   = {70},
  doi      = {10.18919/jkg.70.6_309},
  abstract = {多様な情報と人々をつなぐはずのソーシャルメディアが，社会的分断や情報のタコツボ化を助長しているという問題が顕在化している。特に，エコーチェンバーやフィルターバブルといった閉じた情報環境は，自分の好みに合致した情報のみが来やすく，自分とは異なる観点からの情報が来にくいため，フェイク（偽）ニュースやヘイト（憎悪）の温床となる危険性を孕んでいる。本稿では，計算社会科学の観点から，偽ニュースが拡散する仕組みについて解説し，ウェブの負の側面について論じる。また，今後のウェブの技術が克服すべき問題について議論する。},
  number   = {6},
  journal  = {情報の科学と技術},
  author   = {和俊, 笹原},
  year     = {2020},
  pages    = {309--314},
  file     = {和俊_2020_ウェブの功罪.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\和俊_2020_ウェブの功罪.pdf:application/pdf}
}


@article{conover_partisan_2012,
  title     = {Partisan asymmetries in online political activity},
  volume    = {1},
  copyright = {2012 Conover et al.; licensee Springer.},
  issn      = {2193-1127},
  url       = {https://epjdatascience.springeropen.com/articles/10.1140/epjds6},
  doi       = {10.1140/epjds6},
  abstract  = {We examine partisan differences in the behavior, communication patterns and social interactions of more than 18,000 politically-active Twitter users to produce evidence that points to changing levels of partisan engagement with the American online political landscape. Analysis of a network defined by the communication activity of these users in proximity to the 2010 midterm congressional elections reveals a highly segregated, well clustered, partisan community structure. Using cluster membership as a high-fidelity (87\% accuracy) proxy for political affiliation, we characterize a wide range of differences in the behavior, communication and social connectivity of left- and right-leaning Twitter users. We find that in contrast to the online political dynamics of the 2008 campaign, right-leaning Twitter users exhibit greater levels of political activity, a more tightly interconnected social structure, and a communication network topology that facilitates the rapid and broad dissemination of political information.},
  language  = {en},
  number    = {1},
  urldate   = {2022-01-06},
  journal   = {EPJ Data Science},
  author    = {Conover, Michael D. and Gonçalves, Bruno and Flammini, Alessandro and Menczer, Filippo},
  month     = dec,
  year      = {2012},
  note      = {Number: 1 Publisher: SpringerOpen},
  pages     = {1--19},
  file      = {Conover et al_2012_Partisan asymmetries in online political activity.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Conover et al_2012_Partisan asymmetries in online political activity.pdf:application/pdf}
}

@inproceedings{nagulendra_understanding_2014,
  address    = {New York, NY, USA},
  series     = {{HT} '14},
  title      = {Understanding and controlling the filter bubble through interactive visualization: a user study},
  isbn       = {978-1-4503-2954-5},
  shorttitle = {Understanding and controlling the filter bubble through interactive visualization},
  url        = {https://doi.org/10.1145/2631775.2631811},
  doi        = {10.1145/2631775.2631811},
  abstract   = {The "filter bubble" is a term which refers to people getting encapsulated in streams of data such as news or social network updates that are personalized to their interests. While people need protection from information overload and maybe prefer to see content they feel familiar or agree with, there is the danger that important issues that should be of concern for everyone will get filtered away and people will lack exposure to different views, living in "echo-chambers", blissfully unaware of the reality. We have proposed a design of an interactive visualization, which provides the user of a social networking site with awareness of the personalization mechanism (the semantics and the source of the content that is filtered away), and with means to control the filtering mechanism. The visualization has been implemented in a peer-to-peer social network, called MADMICA, and we present here the results of a large scale lab study with 163 crowd-sourced participants. The results demonstrate that the visualization leads to increased users' awareness of the filter bubble, understandability of the filtering mechanism and to a feeling of control over their data stream.},
  urldate    = {2021-05-30},
  booktitle  = {Proceedings of the 25th {ACM} conference on {Hypertext} and social media},
  publisher  = {Association for Computing Machinery},
  author     = {Nagulendra, Sayooran and Vassileva, Julita},
  month      = sep,
  year       = {2014},
  pages      = {107--115},
  file       = {Full Text PDF:C\:\\Users\\calm3\\Zotero\\storage\\R6RPIXHN\\Nagulendra and Vassileva - 2014 - Understanding and controlling the filter bubble th.pdf:application/pdf}
}

@book{aurellen20,
  author    = {Aurélien Géron, 下田倫大, 長尾高弘},
  title     = {scikit-learn、Keras、TensorFlowによる実践機械学習 第2版},
  publisher = {株式会社オライリー・ジャパン},
  year      = {2020}
}

@article{bahdanau_neural_2016,
  title    = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
  url      = {http://arxiv.org/abs/1409.0473},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  urldate  = {2022-01-07},
  journal  = {arXiv:1409.0473 [cs, stat]},
  author   = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  month    = may,
  year     = {2016},
  note     = {arXiv: 1409.0473},
  file     = {Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf}
}

@article{vaswani_attention_nodate,
  title    = {Attention is {All} you {Need}},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  language = {en},
  author   = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  pages    = {11},
  file     = {Vaswani et al. - Attention is All you Need.pdf:C\:\\Users\\calm3\\Zotero\\storage\\I8GE8GRW\\Vaswani et al. - Attention is All you Need.pdf:application/pdf}
}

@article{devlin_bert_2019,
  title      = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  shorttitle = {{BERT}},
  url        = {http://arxiv.org/abs/1810.04805},
  abstract   = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  urldate    = {2021-07-23},
  journal    = {arXiv:1810.04805 [cs]},
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month      = may,
  year       = {2019},
  note       = {arXiv: 1810.04805},
  file       = {Devlin et al_2019_BERT.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Devlin et al_2019_BERT.pdf:application/pdf}
}

@article{liu_roberta_2019,
  title      = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
  shorttitle = {{RoBERTa}},
  url        = {http://arxiv.org/abs/1907.11692},
  abstract   = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  urldate    = {2021-07-25},
  journal    = {arXiv:1907.11692 [cs]},
  author     = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  month      = jul,
  year       = {2019},
  note       = {arXiv: 1907.11692},
  file       = {Liu et al_2019_RoBERTa.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Liu et al_2019_RoBERTa.pdf:application/pdf}
}

%% professor's template
@comment{
  @book{Kinoshita81,
    author    = "木下 是雄",
    title     = "理科系の作文技術",
    publisher = "中央公論社",
    year      = "1981",
    series    = "中公新書",
    number    = "624",
    URL       = "https://ci.nii.ac.jp/ncid/BN00624169"
  }


  @book{Yuki13,
    author    = "結城 浩",
    title     = "数学文章作法 基礎編",
    publisher = "ちくま学芸文庫",
    year      = "2013"
  }



  @article{Ijiri18,
    title = {Digitization of natural objects with micro CT and photographs},
    author = {Takashi Ijiri and Hideki Todo and Akira Hirabayashi and Kenji Kohiyama and Yoshinori Dobashi},
    journal = {PLoS ONE},
    month = {4},
    number = {4},
    volume = {13},
    year = {2018}
  }


  @misc{TexLive,
          author  = {TeX Users Group},
          title   = {TeX Live. \url{https://www.tug.org/texlive/}},
          note    = {(2021年5月28日参照)}
  }
}
