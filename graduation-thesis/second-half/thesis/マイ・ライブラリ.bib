
@article{barredo_arrieta_explainable_2020,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {15662535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	language = {en},
	urldate = {2021-04-10},
	journal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = jun,
	year = {2020},
	pages = {82--115},
	file = {Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:C\:\\Users\\calm3\\Zotero\\storage\\TJVVLASE\\Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf},
}

@article{_ai_2017,
	title = {{機械学習を用いたゲームAIの人間らしい行動に関する研究}},
	volume = {2017},
	url = {https://ci.nii.ac.jp/naid/170000174753},
	number = {1},
	urldate = {2021-04-10},
	journal = {第79回全国大会講演論文集},
	author = {三浦, 大輔 and 會澤, 邦夫},
	month = mar,
	year = {2017},
	pages = {473--474},
}

@article{_ai_2017-1,
	title = {{機械学習を用いたゲームAIの人間らしい行動に関する研究}},
	volume = {2017},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=180998&item_no=1},
	abstract = {情報学広場 情報処理学会電子図書館},
	language = {ja},
	number = {1},
	urldate = {2021-04-10},
	journal = {第79回全国大会講演論文集},
	author = {大輔, 三浦 and 邦夫, 會澤},
	month = mar,
	year = {2017},
	pages = {473--474},
	file = {大輔_邦夫_2017_機械学習を用いたゲームAIの人間らしい行動に関する研究.pdf:C\:\\Users\\calm3\\Zotero\\storage\\A5NWV3BH\\大輔_邦夫_2017_機械学習を用いたゲームAIの人間らしい行動に関する研究.pdf:application/pdf},
}

@article{__2019,
	title = {物理屋のための深層学習},
	volume = {74},
	doi = {10.11316/butsuri.74.11_759},
	abstract = {このところ深層学習（ディープラーニング）という言葉を頻繁に耳にするようになってきました．巷では技術的特異点などのパワーワードに引っ張られ，人工知能が人間の仕事を奪うかもしれない等と報道されるケースも少なくないため，怪しい分野だと思われている方もいるかもしれませんが，そうではありません．機械学習の理論的下地は確率・統計学にあり，むしろ物理学を修めた方なら誰でも腑に落ちるような枠組みに支えられています．深層学習は，数多ある機械学習の手法群の中の一つの手法です．その一方，親玉である機械学習という分野は，データをもとにそこからパターン・知識を抽出する手法一般の研究開発を指します．物理学者が普段行っているデータ解析のうち，ある程度の割合は機械学習だといっても過言ではないでしょう．深層学習は，ニューラルネットワーク（ニューラルネット）と呼ばれる特殊な数理モデルを用いる点で，他の機械学習の手法と大きく異なります．ニューラルネットは20世紀の中頃，動物の脳の数理モデルとして提唱されましたが，その後はデータの学習のための機械学習モデルとして広く研究されるようになりました．長い研究の歴史を持つニューラルネットですが，2006年頃になり新しい段階に突入し，やがて加速的な発展期を迎えます．ネットワーク構造を深層化することでニューラルネットが極めて高い学習能力を発揮することが実証され，広範なタスクに対応できるネットワーク構造が次々と開発され始めたのです．この一連の流れで発見されたアルゴリズム・技術・ノウハウの総体が深層学習だ，といって良いでしょう．深層学習は画像認識にとどまらず，Google翻訳のような自然言語処理，音声の変換・生成，フェイク動画の生成，アルファ碁に代表されるゲームAIなど，急激にその応用範囲を拡大し，産業・科学技術の風景を変えつつあるのは皆さんもご存知の通りです．では，この間の研究によって全ては理解され尽くされ，応用も試み尽くされたのでしょうか?　実態はそうではなく，むしろその逆です．ニューラルネットが高い性能を実現する理論的なメカニズムは未だほとんど理解されておらず，そのためアドホックなネットワーク構造の設計も当然第一原理に基づくものではありません．そしてゲームAIのような探索的作業への大きな可能性があるにもかかわらず，深層学習の基礎科学への導入は，まだ部分的かつ初歩的な段階にあります．深層学習の高い表現能力や汎化性能の理論的理解や，データサイズに比べてデータ次元が極めて高いような場合に対応できる学習アルゴリズムの発見など，今後物理学者も寄与できる未解決問題も数多くあると考えられます．またこれまでの産業・ソーシャルデジタルデータだけではなく，科学データへの応用を通じて露わになる深層学習の技術的問題点や改良の可能性も数多くあるでしょう．これからは，基礎科学研究によって深層学習の新しい可能性が開けていくものと期待しています．},
	number = {11},
	journal = {日本物理学会誌},
	author = {雅人, 瀧 and 章詞, 田中},
	year = {2019},
	pages = {759--764},
	file = {雅人_章詞_2019_物理屋のための深層学習.pdf:C\:\\Users\\calm3\\Zotero\\storage\\69WYFRXC\\雅人_章詞_2019_物理屋のための深層学習.pdf:application/pdf},
}

@misc{noauthor_jaist_nodate,
	title = {{JAIST} {Repository}: コンピューターゲームプレイヤにおける人間らしさの調査},
	url = {https://dspace.jaist.ac.jp/dspace/handle/10119/16084},
	urldate = {2021-04-10},
}

@misc{noauthor_jaist_nodate-1,
	title = {{JAIST} {Repository}: コンピューターゲームプレイヤにおける人間らしさの調査},
	url = {https://dspace.jaist.ac.jp/dspace/handle/10119/16084},
	urldate = {2021-04-10},
}

@article{_bgm_2020,
	title = {ゲームシナリオと感情状態に合わせてbgmを選曲するシステム},
	volume = {JSAI2020},
	doi = {10.11517/pjsai.JSAI2020.0_4C2GS1304},
	abstract = {本論文では，与えられたゲームシーンに対し自動的にBGMを選曲するシステムについて述べる．本システムは，RPG自動生成システムの一部として機能することが想定されており，自動生成されたゲームシナリオが入力されると，各ゲームシーンの感情状態に合致したBGMをデータベースから選択する．まず，既存RPGにおける各シーンにユーザが感情状態をタグ付けして訓練データを作成する．各シーンに付与されているBGMの音響特徴量を算出し，ニューラルネットワークを用いて，その音響特徴量と各シーンの感情状態の対応関係を学習する．シーンの感情状態を表す指標にはHevnerが定義した音楽心理カテゴリを用いた．予めBGM候補となる楽曲群データベースを用意し，それらの音響特徴量を算出しておく．ゲームシナリオで指定されたシーンの感情状態がニューラルネットワークに入力されると，対応する音響特徴量が得られ，音響特徴量の類似した楽曲がBGM候補群の中から選択される．現在，RPG自動生成システムに組み込んで全体動作を確認しており，被験者実験の準備を進めている．},
	journal = {人工知能学会全国大会論文集},
	author = {拓真, 山内 and さくら, 根本 and 恭介, 長野 and 祥吾, 中村 and 勇璃, 斉藤 and 朗子, 宇田 and 源, 村井 and 和司, 迎山 and 恵美子, 田柳 and 圭二, 平田},
	year = {2020},
	pages = {4C2GS1304--4C2GS1304},
	file = {拓真 et al_2020_ゲームシナリオと感情状態に合わせてbgmを選曲するシステム.pdf:C\:\\Users\\calm3\\Zotero\\storage\\FGK6ELJ6\\拓真 et al_2020_ゲームシナリオと感情状態に合わせてbgmを選曲するシステム.pdf:application/pdf},
}

@article{__2020,
	title = {ゲーム会社に学ぶ 高速リリースの勘所},
	issn = {0285-4619},
	url = {https://ci.nii.ac.jp/naid/40022216605},
	number = {1015},
	urldate = {2021-04-10},
	journal = {日経コンピュータ = Nikkei computer},
	author = {安藤, 正芳},
	month = apr,
	year = {2020},
	note = {Publisher: 日経BP},
	pages = {44--47},
}

@article{__2018,
	title = {デジタルゲームにおける可視化技術},
	volume = {38},
	doi = {10.3154/jvs.38.151_28},
	abstract = {デジタルゲームのサイズと複雑度はますます大きくなり、ゲーム開発者も自分の作るゲームの全容を把握できない状況になりつつある。しかし、人工知能は、データを収集し、解析し、重要な情報を抽出し、可視化することでそのような状況を打開できる力を持つ。本記事では、FINAL FANTASY XVの実例を通じて、AIがどのようにゲームデータを可視化し開発者をサポートするかを説明する。},
	number = {151},
	journal = {可視化情報学会誌},
	author = {陽一郎, 三宅},
	year = {2018},
	pages = {28--33},
	file = {陽一郎_2018_デジタルゲームにおける可視化技術.pdf:C\:\\Users\\calm3\\Zotero\\storage\\IM5AINKC\\陽一郎_2018_デジタルゲームにおける可視化技術.pdf:application/pdf},
}

@misc{noauthor__nodate,
	title = {デジタルゲームにおける人工知能,シミュレーション,インタラクション(小特集 人工知能とインタラクション)｜書誌詳細｜国立国会図書館オンライン},
	url = {https://ndlonline.ndl.go.jp/#!/detail/R300000002-I028905635-00},
	urldate = {2021-04-10},
}

@article{__2017,
	title = {デジタルゲームにおける人工知能,シミュレーション,インタラクション (小特集 人工知能とインタラクション)},
	volume = {36},
	issn = {0285-9947},
	url = {https://ci.nii.ac.jp/naid/40021505619},
	number = {4},
	urldate = {2021-04-10},
	journal = {シミュレーション = Journal of the Japan Society for Simulation Technology},
	author = {三宅, 陽一郎},
	month = dec,
	year = {2017},
	note = {Publisher: 小宮山印刷工業},
	pages = {218--226},
}

@article{__2020-1,
	title = {大規模 デジタルゲームにおける人工知能の一般的体系と実装},
	volume = {35},
	doi = {10.1527/tjsai.B-J64},
	abstract = {A game AI general theory has been researched and developed in game industry in the world, and a new game AI general theory of AI in digital game is supposed with three AIs. For a large scale of game, a game AI system consists of three types of AI such as meta-AI, character AI, and navigation AI. A meta-AI is to control a game dynamically from a bird-view by watching a player’s behavior. A character AI is a brain of a game character such as a buddy, a monster, or a villager to make a decision in real-time. A navigation AI is to recognize an environment of a game to find a path or a best location to move dynamically. Especially, character AI is a main topic to study in game development, and it includes many fields such as multi-layered structure, character animation, agent architecture, decision-making modules, and so on. A new method of decision-making of combination of behavior trees and state machines is supposed. It is called AI Graph. The game AI general theory was applied to an AI system of an action-RPG game “FINAL FANTASY XV”. The results are showed in the paper. All characters’ decision-making system in FINAL FANTASY XV are based on AI Graphs. An AI Graph Editor is a tool to make an AI Graph only by using a mouse and simple text inputs. A dynamics of the new method is showed by explaining AI Graph Editor precisely.},
	number = {2},
	journal = {人工知能学会論文誌},
	author = {陽一郎, 三宅},
	year = {2020},
	pages = {B--J64\_1--16},
	file = {陽一郎_2020_大規模 デジタルゲームにおける人工知能の一般的体系と実装.pdf:C\:\\Users\\calm3\\Zotero\\storage\\7TMJVUER\\陽一郎_2020_大規模 デジタルゲームにおける人工知能の一般的体系と実装.pdf:application/pdf},
}

@article{haasdonk_feature_2005,
	title = {Feature space interpretation of {SVMs} with indefinite kernels},
	volume = {27},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/1401903/},
	doi = {10.1109/TPAMI.2005.78},
	abstract = {Kernel methods are becoming increasingly popular for various kinds of machine learning tasks, the most famous being the support vector machine (SVM) for classification. The SVM is well understood when using conditionally positive definite (cpd) kernel functions. However, in practice, non-cpd kernels arise and demand application in SVMs. The procedure of “plugging” these indefinite kernels in SVMs often yields good empirical classification results. However, they are hard to interpret due to missing geometrical and theoretical understanding. In this paper, we provide a step toward the comprehension of SVM classifiers in these situations. We give a geometric interpretation of SVMs with indefinite kernel functions. We show that such SVMs are optimal hyperplane classifiers not by margin maximization, but by minimization of distances between convex hulls in pseudo-Euclidean spaces. By this, we obtain a sound framework and motivation for indefinite SVMs. This interpretation is the basis for further theoretical analysis, e.g., investigating uniqueness, and for the derivation of practical guidelines like characterizing the suitability of indefinite SVMs.},
	language = {en},
	number = {4},
	urldate = {2021-04-11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Haasdonk, B.},
	month = apr,
	year = {2005},
	pages = {482--492},
	file = {Haasdonk - 2005 - Feature space interpretation of SVMs with indefini.pdf:C\:\\Users\\calm3\\Zotero\\storage\\KMBG8ME2\\Haasdonk - 2005 - Feature space interpretation of SVMs with indefini.pdf:application/pdf},
}

@article{_xaiexplainable_2020,
	title = {{XAI}({eXplainable} {AI})技術の研究動向},
	volume = {34},
	doi = {10.32230/jssmjournal.34.1_20},
	abstract = {深層学習モデルを代表とした機械学習技術の進展に伴い，その活用範囲が急速に広がり，人命や財産に関わる重要な意思決定に使うニーズが増大している．しかし，深層学習モデルなどの機械学習モデルは高度に複雑な構造物であり，人がその動作の全容を把握するのは困難であるため，実質的なブラックボックスとなっている．そのため，機械学習モデルの予測結果を安心して業務に使えないという問題が生じている．そこで，近年機械学習モデルの予測根拠を説明するXAI（eXplainable AI）技術の研究が急速に進み，多種多様な技術へと発展している．XAI技術は大きく，既存の機械学習モデルを説明する技術であるブラックボックス型と，学習過程や構造が人にとって解釈可能な新型の機械学習モデルであるトランスペアレント型とに大別される．本稿では，研究の進展が著しく，汎用性の高いブラックボックス型にフォーカスする．
　ブラックボックス型も説明の種別や，対象とする機械学習モデルの種別に応じて多種多様な技術が提案されている．本稿ではそれらを体系的に整理して俯瞰すると共に，近年のXAI技術を理解する上で欠かせない2つの代表的な技術，すなわち，Shapley値とInfluence Functionについて解説する．さらに，これらのXAI技術を悪用すれば，新たな攻撃のリスクとなりうる点について述べる．},
	number = {1},
	journal = {日本セキュリティ・マネジメント学会誌},
	author = {正史, 恵木},
	year = {2020},
	pages = {20--27},
	file = {正史_2020_XAI(eXplainable AI)技術の研究動向.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\正史_2020_XAI(eXplainable AI)技術の研究動向.pdf:application/pdf},
}

@inproceedings{panigutti_doctor_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Doctor {XAI}: an ontology-based approach to black-box sequential data classification explanations},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Doctor {XAI}},
	url = {https://doi.org/10.1145/3351095.3372855},
	doi = {10.1145/3351095.3372855},
	abstract = {Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.},
	urldate = {2021-04-18},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Panigutti, Cecilia and Perotti, Alan and Pedreschi, Dino},
	month = jan,
	year = {2020},
	pages = {629--639},
	file = {Panigutti et al_2020_Doctor XAI.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Panigutti et al_2020_Doctor XAI.pdf:application/pdf},
}

@article{netcommons__2020,
	title = {ビデオゲームにおける悪役の展開 : ビデオゲームの歴史に見る悪役の作られ方},
	volume = {14},
	issn = {1881316X},
	shorttitle = {ビデオゲームにおける悪役の展開},
	url = {https://toyama.repo.nii.ac.jp/index.php?active_action=repository_view_main_item_detail&page_id=32&block_id=36&item_id=17663&item_no=1},
	abstract = {CMS,Netcommons,Maple},
	language = {ja},
	number = {2},
	urldate = {2021-04-22},
	journal = {富山大学人間発達科学部紀要 = Memoirs of the Faculty of Human Development University of Toyama},
	author = {NetCommons},
	month = mar,
	year = {2020},
	pages = {93--103},
	file = {NetCommons_2020_ビデオゲームにおける悪役の展開.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\NetCommons_2020_ビデオゲームにおける悪役の展開.pdf:application/pdf},
}

@article{__2016,
	title = {逆強化学習を用いた転移可能な報酬関数の推定},
	volume = {JSAI2016},
	doi = {10.11517/pjsai.JSAI2016.0_3D3OS30a2},
	journal = {人工知能学会全国大会論文集},
	author = {勇樹, 北里 and 幸代, 荒井},
	year = {2016},
	pages = {3D3OS30a2--3D3OS30a2},
	file = {勇樹_幸代_2016_逆強化学習を用いた転移可能な報酬関数の推定.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\勇樹_幸代_2016_逆強化学習を用いた転移可能な報酬関数の推定.pdf:application/pdf},
}

@article{spinner_explainer_2020,
	title = {{explAIner}: {A} {Visual} {Analytics} {Framework} for {Interactive} and {Explainable} {Machine} {Learning}},
	volume = {26},
	issn = {1941-0506},
	shorttitle = {{explAIner}},
	doi = {10.1109/TVCG.2019.2934629},
	abstract = {We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Spinner, Thilo and Schlegel, Udo and Schäfer, Hanna and El-Assady, Mennatallah},
	month = jan,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1064--1074},
	file = {Spinner et al_2020_explAIner.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Spinner et al_2020_explAIner.pdf:application/pdf},
}

@article{ma_visual_2021,
	title = {A {Visual} {Analytics} {Framework} for {Explaining} and {Diagnosing} {Transfer} {Learning} {Processes}},
	doi = {10.1109/TVCG.2020.3028888},
	abstract = {Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Ma, Yuxin and Fan, Arlen and He, Jingrui and Nelakurthi, A. R. and Maciejewski, R.},
	year = {2021},
	file = {Ma et al_2021_A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Ma et al_2021_A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning.pdf:application/pdf},
}

@article{__1999,
	title = {色や形状等の表層的特徴量にもとづく画像内容検索技術},
	volume = {40},
	issn = {1882-7799},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=17805&item_no=1},
	abstract = {情報学広場 情報処理学会電子図書館},
	language = {ja},
	number = {SIG03(TOD1)},
	urldate = {2021-05-09},
	journal = {情報処理学会論文誌データベース（TOD）},
	author = {和彦, 串間 and 浩樹, 赤間 and 精一, 紺谷 and 雅司, 山室},
	month = feb,
	year = {1999},
	pages = {171--184},
}

@article{__2018-1,
	title = {招待フィールド論文：端末とサーバによるハイブリッド大規模画像検索システムとその応用},
	volume = {72},
	doi = {10.3169/itej.72.106},
	number = {1},
	journal = {映像情報メディア学会誌},
	author = {行信, 谷口},
	year = {2018},
	pages = {106--112},
	file = {行信_2018_招待フィールド論文：端末とサーバによるハイブリッド大規模画像検索システムとその応用.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\行信_2018_招待フィールド論文：端末とサーバによるハイブリッド大規模画像検索システムとその応用.pdf:application/pdf},
}

@article{bozdag_breaking_2015,
	title = {Breaking the filter bubble: democracy and design},
	volume = {17},
	issn = {1572-8439},
	shorttitle = {Breaking the filter bubble},
	url = {https://doi.org/10.1007/s10676-015-9380-y},
	doi = {10.1007/s10676-015-9380-y},
	abstract = {It has been argued that the Internet and social media increase the number of available viewpoints, perspectives, ideas and opinions available, leading to a very diverse pool of information. However, critics have argued that algorithms used by search engines, social networking platforms and other large online intermediaries actually decrease information diversity by forming so-called “filter bubbles”. This may form a serious threat to our democracies. In response to this threat others have developed algorithms and digital tools to combat filter bubbles. This paper first provides examples of different software designs that try to break filter bubbles. Secondly, we show how norms required by two democracy models dominate the tools that are developed to fight the filter bubbles, while norms of other models are completely missing in the tools. The paper in conclusion argues that democracy itself is a contested concept and points to a variety of norms. Designers of diversity enhancing tools must thus be exposed to diverse conceptions of democracy.},
	language = {en},
	number = {4},
	urldate = {2021-05-23},
	journal = {Ethics and Information Technology},
	author = {Bozdag, Engin and van den Hoven, Jeroen},
	month = dec,
	year = {2015},
	pages = {249--265},
	file = {Bozdag_van den Hoven_2015_Breaking the filter bubble.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Bozdag_van den Hoven_2015_Breaking the filter bubble.pdf:application/pdf},
}

@article{bruns_filter_2019,
	title = {Filter bubble},
	volume = {8},
	issn = {2197-6775},
	url = {https://policyreview.info/concepts/filter-bubble},
	abstract = {Concepts such as ‘filter bubble’ enjoy considerable popularity in scholarly as well as mainstream debates, but are rarely defined with any rigour. This has led to highly contradictory research findings. This article provides a critical review of the ‘filter bubble’ idea, and concludes that its persistence has served only to distract scholarly attention from far more critical areas of enquiry.},
	number = {4},
	urldate = {2021-05-29},
	journal = {Internet Policy Review},
	author = {Bruns, Axel},
	month = nov,
	year = {2019},
	file = {Bruns_2019_Filter bubble.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Bruns_2019_Filter bubble.pdf:application/pdf},
}

@inproceedings{nagulendra_understanding_2014,
	address = {New York, NY, USA},
	series = {{HT} '14},
	title = {Understanding and controlling the filter bubble through interactive visualization: a user study},
	isbn = {978-1-4503-2954-5},
	shorttitle = {Understanding and controlling the filter bubble through interactive visualization},
	url = {https://doi.org/10.1145/2631775.2631811},
	doi = {10.1145/2631775.2631811},
	abstract = {The "filter bubble" is a term which refers to people getting encapsulated in streams of data such as news or social network updates that are personalized to their interests. While people need protection from information overload and maybe prefer to see content they feel familiar or agree with, there is the danger that important issues that should be of concern for everyone will get filtered away and people will lack exposure to different views, living in "echo-chambers", blissfully unaware of the reality. We have proposed a design of an interactive visualization, which provides the user of a social networking site with awareness of the personalization mechanism (the semantics and the source of the content that is filtered away), and with means to control the filtering mechanism. The visualization has been implemented in a peer-to-peer social network, called MADMICA, and we present here the results of a large scale lab study with 163 crowd-sourced participants. The results demonstrate that the visualization leads to increased users' awareness of the filter bubble, understandability of the filtering mechanism and to a feeling of control over their data stream.},
	urldate = {2021-05-30},
	booktitle = {Proceedings of the 25th {ACM} conference on {Hypertext} and social media},
	publisher = {Association for Computing Machinery},
	author = {Nagulendra, Sayooran and Vassileva, Julita},
	month = sep,
	year = {2014},
	pages = {107--115},
	file = {Full Text PDF:C\:\\Users\\calm3\\Zotero\\storage\\R6RPIXHN\\Nagulendra and Vassileva - 2014 - Understanding and controlling the filter bubble th.pdf:application/pdf},
}

@misc{noauthor_unreasonable_nodate,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks}},
	url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	urldate = {2021-06-02},
}

@misc{geron_ageronhandson-ml2_2021,
	title = {ageron/handson-ml2},
	copyright = {Apache-2.0},
	url = {https://github.com/ageron/handson-ml2/blob/8342ab3fa25e4e855a140f008b6dde064064827f/16_nlp_with_rnns_and_attention.ipynb},
	abstract = {A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.},
	urldate = {2021-06-02},
	author = {Geron, Aurélien},
	month = jun,
	year = {2021},
	note = {original-date: 2019-01-08T03:49:07Z},
}

@article{kudo_subword_2018,
	title = {Subword {Regularization}: {Improving} {Neural} {Network} {Translation} {Models} with {Multiple} {Subword} {Candidates}},
	shorttitle = {Subword {Regularization}},
	url = {http://arxiv.org/abs/1804.10959},
	abstract = {Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.},
	urldate = {2021-06-02},
	journal = {arXiv:1804.10959 [cs]},
	author = {Kudo, Taku},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.10959},
	file = {Kudo_2018_Subword Regularization.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Kudo_2018_Subword Regularization.pdf:application/pdf},
}

@article{sennrich_neural_2016,
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://arxiv.org/abs/1508.07909},
	abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
	urldate = {2021-06-02},
	journal = {arXiv:1508.07909 [cs]},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = jun,
	year = {2016},
	note = {arXiv: 1508.07909},
	file = {Sennrich et al_2016_Neural Machine Translation of Rare Words with Subword Units.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Sennrich et al_2016_Neural Machine Translation of Rare Words with Subword Units.pdf:application/pdf},
}

@article{__2015,
	title = {フィルターバブルを気づかせるシステムの提案},
	volume = {JSAI2015},
	doi = {10.11517/pjsai.JSAI2015.0_1H21},
	abstract = {推薦システムの発達により利用者は自身の好む情報に接する機会が増加した。しかしそのような状況では利用者の好まない情報や興味外の情報は排除され、新しい関心を持つ機会や異なる考え方との接触が低下する可能性がある。このような状況はフィルターバブルと呼ばれる。本研究ではフィルターバブルが生じているかを判断し、利用者に提示することで利用者自らフィルターバブルの解決を促すシステムの提案を行う。},
	journal = {人工知能学会全国大会論文集},
	author = {雅裕, 片岡 and 智訓, 橋山 and 俊一, 田野},
	year = {2015},
	pages = {1H21--1H21},
	file = {雅裕 et al_2015_フィルターバブルを気づかせるシステムの提案.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\雅裕 et al_2015_フィルターバブルを気づかせるシステムの提案.pdf:application/pdf},
}

@article{__2018-2,
	title = {分断の時代におけるナラティヴとストーリーテリング教育},
	volume = {16},
	doi = {10.14960/gbkkg.16.45},
	abstract = {現在，インターネット社会において，周縁化されがちな弱者の意見は，フィルター・バブルがひしめく中でかき消され，時に激しいヘイトスピーチの下に晒されがちである。そこで，彼らが声を上げることは難しい。本稿では，米国のストーリーテリング実践，日本の生活記録運動や臨床領域で展開されるナラティヴ・アプローチなど，ストーリーテリングの手法を活用して，他者理解や連帯，社会参画を目的に展開されてきた実践の系譜を辿りながら，周縁化されがちな人びとにとって，物語を協働的，対話的に生成し，共有することがエンパワメントにつながるという道筋を示す。またその視点から，デジタル・ストーリーテリングを再検討し，デジタル時代における協働的なストーリーテリングの利点について再検討する。},
	journal = {言語文化教育研究},
	author = {明子, 小川},
	year = {2018},
	pages = {45--54},
	file = {明子_2018_分断の時代におけるナラティヴとストーリーテリング教育.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\明子_2018_分断の時代におけるナラティヴとストーリーテリング教育.pdf:application/pdf},
}

@article{__2020-2,
	title = {インタビュー 日本発ニュースアプリ、米国で急拡大 コロナ後の連帯へメディアは規律を スマートニュース 会長兼社長 最高経営責任者({CEO}) 鈴木健氏},
	issn = {0285-4619},
	url = {https://ci.nii.ac.jp/naid/40022222512},
	number = {1016},
	urldate = {2021-06-06},
	journal = {日経コンピュータ = Nikkei computer},
	author = {鈴木, 健},
	month = may,
	year = {2020},
	note = {Publisher: 日経BP},
	pages = {48--51},
}

@article{__2009,
	title = {複数国ニュースサイトの比較対象分析システム {NSContrast} における評価手法に関する検討},
	volume = {2009-DD-71},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=62386&item_no=1},
	abstract = {情報学広場 情報処理学会電子図書館},
	language = {ja},
	number = {1},
	urldate = {2021-06-06},
	journal = {研究報告デジタルドキュメント（DD）},
	author = {真治, 吉岡},
	month = may,
	year = {2009},
	pages = {1--8},
	file = {真治_2009_複数国ニュースサイトの比較対象分析システム NSContrast における評価手法に関する検討.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\真治_2009_複数国ニュースサイトの比較対象分析システム NSContrast における評価手法に関する検討.pdf:application/pdf},
}

@misc{noauthor_supporting_nodate,
	title = {Supporting reflective public thought with considerit {\textbar} {Proceedings} of the {ACM} 2012 conference on {Computer} {Supported} {Cooperative} {Work}},
	url = {https://dl.acm.org/doi/abs/10.1145/2145204.2145249},
	urldate = {2021-06-06},
	file = {Supporting reflective public thought with considerit Proceedings of the ACM.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Supporting reflective public thought with considerit Proceedings of the ACM.pdf:application/pdf},
}

@inproceedings{faridani_opinion_2010,
	address = {New York, NY, USA},
	series = {{CHI} '10},
	title = {Opinion space: a scalable tool for browsing online comments},
	isbn = {978-1-60558-929-9},
	shorttitle = {Opinion space},
	url = {https://doi.org/10.1145/1753326.1753502},
	doi = {10.1145/1753326.1753502},
	abstract = {Internet users are increasingly inclined to contribute comments to online news articles, videos, product reviews, and blogs. The most common interface for comments is a list, sorted by time of entry or by binary ratings. It is widely recognized that such lists do not scale well and can lead to "cyberpolarization," which serves to reinforce extreme opinions. We present Opinion Space: a new online interface incorporating ideas from deliberative polling, dimensionality reduction, and collaborative filtering that allows participants to visualize and navigate through a diversity of comments. This self-organizing system automatically highlights the comments found most insightful by users from a range of perspectives. We report results of a controlled user study. When Opinion Space was compared with a chronological List interface, participants read a similar diversity of comments. However, they were significantly more engaged with the system, and they had significantly higher agreement with and respect for the comments they read.},
	urldate = {2021-06-06},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Faridani, Siamak and Bitton, Ephrat and Ryokai, Kimiko and Goldberg, Ken},
	month = apr,
	year = {2010},
	pages = {1175--1184},
	file = {Faridani et al_2010_Opinion space.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Faridani et al_2010_Opinion space.pdf:application/pdf},
}

@article{__2020-3,
	title = {フィルターバブルとマス破壊兵器について},
	volume = {80},
	doi = {10.20627/jsimconf.80.0_63},
	journal = {情報経営},
	author = {広志, 古賀},
	year = {2020},
	pages = {63--66},
	file = {広志_2020_フィルターバブルとマス破壊兵器について.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\広志_2020_フィルターバブルとマス破壊兵器について.pdf:application/pdf},
}

@article{__2020-4,
	title = {人気度に着目したニュース閲覧行動の変容分析},
	volume = {JSAI2020},
	doi = {10.11517/pjsai.JSAI2020.0_1L5GS501},
	abstract = {電子媒体で発信される情報量が増加し，推薦サービスの導入も進んでいる．その中で，過度な推薦により，ユーザに偏った情報のみを提供するフィルターバブルやエコーチェンバーなどの問題が存在するとの指摘もある． 我々はこれらのユーザ行動をログデータから定量的に評価することを目指している．これまでに，記事のカテゴリの多様性に基づきユーザの行動変容を議論してきた．本稿では，記事のカテゴリ情報に加え，記事の人気や記者に関する情報を用いることで，ユーザ行動の変化をより詳細に捉えられることを示した．},
	journal = {人工知能学会全国大会論文集},
	author = {亜斗夢, 園田 and 寛人, 中島 and 不二夫, 鳥海},
	year = {2020},
	pages = {1L5GS501--1L5GS501},
	file = {亜斗夢 et al_2020_人気度に着目したニュース閲覧行動の変容分析.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\亜斗夢 et al_2020_人気度に着目したニュース閲覧行動の変容分析.pdf:application/pdf},
}

@article{__2020-5,
	title = {ウェブの功罪},
	volume = {70},
	doi = {10.18919/jkg.70.6_309},
	abstract = {多様な情報と人々をつなぐはずのソーシャルメディアが，社会的分断や情報のタコツボ化を助長しているという問題が顕在化している。特に，エコーチェンバーやフィルターバブルといった閉じた情報環境は，自分の好みに合致した情報のみが来やすく，自分とは異なる観点からの情報が来にくいため，フェイク（偽）ニュースやヘイト（憎悪）の温床となる危険性を孕んでいる。本稿では，計算社会科学の観点から，偽ニュースが拡散する仕組みについて解説し，ウェブの負の側面について論じる。また，今後のウェブの技術が克服すべき問題について議論する。},
	number = {6},
	journal = {情報の科学と技術},
	author = {和俊, 笹原},
	year = {2020},
	pages = {309--314},
	file = {和俊_2020_ウェブの功罪.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\和俊_2020_ウェブの功罪.pdf:application/pdf},
}

@article{__2019-1,
	title = {オンラインメディア記事の読者の行動分析},
	volume = {JSAI2019},
	doi = {10.11517/pjsai.JSAI2019.0_1I2J504},
	abstract = {インターネットの登場により新聞社等のメディアの発信するニュースの媒体が新聞からwebサイトに拡大し，記事の速報性の向上や記事数の増加が進んでいる．これに伴い，ユーザが記事を選択する際に必要な時間と労力が増大している．それに対応し，技術の発達によってユーザの選好に応じて提示情報をパーソナライズするタイプの推薦システムの導入が進んでいる．一方で，過度の推薦によってユーザに偏った情報のみを提供するフィルターバブルが発生しているとの指摘もある． 本研究では既存のニュースサイト上でのユーザの行動変容について分析を行い，筆者のこれまでの研究と比較する．これにより，既存の結果と比較し，多様性の減少はメディアが異なっても起きているのかを確認する． そのために，既存研究と同様にユーザの行動を閲覧した記事の多様性によって評価し，現実のデータで時間の経過に伴う多様性の指標が変化していることを確認し，多様性の変化と期間・閲覧数の関係についても考察する．また，多様性が増加するユーザの存在を確認し，そのようなユーザの閲覧数の変化の条件についても確認する．},
	journal = {人工知能学会全国大会論文集},
	author = {亜斗夢, 園田 and 喜史, 関 and 不二夫, 鳥海},
	year = {2019},
	pages = {1I2J504--1I2J504},
	file = {亜斗夢 et al_2019_オンラインメディア記事の読者の行動分析.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\亜斗夢 et al_2019_オンラインメディア記事の読者の行動分析.pdf:application/pdf},
}

@article{__2012,
	title = {情報中立推薦システム},
	volume = {JSAI2012},
	doi = {10.11517/pjsai.JSAI2012.0_3E1R61},
	abstract = {推薦システムは，大量の情報の中から関心のある情報を抽出するための手段として普及している．しかし，関心がないものとしてフィルタリングされてしまい，多様な情報に触れる機会を奪っているとするフィルタバブル問題が指摘されている．この問題に対し，ある観点・情報について中立性を保証する推薦システムを提案する．},
	journal = {人工知能学会全国大会論文集},
	author = {敏弘, 神嶌 and 昭太郎, 赤穂 and 英樹, 麻生 and 淳, 佐久間},
	year = {2012},
	pages = {3E1R61--3E1R61},
	file = {敏弘 et al_2012_情報中立推薦システム.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\敏弘 et al_2012_情報中立推薦システム.pdf:application/pdf},
}

@article{bechmann_are_2018,
	title = {Are {We} {Exposed} to the {Same} “{News}” in the {News} {Feed}?},
	volume = {6},
	issn = {2167-0811},
	url = {https://doi.org/10.1080/21670811.2018.1510741},
	doi = {10.1080/21670811.2018.1510741},
	abstract = {In the backdrop of interests in social media news and polarization the aim of this study is to examine to what extent we are exposed to the same “news” in the News Feed? The article defines news not from a classical position but from the content that is judged relevant to the user and made visible by the algorithm. The study examines filter bubbles as information similarity and more specifically nonoverlapping content segments in a unique snapshot dataset of 14 days of personal Facebook News Feeds for 1,000 Danes mirroring the Danish Facebook population. Deploying methods to analyze both link sources and content semantics the study finds that less than 10\% in the link source analysis and 27.8\% in the semantic analysis are in a filter bubble. The article tests and discusses suitable conceptual and empirical thresholds for information similarity that can inspire future studies. The best significant predictors for being in a filter bubble are what we term sociality: number of page likes, group memberships and friends. The study does not find age, gender, education or area of residence isolated to be significant predictors of participants in filter bubbles.},
	number = {8},
	urldate = {2021-06-19},
	journal = {Digital Journalism},
	author = {Bechmann, Anja and Nielbo, Kristoffer L.},
	month = sep,
	year = {2018},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/21670811.2018.1510741},
	pages = {990--1002},
}

@article{seargeant_social_2019,
	series = {Post-truth and the political: {Constructions} and distortions in representing political facts},
	title = {Social media and the future of open debate: {A} user-oriented approach to {Facebook}’s filter bubble conundrum},
	volume = {27},
	issn = {2211-6958},
	shorttitle = {Social media and the future of open debate},
	url = {https://www.sciencedirect.com/science/article/pii/S2211695817302271},
	doi = {10.1016/j.dcm.2018.03.005},
	abstract = {Drawing on a two-year project, Creating Facebook, this article explores how the actions and agency of Facebook users contribute to the distortion of information and polarisation of socio-political opinion. Facebook’s influence as a channel for the circulation of news has come under intense scrutiny recently, especially with regard to the dissemination of false stories. While this criticism has focused on the ‘filter bubbles’ created by the site’s personalisation algorithms, our research indicates that users’ own actions also play a key role in how the site operates as a forum for debate. Our findings show that the strategies people use to navigate the complex social space contribute to the polarising of debate, as they seek to avoid conflict with the diverse members of their network.},
	language = {en},
	urldate = {2021-06-19},
	journal = {Discourse, Context \& Media},
	author = {Seargeant, Philip and Tagg, Caroline},
	month = mar,
	year = {2019},
	pages = {41--48},
	file = {Seargeant_Tagg_2019_Social media and the future of open debate.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Seargeant_Tagg_2019_Social media and the future of open debate.pdf:application/pdf},
}

@article{nguyen_echo_2020,
	title = {Echo {Chambers} and {Epistemic} {Bubbles}},
	volume = {17},
	doi = {10.1017/epi.2018.32},
	number = {2},
	journal = {Episteme},
	author = {Nguyen, C. Thi},
	year = {2020},
	pages = {141--161},
}

@article{gunawardana_survey_2009,
	title = {A {Survey} of {Accuracy} {Evaluation} {Metrics} of {Recommendation} {Tasks}},
	volume = {10},
	issn = {1532-4435},
	abstract = {Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms offline using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of offline experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing offline experiments.},
	journal = {The Journal of Machine Learning Research},
	author = {Gunawardana, Asela and Shani, Guy},
	month = dec,
	year = {2009},
	pages = {2935--2962},
	file = {Gunawardana_Shani_2009_A Survey of Accuracy Evaluation Metrics of Recommendation Tasks.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Gunawardana_Shani_2009_A Survey of Accuracy Evaluation Metrics of Recommendation Tasks.pdf:application/pdf},
}

@article{__2019-2,
	title = {疑似順位付け評価を用いたニュースコメント順位付けモデルの教師なしアンサンブル},
	volume = {JSAI2019},
	url = {https://ci.nii.ac.jp/naid/130007658394},
	doi = {10.11517/pjsai.JSAI2019.0_2I5J901},
	number = {0},
	urldate = {2021-06-20},
	journal = {人工知能学会全国大会論文集},
	author = {藤田, 綜一郎 and 小林, 隼人 and 奥村, 学},
	year = {2019},
	note = {Publisher: 一般社団法人 人工知能学会},
	pages = {2I5J901--2I5J901},
}

@article{__2015-1,
	title = {固有ベクトル法による類似文書抽出},
	volume = {114},
	issn = {0913-5685; 2432-6380},
	url = {https://www.ieice.org/ken/paper/20150205UBXg/},
	language = {ja},
	number = {444},
	urldate = {2021-06-20},
	journal = {電子情報通信学会技術研究報告; 信学技報},
	author = {加藤, 翔子 and 斉藤, 和巳 and 風間, 一洋},
	month = jan,
	year = {2015},
	note = {Publisher: 電子情報通信学会},
	pages = {11--16},
	file = {加藤 et al_2015_固有ベクトル法による類似文書抽出.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\加藤 et al_2015_固有ベクトル法による類似文書抽出.pdf:application/pdf},
}

@misc{noauthor_bert_nodate,
	title = {汎用言語モデル「{BERT}」のビジネス実用化技術に迫る},
	url = {https://journal.ntt.co.jp/},
	abstract = {NTT技術ジャーナルは、NTTグループの総合技術誌として、研究開発・事業動向をタイムリーに紹介しています。},
	language = {ja},
	urldate = {2021-06-20},
	journal = {NTT技術ジャーナル},
}

@article{aronson_effect_1959,
	title = {The effect of severity of initiation on liking for a group.},
	doi = {10.1037/H0047195},
	abstract = {"An experiment was conducted to test the hypothesis that persons who undergo an unpleasant initiation to become members of a group increase their liking for the group; that is, they find the group more attractive than do persons who become members without going through a severe initiation. This hypothesis was derived from Festinger's theory of cognitive dissonance." 3 conditions were employed: reading of "embarrassing material" before a group, mildly embarrassing material to be read, no reading. "The results clearly verified the hypothesis." (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
	author = {Aronson, E. and Mills, J.},
	year = {1959},
}

@article{alyari_recommender_2018,
	title = {Recommender systems: {A} systematic review of the state of the art literature and suggestions for future research},
	volume = {47},
	issn = {0368-492X},
	shorttitle = {Recommender systems},
	url = {https://doi.org/10.1108/K-06-2017-0196},
	doi = {10.1108/K-06-2017-0196},
	abstract = {Purpose This paper aims to identify, evaluate and integrate the findings of all relevant and high-quality individual studies addressing one or more research questions about recommender systems and performing a comprehensive study of empirical research on recommender systems that have been divided into five main categories. To achieve this aim, the authors use systematic literature review (SLR) as a powerful method to collect and critically analyze the research papers. Also, the authors discuss the selected recommender systems and its main techniques, as well as their benefits and drawbacks in general. Design/methodology/approach In this paper, the SLR method is utilized with the aim of identifying, evaluating and integrating the findings of all relevant and high-quality individual studies addressing one or more research questions about recommender systems and performing a comprehensive study of empirical research on recommender systems that have been divided into five main categories. Also, the authors discussed recommender system and its techniques in general without a specific domain. Findings The major developments in categories of recommender systems are reviewed, and new challenges are outlined. Furthermore, insights on the identification of open issues and guidelines for future research are provided. Also, this paper presents the systematical analysis of the recommender system literature from 2005. The authors identified 536 papers, which were reduced to 51 primary studies through the paper selection process. Originality/value This survey will directly support academics and practical professionals in their understanding of developments in recommender systems and its techniques.},
	number = {5},
	urldate = {2021-06-25},
	journal = {Kybernetes},
	author = {Alyari, Fatemeh and Jafari Navimipour, Nima},
	month = jan,
	year = {2018},
	note = {Publisher: Emerald Publishing Limited},
	pages = {985--1017},
}

@article{batmaz_review_2019,
	title = {A review on deep learning for recommender systems: challenges and remedies},
	volume = {52},
	issn = {1573-7462},
	shorttitle = {A review on deep learning for recommender systems},
	url = {https://doi.org/10.1007/s10462-018-9654-y},
	doi = {10.1007/s10462-018-9654-y},
	abstract = {Recommender systems are effective tools of information filtering that are prevalent due to increasing access to the Internet, personalization trends, and changing habits of computer users. Although existing recommender systems are successful in producing decent recommendations, they still suffer from challenges such as accuracy, scalability, and cold-start. In the last few years, deep learning, the state-of-the-art machine learning technique utilized in many complex tasks, has been employed in recommender systems to improve the quality of recommendations. In this study, we provide a comprehensive review of deep learning-based recommendation approaches to enlighten and guide newbie researchers interested in the subject. We analyze compiled studies within four dimensions which are deep learning models utilized in recommender systems, remedies for the challenges of recommender systems, awareness and prevalence over recommendation domains, and the purposive properties. We also provide a comprehensive quantitative assessment of publications in the field and conclude by discussing gained insights and possible future work on the subject.},
	language = {en},
	number = {1},
	urldate = {2021-06-25},
	journal = {Artificial Intelligence Review},
	author = {Batmaz, Zeynep and Yurekli, Ali and Bilge, Alper and Kaleli, Cihan},
	month = jun,
	year = {2019},
	pages = {1--37},
	file = {Batmaz et al_2019_A review on deep learning for recommender systems.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Batmaz et al_2019_A review on deep learning for recommender systems.pdf:application/pdf},
}

@article{karimi_news_2018,
	title = {News recommender systems – {Survey} and roads ahead},
	volume = {54},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/S030645731730153X},
	doi = {10.1016/j.ipm.2018.04.008},
	abstract = {More and more people read the news online, e.g., by visiting the websites of their favorite newspapers or by navigating the sites of news aggregators. However, the abundance of news information that is published online every day through different channels can make it challenging for readers to locate the content they are interested in. The goal of News Recommender Systems (NRS) is to make reading suggestions to users in a personalized way. Due to their practical relevance, a variety of technical approaches to build such systems have been proposed over the last two decades. In this work, we review the state-of-the-art of designing and evaluating news recommender systems over the last ten years. One main goal of the work is to analyze which particular challenges of news recommendation (e.g., short item life times and recency aspects) have been well explored and which areas still require more work. Furthermore, in contrast to previous surveys, the paper specifically discusses methodological questions and today’s academic practice of evaluating and comparing different algorithmic news recommendation approaches based on accuracy measures.},
	language = {en},
	number = {6},
	urldate = {2021-06-25},
	journal = {Information Processing \& Management},
	author = {Karimi, Mozhgan and Jannach, Dietmar and Jugovac, Michael},
	month = nov,
	year = {2018},
	pages = {1203--1227},
	file = {Karimi et al_2018_News recommender systems – Survey and roads ahead.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Karimi et al_2018_News recommender systems – Survey and roads ahead.pdf:application/pdf},
}

@article{khan_content-based_2020,
	title = {A content-based technique for linking dual language news articles in an archive},
	issn = {0165-5515},
	url = {https://doi.org/10.1177/0165551520937614},
	doi = {10.1177/0165551520937614},
	abstract = {To retrieve a specific news article from a vast archive containing multilingual news articles against a user query or based on similarity among news articles is a challenging task. The task becomes even further complicated when the archive contains articles from a low resourced and morphologically complex language like Urdu, along with English new articles. The article proposes a content-based (lexical) similarity measure, that is, Common Ratio Measure for Dual Language (CRMDL), for linking digital news articles published in various online news sources. The similarity measure links Urdu-to-English news articles during the preservation process using an Urdu-to-English lexicon. A literature review showed that an Urdu-to-English lexicon did not exist, and therefore, the first task was to build a lexicon from multiple sources. The proposed similarity measure, that is, CRMDL, is evaluated rigorously on different data sets, of varying sizes, to assess the effectiveness. The experimental results show that the proposed measure is feasible and effective for similarity computation between Urdu and English news articles, which can obtain, on average, 50\% precision and 67\% recall. The performance can be improved sufficiently by managing the limitations summarised in the study.},
	language = {en},
	urldate = {2021-06-28},
	journal = {Journal of Information Science},
	author = {Khan, Muzammil and Rahman, Arif Ur and Ahmad, Arshad and Khan, Sarwar Shah},
	month = aug,
	year = {2020},
	note = {Publisher: SAGE Publications Ltd},
	pages = {0165551520937614},
}

@inproceedings{del_carmenrodriguez-hernandez_experimental_2020,
	title = {An {Experimental} {Evaluation} of {Content}-based {Recommendation} {Systems}: {Can} {Linked} {Data} and {BERT} {Help}?},
	shorttitle = {An {Experimental} {Evaluation} of {Content}-based {Recommendation} {Systems}},
	doi = {10.1109/AICCSA50499.2020.9316466},
	abstract = {Content-Based Recommendation Systems suggest items (e.g., articles, products, objects, services, or places) that are relevant to the user based on the features describing the items. In many content-based recommendation systems we can find, along with discrete attributes, textual features (e.g., text summaries or comments) obtained from web pages, news articles, etc. Traditionally, to enable its exploitation, the textual information of items is represented by using basic information retrieval models (such as the vector space model), which do not take into account natural language challenges involving the semantics of the words (synonymy, polysemy and hiperonymy, etc.) or language understanding. Other solutions try to exploit those semantics. In this paper, we present an experimental evaluation where we compare several recommendation approaches, including a content-based recommender based on vector space models, a deep learning and content-based recommendation approach, and a semantic-aware content-based recommendation model. This last approach exploits textual features of items obtained from the Linked Open Data (LOD) and BERT (Bidirectional Encoder Representations from Transformers) for language modelling. Deep Learning transformers are achieving good results in different NLP (Natural Language Processing) problems, but using them to build content-based recommendation systems has not been explored in depth so far. Our experimental results, focused on the domain of movie recommendations, show that a approach based on the use of BERT can provide good results if enough training data are available.},
	booktitle = {2020 {IEEE}/{ACS} 17th {International} {Conference} on {Computer} {Systems} and {Applications} ({AICCSA})},
	author = {del CarmenRodríguez-Hernández, María and del-Hoyo-Alonso, Rafael and Ilarri, Sergio and Montafñés-Salas, Rosa María and Sabroso-Lasa, Sergio},
	month = nov,
	year = {2020},
	note = {ISSN: 2161-5330},
	pages = {1--8},
	file = {del CarmenRodríguez-Hernández et al_2020_An Experimental Evaluation of Content-based Recommendation Systems.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\del CarmenRodríguez-Hernández et al_2020_An Experimental Evaluation of Content-based Recommendation Systems.pdf:application/pdf},
}

@inproceedings{hassan_bert_2019,
	title = {{BERT}, {ELMo}, {USE} and {InferSent} {Sentence} {Encoders}: {The} {Panacea} for {Research}-{Paper} {Recommendation}?},
	shorttitle = {{BERT}, {ELMo}, {USE} and {InferSent} {Sentence} {Encoders}},
	abstract = {Content-based approaches to research paper recommendation are important when user feedback is sparse or not available. The task of content-based matching is challenging, mainly due to the problem of determining the semantic similarity of texts. Nowadays, there exist many sentence embedding models that learn deep semantic representations by being trained on huge corpora, aiming to provide transfer learning to a wide variety of natural language processing tasks. In this work, we present a comparative evaluation among five well-known pre-trained sentence encoders deployed in the pipeline of title-based research paper recommendation. The experimented encoders are USE, BERT, ACM RecSys 2019 Late-breaking Results, 16th-20th September 2019, Copenhagen, Denmark Copyright ©2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). BERT, ELMo, USE and InferSent Sentence Encoders: The Panacea for Research-Paper Recommendation? ACM RecSys 2019 Late-breaking Results, 16th-20th September 2019, Copenhagen, Denmark InferSent, ELMo, and SciBERT. For our study, we propose a methodology for evaluating such models in reranking BM25-based recommendations. The experimental results show that the sole consideration of semantic information from these encoders does not lead to improved recommendation performance over the traditional BM25 technique, while their integration enables the retrieval of a set of relevant papers that may not be retrieved by the BM25 ranking function.},
	booktitle = {{RecSys}},
	author = {Hassan, H. and Sansonetti, Giuseppe and Gasparetti, Fabio and Micarelli, A. and Beel, J.},
	year = {2019},
}

@book{noauthor_notitle_nodate,
}

@misc{noauthor_japan_nodate,
	title = {Japan : {Tradition} and business interests {\textbar} {Reporters} without borders},
	shorttitle = {Japan},
	url = {https://rsf.org/en/japan},
	abstract = {Yoshihide Suga, Shinzo Abe’s former right-hand man and successor as prime minister since September 2020, has done nothing to improve the climate for press freedom. The world’s third biggest economic power, Japan respects the principles of media freedom and pluralism. But journalists find it hard to fully play their role as democracy’s watchdog because of the influence of tradition and business interests. Journalists have been complaining of a climate of mistrust toward them ever since the nationalist right swept to power in the 2012 general election. The system of “kisha clubs” (reporters’ clubs) continues to discriminate against freelancers and foreign reporters. On social networks, nationalist groups harass journalists who are critical of the government or cover “anti-patriotic” subjects such as the Fukushima Daiichi nuclear disaster or the US military presence in Okinawa. The government continues to refuse any debate about a law protecting “Specially-Designated Secrets,” under which whistleblowers, journalists and bloggers face up to ten years in prison if convicted of publishing information obtained “illegally”.},
	language = {en},
	urldate = {2021-07-04},
	journal = {RSF},
}

@misc{noauthor_2021_2021,
	title = {2021 {World} {Press} {Freedom} {Index}: {Journalism}, the vaccine against disinformation, blocked in more than 130 countries},
	shorttitle = {2021 {World} {Press} {Freedom} {Index}},
	url = {https://rsf.org/en/2021-world-press-freedom-index-journalism-vaccine-against-disinformation-blocked-more-130-countries},
	abstract = {Читать на русском / Read in Russian This year’s Index, which evaluates the press freedom situation in 180 countries and territories annually, shows that journalism, journalism, which is arguably the best vaccine against the virus of disinformation, is totally blocked or seriously impeded in 73 countries and constrained in 59 others, which together represent 73\% of the countries evaluated. These countries are classified as having “very bad,” “bad” or “problematic” environments for press freedom, and are identified accordingly in black, red or orange on the World Press Freedom map.},
	language = {en},
	urldate = {2021-07-04},
	journal = {RSF},
	month = apr,
	year = {2021},
}

@inproceedings{kapoor_i_2015,
	address = {New York, NY, USA},
	series = {{RecSys} '15},
	title = {"{I} like to explore sometimes": {Adapting} to {Dynamic} {User} {Novelty} {Preferences}},
	isbn = {978-1-4503-3692-5},
	shorttitle = {"{I} like to explore sometimes"},
	url = {https://doi.org/10.1145/2792838.2800172},
	doi = {10.1145/2792838.2800172},
	abstract = {Studies have shown that the recommendation of unseen, novel or serendipitous items is crucial for a satisfying and engaging user experience. As a result, recent developments in recommendation research have increasingly focused towards introducing novelty in user recommendation lists. While, existing solutions aim to find the right balance between the similarity and novelty of the recommended items, they largely ignore the user needs for novelty. In this paper, we show that there are large individual and temporal differences in the users' novelty preferences. We develop a regression model to predict these dynamic novelty preferences of users using features derived from their past interactions. Finally, we describe an adaptive recommender,{\textasciitilde}{\textbackslash}emph\{adaNov-R\}, that adapts to the user needs for novel items and show that the model achieves better recommendation performance on a metric that considers both novel and familiar items.},
	urldate = {2021-07-04},
	booktitle = {Proceedings of the 9th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kapoor, Komal and Kumar, Vikas and Terveen, Loren and Konstan, Joseph A. and Schrater, Paul},
	month = sep,
	year = {2015},
	pages = {19--26},
	file = {Kapoor et al_2015_I like to explore sometimes.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Kapoor et al_2015_I like to explore sometimes.pdf:application/pdf},
}

@inproceedings{ferdous_semantic_2017,
	title = {A semantic content based recommendation system for cross-lingual news},
	doi = {10.1109/ICIVPR.2017.7890880},
	abstract = {News articles in web narrate important events happening worldwide. These articles are not only written in English, but also in different languages for different native people. In this paper, we propose an approach for an automated Bengali-English semantic recommender system based on ontology by analyzing news domain. News ontology is designed automatically by using information extraction techniques. Both the news title and news body are considered separately in the ontology creation process. First, important information from news is extracted and ontology is created from the source language document. Then, ontology is created from target language document following similar technique. Next, ontology matching is performed between the translated source ontology and target English Ontology. Matching can also be done with synonymous documents. A matching factor is calculated which can be taken as the semantic similarity measure between the cross-lingual documents. Recommendation of news items is done based on this matching factor. The experiment study verifies the proposed method adopted by us.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Imaging}, {Vision} {Pattern} {Recognition} ({icIVPR})},
	author = {Ferdous, Syeda Nyma and Ali, Muhammad Masroor},
	month = feb,
	year = {2017},
	pages = {1--6},
	file = {Ferdous_Ali_2017_A semantic content based recommendation system for cross-lingual news.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Ferdous_Ali_2017_A semantic content based recommendation system for cross-lingual news2.pdf:application/pdf},
}

@article{chan_reproducible_2020,
	title = {Reproducible {Extraction} of {Cross}-lingual {Topics} (rectr)},
	volume = {14},
	issn = {1931-2458},
	url = {https://doi.org/10.1080/19312458.2020.1812555},
	doi = {10.1080/19312458.2020.1812555},
	abstract = {With global media content databases and online content being available, analyzing topical structures in different languages simultaneously has become an urgent computational task. Some previous studies have analyzed topics in a multilingual corpus by translating all items into a single language using a machine translation service, such as Google Translate. We argue that this method is not reproducible in the long run and proposes a new method – Reproducible Extraction of Cross-lingual Topics Using R (rectr). Our method utilizes open-source-aligned word embeddings to understand the cross-lingual meanings of words and has a mechanism to normalize residual influence from language differences. We present a benchmark that compares the topics extracted from a corpus of English, German, and French news using our method with methods used in the literature. We show that our method is not only reproducible but can also generate high-quality cross-lingual topics. We demonstrate how our method can be applied in tracking news topics across time and languages.},
	number = {4},
	urldate = {2021-07-04},
	journal = {Communication Methods and Measures},
	author = {Chan, Chung-Hong and Zeng, Jing and Wessler, Hartmut and Jungblut, Marc and Welbers, Kasper and Bajjalieh, Joseph W. and Atteveldt, Wouter van and Althaus, Scott L.},
	month = oct,
	year = {2020},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/19312458.2020.1812555},
	keywords = {unread, prior},
	pages = {285--305},
}

@misc{noauthor_detailed_nodate,
	title = {Detailed methodology {\textbar} {RSF}},
	url = {https://rsf.org/en/detailed-methodology},
	urldate = {2021-07-05},
}

@inproceedings{ijntema_ontology-based_2010,
	title = {Ontology-based news recommendation},
	doi = {10.1145/1754239.1754257},
	abstract = {Recommending news items is traditionally done by term-based algorithms like TF-IDF. This paper concentrates on the benefits of recommending news items using a domain ontology instead of using a term-based approach. For this purpose, we propose Athena, which is an extension to the existing Hermes framework. Athena employs a user profile to store terms or concepts found in news items browsed by the user. Based on this information, the framework uses a traditional method based on TF-IDF, and several ontology-based methods to recommend new articles to the user. The paper concludes with the evaluation of the different methods, which shows that the new ontology-based method that we propose in this paper performs better (w.r.t. accuracy, precision, and recall) than the traditional method and, with the exception of one measure (recall), also better than the other considered ontology-based approaches.},
	booktitle = {{EDBT} '10},
	author = {IJntema, W. and Goossen, F. and Frasincar, F. and Hogenboom, F.},
	year = {2010},
}

@article{narducci_concept-based_2016,
	title = {Concept-based item representations for a cross-lingual content-based recommendation process},
	volume = {374},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516308076},
	doi = {10.1016/j.ins.2016.09.022},
	abstract = {The growth of the Web is the most influential factor that contributes to the increasing importance of text retrieval and filtering systems. On one hand, the Web is becoming more and more multilingual, and on the other hand users themselves are becoming increasingly polyglot. In this context, platforms for intelligent information access as search engines or recommender systems need to evolve to deal with this increasing amount of multilingual information. This paper proposes a content-based recommender system able to generate cross-lingual recommendations. The idea is to exploit user preferences learned in a given language, to suggest item in another language. The main intuition behind the work is that, differently from keywords which are inherently language dependent, concepts are stable across different languages, allowing to deal with multilingual and cross-lingual scenarios. We propose four knowledge-based strategies to build concept-based representation of items, by relying on the knowledge contained in two knowledge sources, i.e. Wikipedia and BabelNet. We learn user profiles by leveraging the different concept-based representations, in order to define a cross-lingual recommendation process. The empirical evaluation carried out on two state of the art datasets, DBbook and Movielens, shows that concept-based approaches are suitable to provide cross-lingual recommendations, even though there is not a clear advantage of using one of the different proposed representations. However, it emerges that most of the times the approaches based on BabelNet outperform those based on Wikipedia, which clearly shows the advantage of using a native multilingual knowledge source.},
	language = {en},
	urldate = {2021-07-06},
	journal = {Information Sciences},
	author = {Narducci, Fedelucio and Basile, Pierpaolo and Musto, Cataldo and Lops, Pasquale and Caputo, Annalina and de Gemmis, Marco and Iaquinta, Leo and Semeraro, Giovanni},
	month = dec,
	year = {2016},
	pages = {15--31},
}

@misc{5__nodate,
	title = {【入門】トピックモデルとは？トピック分析の３つの手法を解説},
	url = {https://spjai.com/topic-model/},
	abstract = {目次１．あらすじ２．トピックモデルとは？３．トピック分析の３つの手法３－１．Latent Semantic Index３－２．Probabilistic Latent Semantic Indexing３－３．Laten ...},
	language = {ja},
	urldate = {2021-07-11},
	author = {{5分}},
	keywords = {LDA, LSI, PLSI, topic model},
	file = {Snapshot:C\:\\Users\\calm3\\Zotero\\storage\\7ZH7Y57N\\topic-model.html:text/html},
}

@inproceedings{tian_labeled_2018,
	title = {Labeled {Bilingual} {Topic} {Model} for {Cross}-{Lingual} {Text} {Classification} and {Label} {Recommendation}},
	doi = {10.1109/ICISCE.2018.00067},
	abstract = {Aiming at the increasingly rich multi language information resources and multi-label data in news reports and scientific literatures, in order to mining the relevance between languages and the correlation between data, this paper proposed labeled bilingual topic model, applied on cross-lingual text classification and label recommendation. First of all, it could assume that the keywords in the scientific literature are relevant to the abstract in same article, then extracted the keywords and regarded it as labels, and aligned the labels with topics in topic model, instantiated the “latent” topic. Secondly, trained the abstracts in article through the topic model proposed by this paper. Finally, classified the new documents by cross-lingual text classifier, also recommended the labels. The experiment result show that Micro-F1 measure reaches 94.81\% in cross-lingual text classification task, and the recommended labels also reflects the sematic relevance with documents.},
	booktitle = {2018 5th {International} {Conference} on {Information} {Science} and {Control} {Engineering} ({ICISCE})},
	author = {Tian, Ming-Jie and Huang, Zheng-Hao and Cui, Rong-Yi},
	month = jul,
	year = {2018},
	keywords = {topic model, cross-lingual text classification, Data models, Dictionaries, label, label recommendation, latent topic, Probabilistic logic, Probability, Semantics, Task analysis, Text categorization},
	pages = {285--289},
	file = {Tian et al. - 2018 - Labeled Bilingual Topic Model for Cross-Lingual Te.pdf:C\:\\Users\\calm3\\Zotero\\storage\\J5YY896G\\Tian et al. - 2018 - Labeled Bilingual Topic Model for Cross-Lingual Te.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\calm3\\Zotero\\storage\\RCWFIPXW\\8612565.html:text/html},
}

@article{__2019-3,
	title = {閲覧中のニュース記事に不足している情報を補完するニュース記事推薦手法の提案},
	volume = {119},
	issn = {0913-5685; 2432-6380},
	url = {https://www.ieice.org/ken/paper/20191224R1TJ/},
	language = {ja},
	number = {354},
	urldate = {2021-07-13},
	journal = {電子情報通信学会技術研究報告; 信学技報},
	author = {三好, 良弥 and 奥野, 拓},
	month = dec,
	year = {2019},
	note = {Publisher: 電子情報通信学会},
	pages = {7--11},
}

@misc{noauthor__nodate-1,
	title = {情報学広場：情報処理学会電子図書館},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=191192&item_no=1&page_id=13&block_id=8},
	urldate = {2021-07-13},
}

@article{__2018-3,
	title = {ニューラルネットワークを用いた国際的なニューストピックスへの国民感情を踏まえた意見表明},
	volume = {2018},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=191192&item_no=1},
	abstract = {情報学広場 情報処理学会電子図書館},
	language = {ja},
	urldate = {2021-07-13},
	journal = {エンタテインメントコンピューティングシンポジウム2018論文集},
	author = {廉, 樋口 and 邊誠, 川野},
	month = sep,
	year = {2018},
	pages = {226--229},
	file = {廉_邊誠_2018_ニューラルネットワークを用いた国際的なニューストピックスへの国民感情を踏まえた意見表明.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\廉_邊誠_2018_ニューラルネットワークを用いた国際的なニューストピックスへの国民感情を踏まえた意見表明.pdf:application/pdf},
}

@misc{noauthor_can_nodate,
	title = {Can {Trust} in {Traditional} {News} {Media} {Explain} {Cross}-{National} {Differences} in {News} {Exposure} of {Young} {People} {Online}?: {A} comparative study of {Israel}, {Norway} and the {United} {Kingdom}: {Digital} {Journalism}: {Vol} 6, {No} 2},
	url = {https://www.tandfonline.com/doi/abs/10.1080/21670811.2017.1332484},
	urldate = {2021-07-13},
}

@misc{noauthor_can_nodate-1,
	title = {Can {Trust} in {Traditional} {News} {Media} {Explain} {Cross}-{National} {Differences} in {News} {Exposure} of {Young} {People} {Online}?: {A} comparative study of {Israel}, {Norway} and the {United} {Kingdom}: {Digital} {Journalism}: {Vol} 6, {No} 2},
	url = {https://www.tandfonline.com/doi/abs/10.1080/21670811.2017.1332484},
	urldate = {2021-07-13},
	keywords = {unread},
}

@article{kilgo_ferguson_2018,
	title = {From \#{Ferguson} to \#{Ayotzinapa}: {Analyzing} {Differences} in {Domestic} and {Foreign} {Protest} {News} {Shared} on {Social} {Media}},
	volume = {21},
	issn = {1520-5436},
	shorttitle = {From \#{Ferguson} to \#{Ayotzinapa}},
	url = {https://doi.org/10.1080/15205436.2018.1469773},
	doi = {10.1080/15205436.2018.1469773},
	abstract = {This study compares U.S. digital news coverage of recent foreign and domestic protests. Differences in coverage’s framing, sourcing, and device emphases were analyzed for two cases: protests that erupted after the death of Michael Brown and protests demanding justice for the 43 missing students from Ayotzinapa, Mexico. Building on protest paradigm literature, content analysis results show that news articles that appeared on Facebook and Twitter emphasized legitimizing frames for foreign protests more than domestic protests. Foreign protests were framed with the spectacle frame more than domestic protests, which were more often portrayed as confrontational. Digitally native news organizations produced content that deviated from expected paradigmatic norms the most. In addition, this research examines the relationship between content and sharing on Facebook and Twitter. Implications of these findings within the theoretical framework of the protest paradigm are discussed.},
	number = {5},
	urldate = {2021-07-13},
	journal = {Mass Communication and Society},
	author = {Kilgo, Danielle K. and Harlow, Summer and García-Perdomo, Victor and Salaverría, Ramón},
	month = sep,
	year = {2018},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/15205436.2018.1469773},
	pages = {606--630},
}

@book{fang_writing_1991,
	title = {Writing {Style} {Differences} in {Newspaper}, {Radio}, and {Television} {News}. {Monograph} {Series} {No}. 1},
	isbn = {978-1-881221-00-5},
	url = {https://eric.ed.gov/?id=ED377481},
	abstract = {This monograph examines the features of writing in three journalistic media: television, radio and print. The monograph is relevant not only to students and teachers of journalism but also to those interested generally in the field of composition studies. It offers a highly specific analysis of the differences among the writing styles for these three media--differences that are often felt by student and teacher alike but rarely described in detail. Comparative in orientation, the monograph examines each of the three media in all major sections: (1) Leads; (2) Story Structure; (3) Sentence Structure; (4) Word Choice; (5) Names, Quotes and Attribution. Each of these sections discusses a number of distinct points; three columns, one for each of the media, highlight the comparisons.  The section on "leads," for instance, looks at summary leads, datelines, beginning with names, the umbrella lead, an item lead, an anecdotal lead, a delayed lead, a buried lead, question leads and quote leads. Also, Appendix 1 shows the results of several interviews with media writers; they were asked a broad range of questions concerning clarity in the media. Appendix 2 offers a college curriculum guide to students preparing for a career in journalism. A 47-item bibliography relating to journalism skills is attached. (TB)},
	language = {en},
	urldate = {2021-07-13},
	publisher = {Center for Interdisciplinary Studies in Writing, University of Minnesota, 227 Lind Hall, 207 Church St},
	author = {Fang, Irving},
	year = {1991},
	file = {Fang_1991_Writing Style Differences in Newspaper, Radio, and Television News.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Fang_1991_Writing Style Differences in Newspaper, Radio, and Television News.pdf:application/pdf},
}

@misc{noauthor_latent_nodate,
	title = {Latent {Dirichlet} {Allocation}({LDA})を用いたニュース記事の分類},
	url = {https://developer.smartnews.com/blog/2013/08/19/lda-based-channel-categorization-in-smartnews/},
	abstract = {{\textless}p{\textgreater}{\textless}a href="http://www.gocro.jp/" target="\_blank"{\textgreater}株式会社ゴクロ{\textless}/a{\textgreater}の中路です。{\textless}/p{\textgreater}{\textless}p{\textgreater}以前の{\textless}a href="http://developer.smartnews.be/blog/2013/07/23/bayes-classification-based-channel-categorization-in-smartnews/" target="blank\_"{\textgreater}ベイズ分類をベースにしたSmartNewsのチャンネル判定{\textless}/a{\textgreater}で触れたように、{\textless}a href="https://www.smartnews.be/" target="blank\_"{\textgreater}SmartNews{\textless}/a{\textgreater}で配信する記事を「スポーツ」「エンタメ」「コラム」のようなチャンネルに分類しているのは、人ではなく機械です。そのアルゴリズムとして前回ご紹介したのは「ナイーブベイズ分類器」ですが、記事の分類を行う手法は、他にも様々なものがあります。その中で今回は{\textless}a href="https://en.wikipedia.org/wiki/Latent\_Dirichlet\_allocation" target="blank\_"{\textgreater}Latent Dirichlet Allocation(以下LDA){\textless}/a{\textgreater}について、先日東京大学の博士課程の皆さんと、社内で合同勉強会を行った際に作成した資料をベースにご紹介します。{\textless}/p{\textgreater}},
	urldate = {2021-07-13},
	journal = {SmartNews Engineering Blog},
}

@misc{noauthor_latent_nodate-1,
	title = {Latent {Dirichlet} {Allocation}({LDA})を用いたニュース記事の分類},
	url = {https://developer.smartnews.com/blog/2013/08/19/lda-based-channel-categorization-in-smartnews/},
	abstract = {{\textless}p{\textgreater}{\textless}a href="http://www.gocro.jp/" target="\_blank"{\textgreater}株式会社ゴクロ{\textless}/a{\textgreater}の中路です。{\textless}/p{\textgreater}{\textless}p{\textgreater}以前の{\textless}a href="http://developer.smartnews.be/blog/2013/07/23/bayes-classification-based-channel-categorization-in-smartnews/" target="blank\_"{\textgreater}ベイズ分類をベースにしたSmartNewsのチャンネル判定{\textless}/a{\textgreater}で触れたように、{\textless}a href="https://www.smartnews.be/" target="blank\_"{\textgreater}SmartNews{\textless}/a{\textgreater}で配信する記事を「スポーツ」「エンタメ」「コラム」のようなチャンネルに分類しているのは、人ではなく機械です。そのアルゴリズムとして前回ご紹介したのは「ナイーブベイズ分類器」ですが、記事の分類を行う手法は、他にも様々なものがあります。その中で今回は{\textless}a href="https://en.wikipedia.org/wiki/Latent\_Dirichlet\_allocation" target="blank\_"{\textgreater}Latent Dirichlet Allocation(以下LDA){\textless}/a{\textgreater}について、先日東京大学の博士課程の皆さんと、社内で合同勉強会を行った際に作成した資料をベースにご紹介します。{\textless}/p{\textgreater}},
	urldate = {2021-07-13},
	journal = {SmartNews Engineering Blog},
	keywords = {read},
}

@inproceedings{zhang_dynamic_2019,
	title = {Dynamic {News} {Recommendation} with {Hierarchical} {Attention} {Network}},
	doi = {10.1109/ICDM.2019.00190},
	abstract = {News recommendation is an effective information dissemination solution in modern society. In general, news articles can be modeled from multiple granularities: sentence-, element-and news-level. However, the first two levels have been largely ignored in existing methods and it is also unclear how such multi-granularity modeling can enhance news recommendation. In this paper, we propose a novel dynamic model for news recommendation. A unique perspective of our model is to discriminate the contributions of previously interacted contents for triggering the next news-reading, in sentence-, element-and news-level simultaneously. To this end, we design a hierarchical attention network of which the lower layer learns the impacts of sentences and elements, while the upper layer captures disparity of news. Moreover, we incorporate a time-decaying factor to reflect the dynamism, as well as convolution neural networks for learning sequential influence. Using three real-world datasets, we conduct extensive experiments to verify the superiority of our model, compared with several state-of-the-art approaches.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Zhang, Hui and Chen, Xu and Ma, Shuai},
	month = nov,
	year = {2019},
	note = {ISSN: 2374-8486},
	keywords = {unread, prior},
	pages = {1456--1461},
	file = {Zhang et al_2019_Dynamic News Recommendation with Hierarchical Attention Network.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Zhang et al_2019_Dynamic News Recommendation with Hierarchical Attention Network.pdf:application/pdf},
}

@article{wang_estimation_2018,
	title = {Estimation of {Cross}-{Lingual} {News} {Similarities} {Using} {Text}-{Mining} {Methods}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1911-8074/11/1/8},
	doi = {10.3390/jrfm11010008},
	abstract = {In this research, two estimation algorithms for extracting cross-lingual news pairs based on machine learning from financial news articles have been proposed. Every second, innumerable text data, including all kinds news, reports, messages, reviews, comments, and tweets are generated on the Internet, and these are written not only in English but also in other languages such as Chinese, Japanese, French, etc. By taking advantage of multi-lingual text resources provided by Thomson Reuters News, we developed two estimation algorithms for extracting cross-lingual news pairs from multilingual text resources. In our first method, we propose a novel structure that uses the word information and the machine learning method effectively in this task. Simultaneously, we developed a bidirectional Long Short-Term Memory (LSTM) based method to calculate cross-lingual semantic text similarity for long text and short text, respectively. Thus, when an important news article is published, users can read similar news articles that are written in their native language using our method.},
	language = {en},
	number = {1},
	urldate = {2021-07-13},
	journal = {Journal of Risk and Financial Management},
	author = {Wang, Zhouhao and Liu, Enda and Sakaji, Hiroki and Ito, Tomoki and Izumi, Kiyoshi and Tsubouchi, Kota and Yamashita, Tatsuo},
	month = mar,
	year = {2018},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {unread},
	pages = {8},
	file = {Wang et al_2018_Estimation of Cross-Lingual News Similarities Using Text-Mining Methods.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Wang et al_2018_Estimation of Cross-Lingual News Similarities Using Text-Mining Methods.pdf:application/pdf},
}

@article{lai_bridging_2019,
	title = {Bridging the domain gap in cross-lingual document classification},
	url = {http://arxiv.org/abs/1909.07009},
	abstract = {The scarcity of labeled training data often prohibits the internationalization of NLP models to multiple languages. Recent developments in cross-lingual understanding (XLU) has made progress in this area, trying to bridge the language barrier using language universal representations. However, even if the language problem was resolved, models trained in one language would not transfer to another language perfectly due to the natural domain drift across languages and cultures. We consider the setting of semi-supervised cross-lingual understanding, where labeled data is available in a source language (English), but only unlabeled data is available in the target language. We combine state-of-the-art cross-lingual methods with recently proposed methods for weakly supervised learning such as unsupervised pre-training and unsupervised data augmentation to simultaneously close both the language gap and the domain gap in XLU. We show that addressing the domain gap is crucial. We improve over strong baselines and achieve a new state-of-the-art for cross-lingual document classification.},
	urldate = {2021-07-13},
	journal = {arXiv:1909.07009 [cs]},
	author = {Lai, Guokun and Oguz, Barlas and Yang, Yiming and Stoyanov, Veselin},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.07009},
	keywords = {unread, prior},
	file = {Lai et al_2019_Bridging the domain gap in cross-lingual document classification.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Lai et al_2019_Bridging the domain gap in cross-lingual document classification.pdf:application/pdf},
}

@misc{dai_newsarticlescsv_2017,
	title = {{NewsArticles}.csv},
	url = {https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/GMFCTR/IZQODZ},
	urldate = {2021-07-13},
	publisher = {Harvard Dataverse},
	author = {Dai, Tianru},
	year = {2017},
	doi = {10.7910/DVN/GMFCTR/IZQODZ},
	note = {Type: dataset},
}

@article{beel_research-paper_2016,
	title = {Research-paper recommender systems: a literature survey},
	volume = {17},
	issn = {1432-5012, 1432-1300},
	shorttitle = {Research-paper recommender systems},
	url = {http://link.springer.com/10.1007/s00799-015-0156-0},
	doi = {10.1007/s00799-015-0156-0},
	language = {en},
	number = {4},
	urldate = {2021-07-14},
	journal = {International Journal on Digital Libraries},
	author = {Beel, Joeran and Gipp, Bela and Langer, Stefan and Breitinger, Corinna},
	month = nov,
	year = {2016},
	pages = {305--338},
	file = {Beel et al. - 2016 - Research-paper recommender systems a literature s.pdf:C\:\\Users\\calm3\\Zotero\\storage\\FJF27WUP\\Beel et al. - 2016 - Research-paper recommender systems a literature s.pdf:application/pdf},
}

@article{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {https://arxiv.org/abs/1908.10084v1},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	language = {en},
	urldate = {2021-07-15},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = aug,
	year = {2019},
	file = {Reimers_Gurevych_2019_Sentence-BERT.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Reimers_Gurevych_2019_Sentence-BERT2.pdf:application/pdf},
}

@misc{mm0824_sentence-bert_2020,
	title = {Sentence-{BERTを理解する}},
	url = {https://data-analytics.fun/2020/08/04/understanding-sentence-bert/},
	abstract = {今回はBERTの埋め込み表現の質を改善する“Sentence-BERT”について解説したいと思います。まず、ラベル付きデータを用いたセンチメント分析や、2つの文章をインプットとした類似度の予測等であればBERTの日本語モデルをそのまま使うこ},
	language = {ja},
	urldate = {2021-07-15},
	journal = {楽しみながら理解する自然言語処理入門},
	author = {{mm0824}},
	month = aug,
	year = {2020},
	note = {Section: AI・機械学習},
}

@misc{noauthor_japanese_nodate,
	title = {Japanese {FakeNews} {Dataset}},
	url = {https://kaggle.com/tanreinama/japanese-fakenews-dataset},
	abstract = {This dataset consists of news articles and deep fake articles in Japanese.},
	language = {en},
	urldate = {2021-07-19},
}

@misc{noauthor_ibm_2017,
	type = {{CT002}},
	title = {{IBM} {Research}},
	copyright = {© Copyright IBM Corp. 2011},
	url = {http://www.research.ibm.com/labs/haifa/},
	abstract = {IBM Research - Haifa is the largest lab of IBM Research Division outside of the United States. Founded as a small scientific center in 1972, it grew into a major lab that leads the development of innovative technological products and solutions for the IBM corporation.},
	language = {en-US},
	urldate = {2021-07-19},
	month = feb,
	year = {2017},
	note = {Publisher: IBM Corporation},
}

@article{blei_latent_nodate,
	title = {Latent {Dirichlet} {Allocation}},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	language = {en},
	author = {Blei, David M},
	pages = {30},
	file = {Blei - Latent Dirichlet Allocation.pdf:C\:\\Users\\calm3\\Zotero\\storage\\P8CUU5P6\\Blei - Latent Dirichlet Allocation.pdf:application/pdf},
}

@article{vaswani_attention_nodate,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	pages = {11},
	file = {Vaswani et al. - Attention is All you Need.pdf:C\:\\Users\\calm3\\Zotero\\storage\\I8GE8GRW\\Vaswani et al. - Attention is All you Need.pdf:application/pdf},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-07-23},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	file = {Devlin et al_2019_BERT.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Devlin et al_2019_BERT.pdf:application/pdf},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2021-07-25},
	journal = {arXiv:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.11692},
	file = {Liu et al_2019_RoBERTa.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Liu et al_2019_RoBERTa.pdf:application/pdf},
}

@misc{kouji_ohno__nodate,
	title = {ゲーム技術の研究所　テーマ「{Narrative}（ナラティブ）」},
	url = {https://www.slideshare.net/KoujiOhno/o-planning-narrative1b},
	urldate = {2021-07-31},
	author = {Kouji Ohno},
}

@misc{noauthor_book_nodate,
	title = {Book {Excerpt} and {Review} - {Game} {Writing}: {Narrative} {Skills} for {Videogames}},
	shorttitle = {Book {Excerpt} and {Review} - {Game} {Writing}},
	url = {https://www.gamasutra.com/view/feature/130255/book_excerpt_and_review__game_.php},
	abstract = {In today's Gamasutra feature, we reproduce the first chapter of Charles River Media's 'Game Writing: Narrative Skills For Videogames,' and follow it up with a review of the book's entirety by Gamasutra regular Brad Kane.},
	language = {en},
	urldate = {2021-07-31},
}

@misc{noauthor__nodate-2,
	title = {松永 伸司 ({Shinji} {Matsunaga}) - 資料公開 - researchmap},
	url = {https://researchmap.jp/multidatabases/multidatabase_contents/detail/243574/38daecd3b17498c26ff1b89983533fef?frame_id=726294},
	urldate = {2021-08-17},
	file = {松永 伸司 (Shinji Matsunaga) - 資料公開 - researchmap:C\:\\Users\\calm3\\Zotero\\storage\\Q3T8M6JJ\\38daecd3b17498c26ff1b89983533fef.html:text/html},
}

@misc{noauthor_investigating_nodate,
	title = {Investigating {COVID}-19 {News} {Across} {Four} {Nations}: {A} {Topic} {Modeling} and {Sentiment} {Analysis} {Approach} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/9366469},
	urldate = {2021-10-09},
}

@article{ghasiya_investigating_2021,
	title = {Investigating {COVID}-19 {News} {Across} {Four} {Nations}: {A} {Topic} {Modeling} and {Sentiment} {Analysis} {Approach}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Investigating {COVID}-19 {News} {Across} {Four} {Nations}},
	doi = {10.1109/ACCESS.2021.3062875},
	abstract = {Newspapers are very important for a society as they inform citizens about the events around them and how they can impact their life. Their importance becomes more crucial and indispensable in the times of health crisis such as the current COVID-19 pandemic. Since the starting of this pandemic newspapers are providing rich information to the public about various issues such as the discovery of a new strain of coronavirus, lockdown and other restrictions, government policies, and information related to the vaccine development for the same. In this scenario, analysis of emergent and widely reported topics/themes/issues and associated sentiments from various countries can help us better understand the COVID-19 pandemic. In our research, the database of more than 100,000 COVID-19 news headlines and articles were analyzed using top2vec (for topic modeling) and RoBERTa (for sentiment classification and analysis). Our topic modeling results highlighted that education, economy, US, and sports are some of the most common and widely reported themes across UK, India, Japan, South Korea. Further, our sentiment classification model achieved 90\% validation accuracy and the analysis showed that the worst affected country, i.e. the UK (in our dataset) also has the highest percentage of negative sentiment.},
	journal = {IEEE Access},
	author = {Ghasiya, Piyush and Okamura, Koji},
	year = {2021},
	note = {Conference Name: IEEE Access},
	pages = {36645--36656},
	file = {Ghasiya_Okamura_2021_Investigating COVID-19 News Across Four Nations.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Ghasiya_Okamura_2021_Investigating COVID-19 News Across Four Nations.pdf:application/pdf},
}

@article{qiao_understanding_2019,
	title = {Understanding the {Behaviors} of {BERT} in {Ranking}},
	url = {http://arxiv.org/abs/1904.07531},
	abstract = {This paper studies the performances and behaviors of BERT in ranking tasks. We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking. Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model. Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking. Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker.},
	urldate = {2021-10-11},
	journal = {arXiv:1904.07531 [cs]},
	author = {Qiao, Yifan and Xiong, Chenyan and Liu, Zhenghao and Liu, Zhiyuan},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.07531},
	file = {Qiao et al_2019_Understanding the Behaviors of BERT in Ranking.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Qiao et al_2019_Understanding the Behaviors of BERT in Ranking.pdf:application/pdf},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {Paper} tables with annotated results for {Stanza}: {A} {Python} {Natural} {Language} {Processing} {Toolkit} for {Many} {Human} {Languages}},
	shorttitle = {Papers with {Code} - {Paper} tables with annotated results for {Stanza}},
	url = {https://paperswithcode.com/paper/stanza-a-python-natural-language-processing/review/},
	abstract = {Paper tables with annotated results for Stanza: A Python Natural Language Processing Toolkit for Many Human Languages},
	language = {en},
	urldate = {2021-11-09},
	file = {Papers with Code - Paper tables with annotated results for Stanza.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Papers with Code - Paper tables with annotated results for Stanza.pdf:application/pdf},
}

@article{qi_stanza_2020,
	title = {Stanza: {A} {Python} {Natural} {Language} {Processing} {Toolkit} for {Many} {Human} {Languages}},
	shorttitle = {Stanza},
	url = {http://arxiv.org/abs/2003.07082},
	abstract = {We introduce Sta n z a , an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Sta n z a features a language-agnostic fully neural pipeline for text analysis, including tokenization, multiword token expansion, lemmatization, part-ofspeech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Sta n z a on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Sta n z a includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https:// stanfordnlp.github.io/stanza/.},
	language = {en},
	urldate = {2021-11-09},
	journal = {arXiv:2003.07082 [cs]},
	author = {Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
	month = apr,
	year = {2020},
	note = {arXiv: 2003.07082},
	file = {Qi et al. - 2020 - Stanza A Python Natural Language Processing Toolk.pdf:C\:\\Users\\calm3\\Zotero\\storage\\EEYCCDGN\\Qi et al. - 2020 - Stanza A Python Natural Language Processing Toolk.pdf:application/pdf},
}

@inproceedings{rinott_show_2015,
	address = {Lisbon, Portugal},
	title = {Show {Me} {Your} {Evidence} - an {Automatic} {Method} for {Context} {Dependent} {Evidence} {Detection}},
	url = {http://aclweb.org/anthology/D15-1050},
	doi = {10.18653/v1/D15-1050},
	abstract = {Engaging in a debate with oneself or others to take decisions is an integral part of our day-today life. A debate on a topic (say, use of performance enhancing drugs) typically proceeds by one party making an assertion/claim (say, PEDs are bad for health) and then providing an evidence to support the claim (say, a 2006 study shows that PEDs have psychiatric side effects). In this work, we propose the task of automatically detecting such evidences from unstructured text that support a given claim. This task has many practical applications in decision support and persuasion enhancement in a wide range of domains. We ﬁrst introduce an extensive benchmark data set tailored for this task, which allows training statistical models and assessing their performance. Then, we suggest a system architecture based on supervised learning to address the evidence detection task. Finally, promising experimental results are reported.},
	language = {en},
	urldate = {2021-12-10},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rinott, Ruty and Dankin, Lena and Alzate Perez, Carlos and Khapra, Mitesh M. and Aharoni, Ehud and Slonim, Noam},
	year = {2015},
	pages = {440--450},
	file = {Evidence2015.pdf:C\:\\Users\\calm3\\Zotero\\storage\\QBED8NEW\\Evidence2015.pdf:application/pdf},
}

@misc{pariser_beware_nodate,
	title = {Beware online "filter bubbles"},
	url = {https://www.ted.com/talks/eli_pariser_beware_online_filter_bubbles},
	abstract = {As web companies strive to tailor their services (including news and search results) to our personal tastes, there's a dangerous unintended consequence: We get trapped in a "filter bubble" and don't get exposed to information that could challenge or broaden our worldview. Eli Pariser argues powerfully that this will ultimately prove to be bad for us and bad for democracy.},
	language = {en},
	urldate = {2022-01-06},
	author = {Pariser, Eli},
}

@misc{noauthor_eli_nodate,
	title = {Eli {Pariser}: {Beware} online "filter bubbles" {\textbar} {TED} {Talk}},
	url = {https://www.ted.com/talks/eli_pariser_beware_online_filter_bubbles/transcript#t-520554},
	urldate = {2022-01-06},
	file = {Eli Pariser\: Beware online "filter bubbles" | TED Talk:C\:\\Users\\calm3\\Zotero\\storage\\SI8739QB\\transcript.html:text/html},
}

@misc{inc_echo_nodate,
	title = {エコー‐チェンバー【echo chamber】},
	url = {https://japanknowledge.com/lib/display/?lid=2001028535800},
	urldate = {2022-01-06},
	journal = {JapanKnowledge},
	author = {Inc, NetAdvance},
}

@article{nguyen_echo_2020-1,
	title = {{ECHO} {CHAMBERS} {AND} {EPISTEMIC} {BUBBLES}},
	volume = {17},
	issn = {1742-3600, 1750-0117},
	url = {https://www.cambridge.org/core/journals/episteme/article/abs/echo-chambers-and-epistemic-bubbles/5D4AC3A808C538E17C50A7C09EC706F0},
	doi = {10.1017/epi.2018.32},
	abstract = {Discussion of the phenomena of post-truth and fake news often implicates the closed epistemic networks of social media. The recent conversation has, however, blurred two distinct social epistemic phenomena. An epistemic bubble is a social epistemic structure in which other relevant voices have been left out, perhaps accidentally. An echo chamber is a social epistemic structure from which other relevant voices have been actively excluded and discredited. Members of epistemic bubbles lack exposure to relevant information and arguments. Members of echo chambers, on the other hand, have been brought to systematically distrust all outside sources. In epistemic bubbles, other voices are not heard; in echo chambers, other voices are actively undermined. It is crucial to keep these phenomena distinct. First, echo chambers can explain the post-truth phenomena in a way that epistemic bubbles cannot. Second, each type of structure requires a distinct intervention. Mere exposure to evidence can shatter an epistemic bubble, but may actually reinforce an echo chamber. Finally, echo chambers are much harder to escape. Once in their grip, an agent may act with epistemic virtue, but social context will pervert those actions. Escape from an echo chamber may require a radical rebooting of one's belief system.},
	language = {en},
	number = {2},
	urldate = {2022-01-06},
	journal = {Episteme},
	author = {Nguyen, C. Thi},
	month = jun,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	pages = {141--161},
}

@article{__2020-6,
	title = {ウェブの功罪},
	volume = {70},
	doi = {10.18919/jkg.70.6_309},
	abstract = {多様な情報と人々をつなぐはずのソーシャルメディアが，社会的分断や情報のタコツボ化を助長しているという問題が顕在化している。特に，エコーチェンバーやフィルターバブルといった閉じた情報環境は，自分の好みに合致した情報のみが来やすく，自分とは異なる観点からの情報が来にくいため，フェイク（偽）ニュースやヘイト（憎悪）の温床となる危険性を孕んでいる。本稿では，計算社会科学の観点から，偽ニュースが拡散する仕組みについて解説し，ウェブの負の側面について論じる。また，今後のウェブの技術が克服すべき問題について議論する。},
	number = {6},
	journal = {情報の科学と技術},
	author = {和俊, 笹原},
	year = {2020},
	pages = {309--314},
	file = {和俊_2020_ウェブの功罪.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\和俊_2020_ウェブの功罪2.pdf:application/pdf},
}

@article{conover_partisan_2012,
	title = {Partisan asymmetries in online political activity},
	volume = {1},
	copyright = {2012 Conover et al.; licensee Springer.},
	issn = {2193-1127},
	url = {https://epjdatascience.springeropen.com/articles/10.1140/epjds6},
	doi = {10.1140/epjds6},
	abstract = {We examine partisan differences in the behavior, communication patterns and social interactions of more than 18,000 politically-active Twitter users to produce evidence that points to changing levels of partisan engagement with the American online political landscape. Analysis of a network defined by the communication activity of these users in proximity to the 2010 midterm congressional elections reveals a highly segregated, well clustered, partisan community structure. Using cluster membership as a high-fidelity (87\% accuracy) proxy for political affiliation, we characterize a wide range of differences in the behavior, communication and social connectivity of left- and right-leaning Twitter users. We find that in contrast to the online political dynamics of the 2008 campaign, right-leaning Twitter users exhibit greater levels of political activity, a more tightly interconnected social structure, and a communication network topology that facilitates the rapid and broad dissemination of political information.},
	language = {en},
	number = {1},
	urldate = {2022-01-06},
	journal = {EPJ Data Science},
	author = {Conover, Michael D. and Gonçalves, Bruno and Flammini, Alessandro and Menczer, Filippo},
	month = dec,
	year = {2012},
	note = {Number: 1
Publisher: SpringerOpen},
	pages = {1--19},
	file = {Conover et al_2012_Partisan asymmetries in online political activity.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Conover et al_2012_Partisan asymmetries in online political activity.pdf:application/pdf},
}

@misc{noauthor_scikit-learnkerastensorflow_nodate,
	title = {scikit-learn、{Keras}、{TensorFlowによる実践機械学習} 第2版},
	url = {https://www.oreilly.co.jp/books/9784873119281/},
	abstract = {本書はコードを動かしながら学び、機械学習が使えるようになることを目的とした書籍です。現実的な問題を出し、サンプルデータを示しながら、機械学習で問題を解決に導くまでの一連の手法を体系立てて解説します。
深層学習以外の機械学習にはscikit-learnを使い、機械学習プロジェクトの流れ、データからモデルを学習する方法、データの処理・クリーニングなどの基礎から、特徴量の選択や過学習、データの次元削減など応用までを学びます。深層学習にはTensorFlowとKerasを使い、ニューラルネットワークの構築と訓練、ニューラルネットワークアーキテクチャ、深層学習や強化学習、さらにTensorFlowの分散処理のメカニズムや実装までを幅広く解説します。
第2版では教師なし学習と深層ネットワーク訓練手法、コンピュータビジョンテクニック、自然言語処理、Tensor Flowの大規模な訓練や効率的なデータの取り扱いについての解説を拡充し、新たに畳み込みニューラルネットワークを使ったシーケンス処理とGANによる画像生成の説明を追加しました。サンプルコードはすべてTensorFlow2に準拠しています。
すべてのコードがGitHub上で公開されており、Jupyter Notebookを使って試しながら学ぶことができます。アルゴリズムの説明に終始せず、実際の業務で必要となる機械学習のスキルをまとめた本書は機械学習を学びたいエンジニア必携の一冊です。},
	language = {ja},
	urldate = {2022-01-06},
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2022-01-07},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	file = {Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf},
}

@inproceedings{kimura_autoclustering_2018,
	title = {{AutoClustering}: {A} {Feed}-{Forward} {Neural} {Network} {Based} {Clustering} {Algorithm}},
	shorttitle = {{AutoClustering}},
	doi = {10.1109/ICDMW.2018.00102},
	abstract = {Since a clustering process can be regarded as a map of data to cluster labels, it should be natural to employ a deep learning technique, especially a feed-forward neural network, to realize the clustering method. In this study, we discussed a novel clustering method realized only by a feed-forward neural network. Unlike self-organizing maps and growing neural gas networks, the proposed method is compatible with deep learning neural networks. The proposed method has three parts: a map of records to clusters (encoder), a map of clusters to their exemplars (decoder), and a loss function to measure positional closeness between the records and the exemplars. In order to accelerate clustering performance, we proposed an improved activation function at the encoder, which migrates a soft-max function to a max function continuously. Though most of the clustering methods require the number of clusters in advance, the proposed method naturally provides the number of clusters as the number of unique one-hot vectors obtained as a result. We also discussed the existence of local minima of the loss function and their relationship to clusters.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
	author = {Kimura, Masaomi},
	month = nov,
	year = {2018},
	note = {ISSN: 2375-9259},
	pages = {659--666},
	file = {Kimura_2018_AutoClustering.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Kimura_2018_AutoClustering.pdf:application/pdf},
}

@article{reddy_newsclaims_2021,
	title = {{NewsClaims}: {A} {New} {Benchmark} for {Claim} {Detection} from {News} with {Background} {Knowledge}},
	shorttitle = {{NewsClaims}},
	url = {http://arxiv.org/abs/2112.08544},
	abstract = {Claim detection and verification are crucial for news understanding and have emerged as promising technologies for mitigating misinformation in news. However, most existing work focus on analysis of claim sentences while overlooking crucial background attributes, such as the claimer, claim objects, and other knowledge connected to the claim. In this work, we present NewsClaims , a new benchmark for knowledge-aware claim detection in the news domain. We re-define the claim detection problem to include extraction of additional background attributes related to the claim and release 529 claims annotated over 103 news articles. In addition, NewsClaims aims to benchmark claim detection systems in emerging scenarios, comprising unseen topics with little or no training data. Finally, we provide a comprehensive evaluation of various zero-shot and prompt-based baselines for this new benchmark.},
	urldate = {2022-01-09},
	journal = {arXiv:2112.08544 [cs]},
	author = {Reddy, Revanth Gangi and Chinthakindi, Sai and Wang, Zhenhailong and Fung, Yi R. and Conger, Kathryn S. and Elsayed, Ahmed S. and Palmer, Martha and Ji, Heng},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.08544},
	file = {Reddy et al_2021_NewsClaims.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Reddy et al_2021_NewsClaims.pdf:application/pdf},
}

@inproceedings{aloshban_act_2020,
	address = {New York, NY, USA},
	series = {{WebSci} '20},
	title = {{ACT} : {Automatic} {Fake} {News} {Classification} {Through} {Self}-{Attention}},
	isbn = {978-1-4503-7989-2},
	shorttitle = {{ACT}},
	url = {https://doi.org/10.1145/3394231.3397901},
	doi = {10.1145/3394231.3397901},
	abstract = {Automatic detection of fake news is an important issue given the disproportionate effect of fake news on democratic processes, individuals and institutions. Research on automated fact-checking has proposed different approaches based on traditional machine learning methods, using hand-crafted lexical features. Nevertheless, these approaches focus on analyzing the text claim without considering the facts that are not explicitly given but can be derived from it. For example, external evidence that is retrieved from the Web as a knowledge source of the claim can provide complementary context of the claim and gives convincing reasons from it to support or oppose. Recent approaches study this deficit by incorporating supportive evidence (article) corresponding to the claim. However, these methods are either requiring substantial feature modeling, not considering several supporting evidences, or even not analyzing the language of the supporting evidence deeply. To this end, we propose an end-to-end framework, named Automatic Fake News Classification Through Self-Attention (ACT), which exploits different supportive articles to a claim which mimics manual fact-checking processes. The model presents an approach that computes the claim credibility by aggregating over the prediction generated by every claim-retrieved article pair. The article input is represented by using self-attention on the top of a bidirectional LSTM neural network. By using the self-attention, the model concentrates on nuanced linguistic features and does not require any feature engineering, lexicons or any other manual intervention. Moreover, different aspects of the supporting article are extracted into multiple vector representations. Hence, different meaningful article representations can be extracted into a two-dimensional matrix to represent the article. In the end, a majority vote over the several external articles of a given claim is applied to assess the claim’s credibility. We conduct experiments on three different real-world datasets, compare them to the state-of-the-art approaches and analyze our results, which shows performance improvements.},
	urldate = {2022-01-09},
	booktitle = {12th {ACM} {Conference} on {Web} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Aloshban, Nujud},
	month = jul,
	year = {2020},
	pages = {115--124},
	file = {Aloshban_2020_ACT.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Aloshban_2020_ACT.pdf:application/pdf},
}

@inproceedings{carlebach_news_2020,
	address = {Online},
	title = {News {Aggregation} with {Diverse} {Viewpoint} {Identification} {Using} {Neural} {Embeddings} and {Semantic} {Understanding} {Models}},
	url = {https://aclanthology.org/2020.argmining-1.7},
	abstract = {Today's news volume makes it impractical for readers to get a diverse and comprehensive view of published articles written from opposing viewpoints. We introduce a transformer-based news aggregation system, composed of topic modeling, semantic clustering, claim extraction, and textual entailment that identifies viewpoints presented in articles within a semantic cluster and classifies them into positive, neutral and negative entailments. Our novel embedded topic model using BERT-based embeddings outperforms baseline topic modeling algorithms by an 11\% relative improvement. We compare recent semantic similarity models in the context of news aggregation, evaluate transformer-based models for claim extraction on news data, and demonstrate the use of textual entailment models for diverse viewpoint identification.},
	urldate = {2022-01-09},
	booktitle = {Proceedings of the 7th {Workshop} on {Argument} {Mining}},
	publisher = {Association for Computational Linguistics},
	author = {Carlebach, Mark and Cheruvu, Ria and Walker, Brandon and Ilharco Magalhaes, Cesar and Jaume, Sylvain},
	month = dec,
	year = {2020},
	pages = {59--66},
	file = {Carlebach et al_2020_News Aggregation with Diverse Viewpoint Identification Using Neural Embeddings.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Carlebach et al_2020_News Aggregation with Diverse Viewpoint Identification Using Neural Embeddings.pdf:application/pdf},
}

@inproceedings{yang_scalable_2021,
	address = {Montpellier, France},
	title = {Scalable {Fact}-checking with {Human}-in-the-{Loop}},
	isbn = {978-1-66541-717-4},
	url = {https://ieeexplore.ieee.org/document/9648388/},
	doi = {10.1109/WIFS53200.2021.9648388},
	abstract = {Researchers have been investigating automated solutions for fact-checking in various fronts. However, current approaches often overlook the fact that information released every day is escalating, and a large amount of them overlap. Intending to accelerate fact-checking, we bridge this gap by proposing a new pipeline – grouping similar messages and summarizing them into aggregated claims. Speciﬁcally, we ﬁrst clean a set of social media posts (e.g., tweets) and build a graph of all posts based on their semantics; Then, we perform two clustering methods to group the messages for further claim summarization. We evaluate the summaries both quantitatively with ROUGE scores and qualitatively with human evaluation. We also generate a graph of summaries to verify that there is no signiﬁcant overlap among them. The results reduced 28,818 original messages to 700 summary claims, showing the potential to speed up the fact-checking process by organizing and selecting representative claims from massive disorganized and redundant messages.},
	language = {en},
	urldate = {2022-01-10},
	booktitle = {2021 {IEEE} {International} {Workshop} on {Information} {Forensics} and {Security} ({WIFS})},
	publisher = {IEEE},
	author = {Yang, Jing and Vega-Oliveros, Didier and Seibt, Tais and Rocha, Anderson},
	month = dec,
	year = {2021},
	pages = {1--6},
	file = {Yang et al. - 2021 - Scalable Fact-checking with Human-in-the-Loop.pdf:C\:\\Users\\calm3\\Zotero\\storage\\5LSI387L\\Yang et al. - 2021 - Scalable Fact-checking with Human-in-the-Loop.pdf:application/pdf},
}
