
@article{karimi_news_2018,
  title        = {News recommender systems – Survey and roads ahead},
  volume       = {54},
  issn         = {0306-4573},
  url          = {https://www.sciencedirect.com/science/article/pii/S030645731730153X},
  doi          = {10.1016/j.ipm.2018.04.008},
  abstract     = {More and more people read the news online, e.g., by visiting the websites of their favorite newspapers or by navigating the sites of news aggregators. However, the abundance of news information that is published online every day through different channels can make it challenging for readers to locate the content they are interested in. The goal of News Recommender Systems ({NRS}) is to make reading suggestions to users in a personalized way. Due to their practical relevance, a variety of technical approaches to build such systems have been proposed over the last two decades. In this work, we review the state-of-the-art of designing and evaluating news recommender systems over the last ten years. One main goal of the work is to analyze which particular challenges of news recommendation (e.g., short item life times and recency aspects) have been well explored and which areas still require more work. Furthermore, in contrast to previous surveys, the paper specifically discusses methodological questions and today’s academic practice of evaluating and comparing different algorithmic news recommendation approaches based on accuracy measures.},
  pages        = {1203--1227},
  number       = {6},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  author       = {Karimi, Mozhgan and Jannach, Dietmar and Jugovac, Michael},
  urldate      = {2021-06-25},
  date         = {2018-11},
  langid       = {english},
  file         = {Karimi et al_2018_News recommender systems – Survey and roads ahead.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Karimi et al_2018_News recommender systems – Survey and roads ahead.pdf:application/pdf}
}

@misc{2021_world_press_freedom_index,
	title = {2021 {World} {Press} {Freedom} {Index}: {Journalism}, the vaccine against disinformation, blocked in more than 130 countries},
	shorttitle = {2021 {World} {Press} {Freedom} {Index}},
    author = {RSF},
	url = {https://rsf.org/en/2021-world-press-freedom-index-journalism-vaccine-against-disinformation-blocked-more-130-countries},
	abstract = {Читать на русском / Read in Russian This year’s Index, which evaluates the press freedom situation in 180 countries and territories annually, shows that journalism, journalism, which is arguably the best vaccine against the virus of disinformation, is totally blocked or seriously impeded in 73 countries and constrained in 59 others, which together represent 73\% of the countries evaluated. These countries are classified as having “very bad,” “bad” or “problematic” environments for press freedom, and are identified accordingly in black, red or orange on the World Press Freedom map.},
	language = {en},
	urldate = {2021-07-04},
	journal = {RSF},
    note = {\url{https://rsf.org/en/2021-world-press-freedom-index-journalism-vaccine-against-disinformation-blocked-more-130-countries} (2021年7月19日参照)}
}
@comment{
%% 	month = apr,
%% 	year = {2021},
}

@inproceedings{tian_labeled_2018,
	title = {Labeled {Bilingual} {Topic} {Model} for {Cross}-{Lingual} {Text} {Classification} and {Label} {Recommendation}},
	doi = {10.1109/ICISCE.2018.00067},
	abstract = {Aiming at the increasingly rich multi language information resources and multi-label data in news reports and scientific literatures, in order to mining the relevance between languages and the correlation between data, this paper proposed labeled bilingual topic model, applied on cross-lingual text classification and label recommendation. First of all, it could assume that the keywords in the scientific literature are relevant to the abstract in same article, then extracted the keywords and regarded it as labels, and aligned the labels with topics in topic model, instantiated the “latent” topic. Secondly, trained the abstracts in article through the topic model proposed by this paper. Finally, classified the new documents by cross-lingual text classifier, also recommended the labels. The experiment result show that Micro-F1 measure reaches 94.81\% in cross-lingual text classification task, and the recommended labels also reflects the sematic relevance with documents.},
	booktitle = {2018 5th {International} {Conference} on {Information} {Science} and {Control} {Engineering} ({ICISCE})},
	author = {Tian, Ming-Jie and Huang, Zheng-Hao and Cui, Rong-Yi},
	month = jul,
	year = {2018},
	keywords = {topic model, cross-lingual text classification, Data models, Dictionaries, label, label recommendation, latent topic, Probabilistic logic, Probability, Semantics, Task analysis, Text categorization},
	pages = {285--289},
	file = {Tian et al. - 2018 - Labeled Bilingual Topic Model for Cross-Lingual Te.pdf:C\:\\Users\\calm3\\Zotero\\storage\\J5YY896G\\Tian et al. - 2018 - Labeled Bilingual Topic Model for Cross-Lingual Te.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\calm3\\Zotero\\storage\\RCWFIPXW\\8612565.html:text/html},
}

@article{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {https://arxiv.org/abs/1908.10084v1},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	language = {en},
	urldate = {2021-07-15},
	author = {Reimers, Nils and Gurevych, Iryna},
	file = {Reimers_Gurevych_2019_Sentence-BERT.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Reimers_Gurevych_2019_Sentence-BERT.pdf:application/pdf},
    note = {\url{https://arxiv.org/abs/1908.10084v1} (2021年7月19日参照)}
}
@comment{
	month = aug,
	year = {2019},
}


@misc{japanese_fakenews_dataset,
    author = {坂本 俊之},
	title = {Japanese {FakeNews} {Dataset}},
	url = {https://kaggle.com/tanreinama/japanese-fakenews-dataset},
	abstract = {This dataset consists of news articles and deep fake articles in Japanese.},
	language = {en},
	urldate = {2021-07-19},
    note = {\url{https://www.kaggle.com/tanreinama/japanese-fakenews-dataset} (2021年7月19日参照)}
}
@comment{
}

@misc{debater_datasets,
	type = {{CT002}},
    author = {IBM Corporation},
	title = {Project Debater Datasets},
	copyright = {© Copyright IBM Corp. 2011},
	url = {http://www.research.ibm.com/labs/haifa/},
	abstract = {IBM Research - Haifa is the largest lab of IBM Research Division outside of the United States. Founded as a small scientific center in 1972, it grew into a major lab that leads the development of innovative technological products and solutions for the IBM corporation.},
	language = {en-US},
	urldate = {2021-07-19},
    note = {\url{https://www.research.ibm.com/haifa/dept/vst/debating_data.shtml} (2021年7月19日参照)}
}
@comment{
	title = {{IBM} {Research}},
	note = {Publisher: IBM Corporation},
	month = feb,
	year = {2017},
}

@article{blei_latent_nodate,
	title = {Latent {Dirichlet} {Allocation}},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	language = {en},
	author = {Blei, David M},
	pages = {30},
	file = {Blei - Latent Dirichlet Allocation.pdf:C\:\\Users\\calm3\\Zotero\\storage\\P8CUU5P6\\Blei - Latent Dirichlet Allocation.pdf:application/pdf},
}
