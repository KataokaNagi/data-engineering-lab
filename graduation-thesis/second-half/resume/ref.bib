
@misc{2021_world_press_freedom_index,
  title      = {2021 {World} {Press} {Freedom} {Index}: {Journalism}, the vaccine against disinformation, blocked in more than 130 countries},
  shorttitle = {2021 {World} {Press} {Freedom} {Index}},
  author     = {RSF},
  url        = {https://rsf.org/en/2021-world-press-freedom-index-journalism-vaccine-against-disinformation-blocked-more-130-countries},
  abstract   = {Читать на русском / Read in Russian This year’s Index, which evaluates the press freedom situation in 180 countries and territories annually, shows that journalism, journalism, which is arguably the best vaccine against the virus of disinformation, is totally blocked or seriously impeded in 73 countries and constrained in 59 others, which together represent 73\% of the countries evaluated. These countries are classified as having “very bad,” “bad” or “problematic” environments for press freedom, and are identified accordingly in black, red or orange on the World Press Freedom map.},
  language   = {en},
  urldate    = {2022-01-12},
  journal    = {RSF},
  note       = {\url{https://rsf.org/en/2021-world-press-freedom-index-journalism-vaccine-against-disinformation-blocked-more-130-countries} (2022年1月12日参照)}
}

@inproceedings{yang_scalable_2021,
  address   = {Montpellier, France},
  title     = {Scalable {Fact}-checking with {Human}-in-the-{Loop}},
  isbn      = {978-1-66541-717-4},
  url       = {https://ieeexplore.ieee.org/document/9648388/},
  doi       = {10.1109/WIFS53200.2021.9648388},
  abstract  = {Researchers have been investigating automated solutions for fact-checking in various fronts. However, current approaches often overlook the fact that information released every day is escalating, and a large amount of them overlap. Intending to accelerate fact-checking, we bridge this gap by proposing a new pipeline – grouping similar messages and summarizing them into aggregated claims. Speciﬁcally, we ﬁrst clean a set of social media posts (e.g., tweets) and build a graph of all posts based on their semantics; Then, we perform two clustering methods to group the messages for further claim summarization. We evaluate the summaries both quantitatively with ROUGE scores and qualitatively with human evaluation. We also generate a graph of summaries to verify that there is no signiﬁcant overlap among them. The results reduced 28,818 original messages to 700 summary claims, showing the potential to speed up the fact-checking process by organizing and selecting representative claims from massive disorganized and redundant messages.},
  language  = {en},
  urldate   = {2022-01-10},
  booktitle = {2021 {IEEE} {International} {Workshop} on {Information} {Forensics} and {Security} ({WIFS})},
  publisher = {IEEE},
  author    = {Yang, Jing and Vega-Oliveros, Didier and Seibt, Tais and Rocha, Anderson},
  month     = dec,
  year      = {2021},
  pages     = {1--6},
  file      = {Yang et al. - 2021 - Scalable Fact-checking with Human-in-the-Loop.pdf:C\:\\Users\\calm3\\Zotero\\storage\\5LSI387L\\Yang et al. - 2021 - Scalable Fact-checking with Human-in-the-Loop.pdf:application/pdf}
}

@article{ghasiya_investigating_2021,
  title      = {Investigating {COVID}-19 {News} {Across} {Four} {Nations}: {A} {Topic} {Modeling} and {Sentiment} {Analysis} {Approach}},
  volume     = {9},
  issn       = {2169-3536},
  shorttitle = {Investigating {COVID}-19 {News} {Across} {Four} {Nations}},
  doi        = {10.1109/ACCESS.2021.3062875},
  abstract   = {Newspapers are very important for a society as they inform citizens about the events around them and how they can impact their life. Their importance becomes more crucial and indispensable in the times of health crisis such as the current COVID-19 pandemic. Since the starting of this pandemic newspapers are providing rich information to the public about various issues such as the discovery of a new strain of coronavirus, lockdown and other restrictions, government policies, and information related to the vaccine development for the same. In this scenario, analysis of emergent and widely reported topics/themes/issues and associated sentiments from various countries can help us better understand the COVID-19 pandemic. In our research, the database of more than 100,000 COVID-19 news headlines and articles were analyzed using top2vec (for topic modeling) and RoBERTa (for sentiment classification and analysis). Our topic modeling results highlighted that education, economy, US, and sports are some of the most common and widely reported themes across UK, India, Japan, South Korea. Further, our sentiment classification model achieved 90\% validation accuracy and the analysis showed that the worst affected country, i.e. the UK (in our dataset) also has the highest percentage of negative sentiment.},
  journal    = {IEEE Access},
  author     = {Ghasiya, Piyush and Okamura, Koji},
  year       = {2021},
  note       = {Conference Name: IEEE Access},
  pages      = {36645--36656},
  file       = {Ghasiya_Okamura_2021_Investigating COVID-19 News Across Four Nations.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Ghasiya_Okamura_2021_Investigating COVID-19 News Across Four Nations.pdf:application/pdf}
}

@article{liu_roberta_2019,
  title      = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
  shorttitle = {{RoBERTa}},
  url        = {http://arxiv.org/abs/1907.11692},
  abstract   = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  urldate    = {2021-07-25},
  journal    = {arXiv:1907.11692 [cs]},
  author     = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  month      = jul,
  year       = {2019},
  note       = {arXiv: 1907.11692},
  file       = {Liu et al_2019_RoBERTa.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Liu et al_2019_RoBERTa.pdf:application/pdf}
}

@inproceedings{rinott_show_2015,
  address   = {Lisbon, Portugal},
  title     = {Show {Me} {Your} {Evidence} - an {Automatic} {Method} for {Context} {Dependent} {Evidence} {Detection}},
  url       = {http://aclweb.org/anthology/D15-1050},
  doi       = {10.18653/v1/D15-1050},
  abstract  = {Engaging in a debate with oneself or others to take decisions is an integral part of our day-today life. A debate on a topic (say, use of performance enhancing drugs) typically proceeds by one party making an assertion/claim (say, PEDs are bad for health) and then providing an evidence to support the claim (say, a 2006 study shows that PEDs have psychiatric side effects). In this work, we propose the task of automatically detecting such evidences from unstructured text that support a given claim. This task has many practical applications in decision support and persuasion enhancement in a wide range of domains. We ﬁrst introduce an extensive benchmark data set tailored for this task, which allows training statistical models and assessing their performance. Then, we suggest a system architecture based on supervised learning to address the evidence detection task. Finally, promising experimental results are reported.},
  language  = {en},
  urldate   = {2021-12-10},
  booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Rinott, Ruty and Dankin, Lena and Alzate Perez, Carlos and Khapra, Mitesh M. and Aharoni, Ehud and Slonim, Noam},
  year      = {2015},
  pages     = {440--450},
  file      = {Evidence2015.pdf:C\:\\Users\\calm3\\Zotero\\storage\\QBED8NEW\\Evidence2015.pdf:application/pdf}
}

@article{qi_stanza_2020,
  title      = {Stanza: {A} {Python} {Natural} {Language} {Processing} {Toolkit} for {Many} {Human} {Languages}},
  shorttitle = {Stanza},
  url        = {http://arxiv.org/abs/2003.07082},
  abstract   = {We introduce Sta n z a , an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Sta n z a features a language-agnostic fully neural pipeline for text analysis, including tokenization, multiword token expansion, lemmatization, part-ofspeech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Sta n z a on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Sta n z a includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https:// stanfordnlp.github.io/stanza/.},
  language   = {en},
  urldate    = {2021-11-09},
  journal    = {arXiv:2003.07082 [cs]},
  author     = {Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
  month      = apr,
  year       = {2020},
  note       = {arXiv: 2003.07082},
  file       = {Qi et al. - 2020 - Stanza A Python Natural Language Processing Toolk.pdf:C\:\\Users\\calm3\\Zotero\\storage\\EEYCCDGN\\Qi et al. - 2020 - Stanza A Python Natural Language Processing Toolk.pdf:application/pdf}
}

@article{reimers_sentence-bert_2019,
  title      = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
  shorttitle = {Sentence-{BERT}},
  url        = {https://arxiv.org/abs/1908.10084v1},
  abstract   = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  language   = {en},
  urldate    = {2021-07-15},
  author     = {Reimers, Nils and Gurevych, Iryna},
  file       = {Reimers_Gurevych_2019_Sentence-BERT.pdf:C\:\\Users\\calm3\\Dropbox\\zotero-sync\\Reimers_Gurevych_2019_Sentence-BERT.pdf:application/pdf},
  note       = {\url{https://arxiv.org/abs/1908.10084v1} (2021年7月19日参照)}
}


@article{chicco_advantages_2020,
  title    = {The advantages of the {Matthews} correlation coefficient ({MCC}) over {F1} score and accuracy in binary classification evaluation},
  volume   = {21},
  issn     = {1471-2164},
  url      = {https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7},
  doi      = {10.1186/s12864-019-6413-7},
  abstract = {Background: To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.
              Results: The Matthews correlation coefficient (MCC), instead, is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset.
              Conclusions: In this article, we show how MCC produces a more informative and truthful score in evaluating binary classifications than accuracy and F1 score, by first explaining the mathematical properties, and then the asset of MCC in six synthetic use cases and in a real genomics scenario. We believe that the Matthews correlation coefficient should be preferred to accuracy and F1 score in evaluating binary classification tasks by all scientific communities.},
  language = {en},
  number   = {1},
  urldate  = {2022-01-17},
  journal  = {BMC Genomics},
  author   = {Chicco, Davide and Jurman, Giuseppe},
  month    = dec,
  year     = {2020},
  pages    = {6},
  file     = {Chicco and Jurman - 2020 - The advantages of the Matthews correlation coeffic.pdf:C\:\\Users\\calm3\\Zotero\\storage\\87UELQ7N\\Chicco and Jurman - 2020 - The advantages of the Matthews correlation coeffic.pdf:application/pdf}
}
