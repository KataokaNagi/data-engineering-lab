<!-- tex script for md -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 概要チェック 議事録 2021年07月15日

## 報告

### 片岡
- お聞きしたい事
    - 〇〇らは、～の形式
        - 手法が有名ならドンと書く
        - 敬意
    - 記事は分類の方が適切？
        - 教師が作れない？
            - 類似したと
        - Transformerは分類が多い
        - LSTMは下位互換？
            - 簡単で拡張しやすい？
    - 日本語か独か仏か
        - 考える
        - ベトナム、タイ、フランス、アラビアは文構造を少し聞ける
            - 友達に聞く
    - スライドで概要書以上の実験を話してよいのか
        - OK
    - 手法が先に来る説明になっていたかも
    - モデルの改良
        - Claim-Evidenceの対応
        - SCDVの組み込み
        - IBMのディベートの学習
            - Project Debater Dataset
            - Argument Mining
        - 人狼知能
    - 予備実験のやめどき
        - 学習量
        - 学習の勾配
        - 過学習
            - Accuracyが大きくなり過ぎたとき
            - ロスが小さくなりすぎたり
            - アーリーストッピング
- 概要書
    - Transformer
    - ~~測ることができない~~
        - 本文丸ごと解析してしまう
        - （主張をクリティカルに）
        - 文中で分かれていない
    - 二重引用符は苦しい
        - Debeat
    - 機械翻訳が後ろに行き過ぎ
    - 機械翻訳何使うか
    - Transformerの分類器がどのようなシステムになっているかを調べる
    - 分類
        - セルフアテンション
        - 学習してエンコーダ部分のみ
        - p.545
    - なぜ
        - LSTMが複雑
        - 学習時間がかかる
        - 線形代数で並列計算可能
    - SBERTは何が5秒
        - コサイン類似度の前のベクター生成
        - コサイン類維持度は行列計算なので高速
    - チャットボット
    - SBERTはTransformerの形をしていないのでなおさらTransformerと言ってはいけない
        - 左右対称な埋め込み、プーリング、正規化、ソフトマックス
        - 何を表現している？
        - 普通のBERTとどう違う
            - 違わない、使いまわし
    - BERTの仕組み
        - Transformerのエンコーダ部分のみを使っている
    - Transformerの説明が薄い
        - 機構
        - エンコーダだけ使う旨
    - 階層的クラスタリングの後、分割型のクラスタリング
    - サブセット
    - データセット
        - コピーライトのランクを見ておく
    - もし予備実験ができないなら
        - 
        - Transformerの方が良いはず

### 亀川
- otherwiseなどの式はカンマとピリオドが必要

### 志田
- 

### 土屋
- 

### 平山
- 

### 増岡
- 

### 松本
- Transformer
    - 単語埋め込みに何を使っているか
        - 単語のID列
    - ジェネレータとディスクリミネータは似たものが良い？
- 予備実験
    - エポック数を明記
    - 図はトリミングして点々
    - ~~予備~~実験では
    - 多様性 -> タイトルらしさ

### モ
- 

### 原田
- 

## 気になったこと
- 

## 業務連絡
- 

