# scikit-learn、Keras、TensorFlowによる実践機械学習 第2版 第10章 読書メモ
- p.279- 第10章 人工ニューラルネットワークとKerasの初歩
    - ニューロン=ユニット
        - 生物学にとどめない
    - ANN
    - MLP
    - Keras API

## 目次

- [10.1 生物学的なニューロンから人工ニューロンへ](#101-生物学的なニューロンから人工ニューロンへ)（片岡担当）        
    - [10.1.1 生物学的ニューロン](#1011-生物学的ニューロン)        
    - [10.1.2 ニューロンによる論理演算](#1012-ニューロンによる論理演算)        
    - [10.1.3 パーセプトロン](#1013-パーセプトロン)        
    - [10.1.4 MLPとバックプロバケーション](#1014-mlpとバックプロバケーション)        
    - [10.1.5 回帰MLP](#1015-回帰mlp)        
    - [10.1.6 分類MLP](#1016-分類mlp)    
- [10.2 KerasによるMLPの実装](#102-kerasによるmlpの実装)        
    - [10.2.1 TensorFlow2のインストール](#1021-tensorflow2のインストール)        
    - [10.2.2 シーケンシャルAPIを使った画像分類器の構築](#1022-シーケンシャルapiを使った画像分類器の構築)        
    - [10.2.3 シーケンシャルAPIを使った回帰MLPの構築](#1023-シーケンシャルapiを使った回帰mlpの構築)        
    - [10.2.4 関数型APIを使った複雑なモデルの構築](#1024-関数型apiを使った複雑なモデルの構築)        
    - [10.2.5 サブクラス化APIを使ったダイナミックなモデルの構築](#1025-サブクラス化apiを使ったダイナミックなモデルの構築)        
    - [10.2.6 モデルの保存と復元](#1026-モデルの保存と復元)        
    - [10.2.7 コールバックの使い方](#1027-コールバックの使い方)        
    - [10.2.8 TensorBoardを使った可視化](#1028-tensorboardを使った可視化)    
- [10.3 NNのハイパラの微調整](#103-nnのハイパラの微調整)（片岡担当）        
    - [10.3.1 隠れ層の数](#1031-隠れ層の数)        
    - [10.3.2 隠れ層あたりのニューロン数](#1032-隠れ層あたりのニューロン数)        
    - [10.3.3 学習率、バッチサイズ、その他のハイパラ](#1033-学習率バッチサイズその他のハイパラ)    
- [10.4 演習問題](#104-演習問題) （片岡担当）

## 10.1 生物学的なニューロンから人工ニューロンへ
歴史2
- 1943-
    - ANN
    - 命題論理
- 1960s
    - コネクショニズム
- 1990
    - SVM

歴史2
- 近年
    - 再びANN？
        - 膨大なデータ
        - ムーアの法則
        - アルゴリズムの改良
        - 局所的な最適値は意外と無害
        - 流行りと資金

### 10.1.1 生物学的ニューロン
BNNの構造
-  細胞体
-  樹状突起
-  軸索
    -  細胞体の数倍から数万倍の長さ
-  終末分岐
-  シナプス終端

BNNの挙動
- 活動電位（AP）か信号
- シナプスからの神経伝達物質
- 発火
- 

### 10.1.2 ニューロンによる論理演算
ニューロン
- 複数のバイナリ入力
    - 2つの1で1
- 1つのバイナリ出力
    - 分岐も可能だが両方同じ値
- 任意の論理命題に対応
    - 恒等写像
    - AND
    - OR
    - NOT
        - 1によって出力を禁止する機構
        - 常に1であるニューロンと組み合わせる

### 10.1.3 パーセプトロン
パーセプトロン
- ANNアーキテクチャ
- TLU（Threshold Logic Unit：閾値論理素子）のNW
    - 別名 LTU（Linear Threshold Unit：線形閾値素子）
    - 人工ニューロン
    - 入力の加重総和 x^T w をステップ関数（後述）に代入
    - 単体でもNNのように機能
- 接続重み

ステップ関数
- ヘヴィサイドステップ関数
    - heaviside(z)
    - 0以上で1
- 符号関数
    - sgn(z)
    - 0は0
    - 他は符号付きの1

全結合層（密層）
- 前の層の全てのニューロンが次の全てのニューロンに接続
- パーセプトロンも全結合層
    - 入力層には恒等写像の入力ニューロン
    - 1を出力するバイアスニューロン
- 出力 h_{w, b} (x) = φ(xw + b)
    - x：インスタンスごとに1行、特徴量ごとに1列
    - w：入力ニューロンごとに1行、直近の層のニューロンあたりに1列
    - b：バイアスベクトル
    - φ：ステップ関数などの活性化関数

パーセプトロンの訓練
- ヘッブの法則（ヘッブ学習）
    - 同時に発火するニューロン同士の重みを強化
- 誤った予測をしたニューロンの重みを上げる
    - w_{i,j}^{next} = w__{i,j} + η(y_j - y^_j)x_i ???
- パーセプトロンの収束定理
    - 境界が線形なので複雑なパターンには向かないが
    - 訓練インスタンスが線形分離可能なら解が収束

- 実装
    - scikit-learnのPerceptionクラス
    - 確率的勾配降下法の特別なケースと等価
    - ハード投票分類をするだけ
        - 排他的ORのような簡単な問題も解けない
        - 確率を出力するロジスティック回帰の方が優れている
        - MLP（Multi-layer Perceptron：多層パーセプトロン）で解決 

MLPのXOR
- 図で確認

### 10.1.4 MLPとバックプロバケーション
MLP
- 入力層、複数のTLU層、出力層
- 出力層に近いほど上位層、遠いほど下位層と呼ばれる
    - 一方通行なアーキテクチャをFNN（Feedforward NN：順伝搬型NN）
- 出力層以外にバイアスニューロン
    - 通常は省略される

DNN
- ANNが深い隠れ層をもったもの
    - 層の数は曖昧になってきている
- 深層学習という分野で研究されるもの
- 1986 バックプロバケーション（誤差逆電波法）で再燃
    - 勾配降下法の自動化（リバースモードの自動微分）
        - 高速
        - 正確
        - 微分対象が様々な変数を持ち、出力が少ない場合に最適
    - 往復1回で重みの誤差の勾配がわかる

バックプロバケーション
- 1度に1個のミニバッチ（複数インスタンス）を処理（1エポック）
- 手順
    - 全ての接続重みを無作為に初期化
        - 対称性のある初期化をすると、ニューロンが少ないNNのように表現力が下がる
    - 前進パス
        - 処理結果を記録しつつ下位層から順に処理
    - 後退パス
        - 出力誤差をコスト関数で評価
        - 連鎖律で個々の出力接続部の誤差を計算
    - 勾配降下ステップ
        - 誤差が小さくなるように接続重みを調節

バックプロバケーションのための改良
- 活性化関数の改良
    - ステップ関数（カクカク） -> ロジスティック関数（滑らか）
        - 連鎖律のために勾配が取れるように
        - 生物学的ニューロンに近しい
    - 双曲線正接
        - tanh(z) = 2σ(2z)-１
        - -1 < tanh(z) < 1 のため、出力が0を中心として散らばり収束が早まる
    - ReLU関数
        - ReLU(z) = max(0, z)
        - 勾配が z = 0 で発散し、z < 0 で0となるが、高速で最も利用される
        - 出力の最大値がないため、勾配消失問題を解決する（11章で詳解）
    - そもそも活性化関数は、線形関数の組み合わせで線形問題しか解けなかった問題を解消するための非線形関数


### 10.1.5 回帰MLP
MLPによる回帰タスク
- 予測の数 = 出力ニューロンの数
- 出力ニューロンには活性化関数は用いないことが多い
    - 回帰では任意の値を出力させるため
- 正の数に限定するのであればReLU関数やsoftplus関数を用いることも
    - softplus(z) = ln(1 + exp(z)) 
- 予測値の範囲を制限するには、ロジスティック関数や双極正接関数に通してスケーリング

回帰MLPのコスト関数
- 平均二乗誤差
    - デファクトスタンダード
- 平均絶対誤差
    - 2乗しないので外れ値に強い
- フーバー損失関数
    - 誤差が閾値（一般に1）以下のとき
        - 平均二乗誤差に近い、高速で正確な式
    - 誤差が閾値より大きいのとき
        - 平均絶対誤差に近い、外れ値に近い式
    - https://qiita.com/Hatomugi/items/d00c1a7df07e0e3925a8

### 10.1.6 分類MLP
- MLPによる分類タスク
    - 表を挿入
    - ロジスティック関数は合計が1とは限らない
        - 重複があるため
    - ソフトマックス関数は合計が1になる
        - 重複がないため
    - 例
    - 二項分類
        - 0～1のロジスティック活性化関数で推定確率を表現
    - 多クラス分類

## 10.2 KerasによるMLPの実装
- 

### 10.2.1 TensorFlow2のインストール
- 

### 10.2.2 シーケンシャルAPIを使った画像分類器の構築
- 

### 10.2.3 シーケンシャルAPIを使った回帰MLPの構築
- 

### 10.2.4 関数型APIを使った複雑なモデルの構築
- 

### 10.2.5 サブクラス化APIを使ったダイナミックなモデルの構築
- 

### 10.2.6 モデルの保存と復元
- 

### 10.2.7 コールバックの使い方
- 

### 10.2.8 TensorBoardを使った可視化
- 

## 10.3 NNのハイパラの微調整
単純なMLPの調整要素
- 層の数
- 層ごとのニューロン
- 活性化関数
- 重みの初期かロジック
- etc.

ANNの調整の方法1
- 手動で調節して交差検証で良いものを選択
    - scikit-learn風に使うためにラップ
    - 損失ではなくスコアを用いることに注意
    - ハイパーパラメータが多いのでグリッドサーチではなくランダムサーチで一部を評価
    - コード
- 広範囲のハイパーパラメータ->最良のハイパーパラメータ付近
    - 時間がかかる
- ある領域が良いとわかったらズームイン

ズームインするライブラリ1
    - Hyperopt
        - あらゆるタイプの複雑な探索空間に対応する広く使われるライブラリ
    - Hyperas
        - Kerasの最適化ライブラリ
    - kopt
        - Kerasの最適化ライブラリ
    - Talos
        - Kerasの最適化ライブラリ
    - Keras Tuner
        - Kerasの最適化ライブラリ
        - 可視化と解析も可能

ズームインするライブラリ2
    - Scikit-Optimize
        - GridSearchCVクラスと似たベイズ最適化が可能
    - Spearmint
        - ベイズ最適化ライブラリ
    - Hyperband
        - 高速
    - Sklearn
        - 進化的アルゴリズム
        - GridSearchCV風
- 
ズームインするツール
- Google Cloud API のハイパーパラメータ調整サービス
- Arimo
- SigOpt
- CallDeskのOscar
- DeepMind (2017) "Population Based Training of Neural Networks"
- GoogleのAutoMLスイート

### 10.3.1 隠れ層の数
- 浅いNN
    - 複雑でなければ隠れ層は1つで十分なことが多い
    - MNISTで97%
- 深いNW
    - 森を描くときに枝葉や木をコピペできるようなイメージ
    - 指数的に少ないニューロンで済む
    - 収束が早い
    - 汎化能力が高い
        - 髪の訓練に頭部の分類器の下位層を使いまわせる
        - 転移学習という
    - 過学習の手前まで層を増やす
- 

### 10.3.2 隠れ層あたりのニューロン数
層ごとのニューロン数
- 上位層に向けて萎むパターン
    - 従来手法
- 全ての層で同じ数にするパターン
    - 多くの場合、性能は同じか向上
    - ハイパーパラメータがn層分から1つに統一可能
- 過学習の手前まで数を増やすパターン
- ストレッチパンツアプローチ
    - - 必要以上に多く取り、早期打ち切りや正則化で過学習対策
    - ボトルネックとなる層が発生しない
- ニューロン数より層の数を増やした方が効果は得やすい

### 10.3.3 学習率、バッチサイズ、その他のハイパラ
MLPの重要なハイパーパラメータ1
- 学習率
    - 一般に、出力値が発散し始める値の半分が最適
    - 実装例：10^-5から数百回イテレートして10へ
    - 対数軸を取って損失をグラフ化する
    - 一般に、折り返しの1/10手前が最適
        - 過学習を恐れて？
- オプティマイザ
    - ミニバッチ勾配降下法の代替法
    - 11章で詳解

MLPの重要なハイパーパラメータ2
- バッチサイズ
    - 大きいほどGPUが同時に沢山処理
    - RAMの制限を受ける
    - 大きすぎると訓練初期に不安定＆汎化しない
        - 2～32が推奨されたりされなかったり
        - 段々と大きくしていく手法も存在
- 活性化関数
    - 出力層以外はReLU関数でよい
- イテレーション数
    - ほぼ操作することはないが、早期打ち切りで意識することがある
    - 
NNのハイパーパラメータ調整のベストプラクティス
- Leslie N. Smith (2018) "Accurate, Large Minibatch SDG: Training ImageNet in 1 Hour"
    - 

## 10.4 演習問題
1. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
2. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
3. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
4. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
5. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
6. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
7. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
8. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
9. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
- 

10. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
11. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
12. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
13. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
14. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
15. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
16. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
17. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
18. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
19. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
20. 
    - 片岡の解答
        - 
    - 本書の解答
        - 
