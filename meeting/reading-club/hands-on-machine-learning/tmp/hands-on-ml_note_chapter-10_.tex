\hypertarget{scikit-learnkerastensorflowux306bux3088ux308bux5b9fux8df5ux6a5fux68b0ux5b66ux7fd2-ux7b2c2ux7248-ux7b2c10ux7ae0-ux8aadux66f8ux30e1ux30e2}{%
\section{scikit-learn、Keras、TensorFlowによる実践機械学習 第2版 第10章
読書メモ}\label{scikit-learnkerastensorflowux306bux3088ux308bux5b9fux8df5ux6a5fux68b0ux5b66ux7fd2-ux7b2c2ux7248-ux7b2c10ux7ae0-ux8aadux66f8ux30e1ux30e2}}

\begin{itemize}
\tightlist
\item
  p.279 第10章 人工ニューラルネットワークとKerasの初歩

  \begin{itemize}
  \tightlist
  \item
    ニューロン=ユニット

    \begin{itemize}
    \tightlist
    \item
      生物学にとどめない
    \end{itemize}
  \item
    ANN
  \item
    MLP
  \item
    Keras API
  \end{itemize}
\end{itemize}

\hypertarget{ux751fux7269ux5b66ux7684ux306aux30cbux30e5ux30fcux30edux30f3ux304bux3089ux4ebaux5de5ux30cbux30e5ux30fcux30edux30f3ux3078}{%
\section{10.1
生物学的なニューロンから人工ニューロンへ}\label{ux751fux7269ux5b66ux7684ux306aux30cbux30e5ux30fcux30edux30f3ux304bux3089ux4ebaux5de5ux30cbux30e5ux30fcux30edux30f3ux3078}}

\begin{itemize}
\tightlist
\item
  歴史2
\item
  1943

  \begin{itemize}
  \tightlist
  \item
    ANN
  \item
    命題論理
  \end{itemize}
\item
  1960s

  \begin{itemize}
  \tightlist
  \item
    コネクショニズム
  \end{itemize}
\item
  1990

  \begin{itemize}
  \tightlist
  \item
    SVM
  \end{itemize}
\item
  歴史2
\item
  近年

  \begin{itemize}
  \tightlist
  \item
    再びANN？

    \begin{itemize}
    \tightlist
    \item
      膨大なデータ
    \item
      ムーアの法則
    \item
      アルゴリズムの改良
    \item
      局所的な最適値は意外と無害
    \item
      流行りと資金
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{ux751fux7269ux5b66ux7684ux30cbux30e5ux30fcux30edux30f3}{%
\subsection{10.1.1
生物学的ニューロン}\label{ux751fux7269ux5b66ux7684ux30cbux30e5ux30fcux30edux30f3}}

\begin{itemize}
\item
  BNNの構造
\item
  細胞体
\item
  樹状突起
\item
  軸索

  \begin{itemize}
  \tightlist
  \item
    細胞体の数倍から数万倍の長さ
  \end{itemize}
\item
  終末分岐
\item
  シナプス終端
\item
  BNNの挙動
\item
  活動電位（AP）か信号
\item
  シナプスからの神経伝達物質
\item
  発火
\end{itemize}

\hypertarget{ux30cbux30e5ux30fcux30edux30f3ux306bux3088ux308bux8ad6ux7406ux6f14ux7b97}{%
\subsection{10.1.2
ニューロンによる論理演算}\label{ux30cbux30e5ux30fcux30edux30f3ux306bux3088ux308bux8ad6ux7406ux6f14ux7b97}}

\begin{itemize}
\tightlist
\item
  ニューロン
\item
  複数のバイナリ入力

  \begin{itemize}
  \tightlist
  \item
    2つの1で1
  \end{itemize}
\item
  1つのバイナリ出力

  \begin{itemize}
  \tightlist
  \item
    分岐も可能だが両方同じ値
  \end{itemize}
\item
  任意の論理命題に対応

  \begin{itemize}
  \tightlist
  \item
    恒等写像
  \item
    AND
  \item
    OR
  \item
    NOT

    \begin{itemize}
    \tightlist
    \item
      1によって出力を禁止する機構
    \item
      常に1であるニューロンと組み合わせる
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{ux30d1ux30fcux30bbux30d7ux30c8ux30edux30f3}{%
\subsection{10.1.3
パーセプトロン}\label{ux30d1ux30fcux30bbux30d7ux30c8ux30edux30f3}}

\begin{itemize}
\item
  パーセプトロン
\item
  ANNアーキテクチャ
\item
  TLU（Threshold Logic Unit：閾値論理素子）のNW

  \begin{itemize}
  \tightlist
  \item
    別名 LTU（Linear Threshold Unit：線形閾値素子）
  \item
    人工ニューロン
  \item
    入力の加重総和 x\^{}T w をステップ関数（後述）に代入
  \item
    単体でもNNのように機能
  \end{itemize}
\item
  接続重み
\item
  ステップ関数
\item
  ヘヴィサイドステップ関数

  \begin{itemize}
  \tightlist
  \item
    heaviside(z)
  \item
    0以上で1
  \end{itemize}
\item
  符号関数

  \begin{itemize}
  \tightlist
  \item
    sgn(z)
  \item
    0は0
  \item
    他は符号付きの1
  \end{itemize}
\item
  全結合層（密層）
\item
  前の層の全てのニューロンが次の全てのニューロンに接続
\item
  パーセプトロンも全結合層

  \begin{itemize}
  \tightlist
  \item
    入力層には恒等写像の入力ニューロン
  \item
    1を出力するバイアスニューロン
  \end{itemize}
\item
  出力 h\_\{w, b\} (x) = φ(xw + b)

  \begin{itemize}
  \tightlist
  \item
    x：インスタンスごとに1行、特徴量ごとに1列
  \item
    w：入力ニューロンごとに1行、直近の層のニューロンあたりに1列
  \item
    b：バイアスベクトル
  \item
    φ：ステップ関数などの活性化関数
  \end{itemize}
\item
  パーセプトロンの訓練
\item
  ヘッブの法則（ヘッブ学習）

  \begin{itemize}
  \tightlist
  \item
    同時に発火するニューロン同士の重みを強化
  \end{itemize}
\item
  誤った予測をしたニューロンの重みを上げる

  \begin{itemize}
  \tightlist
  \item
    w\_\{i,j\}\^{}\{next\} = w\_\_\{i,j\} + η(y\_j - y\^{}\_j)x\_i ???
  \end{itemize}
\item
  パーセプトロンの収束定理

  \begin{itemize}
  \tightlist
  \item
    境界が線形なので複雑なパターンには向かないが
  \item
    訓練インスタンスが線形分離可能なら解が収束
  \end{itemize}
\item
  実装

  \begin{itemize}
  \tightlist
  \item
    scikit-learnのPerceptionクラス
  \item
    確率的勾配降下法の特別なケースと等価
  \item
    ハード投票分類をするだけ

    \begin{itemize}
    \tightlist
    \item
      排他的ORのような簡単な問題も解けない
    \item
      確率を出力するロジスティック回帰の方が優れている
    \item
      MLP（Multi-layer Perceptron：多層パーセプトロン）で解決
    \end{itemize}
  \end{itemize}
\item
  MLPのXOR
\item
  図で確認
\end{itemize}

\hypertarget{mlpux3068ux30d0ux30c3ux30afux30d7ux30edux30d0ux30b1ux30fcux30b7ux30e7ux30f3}{%
\subsection{10.1.4
MLPとバックプロバケーション}\label{mlpux3068ux30d0ux30c3ux30afux30d7ux30edux30d0ux30b1ux30fcux30b7ux30e7ux30f3}}

\begin{itemize}
\tightlist
\item
  M- LP
\item
  入力層、複数のTLU層、出力層
\item
  出力層に近いほど上位層、遠いほど下位層と呼ばれる

  \begin{itemize}
  \tightlist
  \item
    一方通行なアーキテクチャをFNN（Feedforward NN：順伝搬型NN）
  \end{itemize}
\item
  出力層以外にバイアスニューロン

  \begin{itemize}
  \tightlist
  \item
    通常は省略される
  \end{itemize}
\item
  DNN
\item
  ANNが深い隠れ層をもったもの

  \begin{itemize}
  \tightlist
  \item
    層の数は曖昧になってきている
  \end{itemize}
\item
  深層学習という分野で研究されるもの
\item
  1986 バックプロバケーション（誤差逆電波法）で再燃

  \begin{itemize}
  \tightlist
  \item
    勾配降下法の自動化（リバースモードの自動微分）

    \begin{itemize}
    \tightlist
    \item
      高速
    \item
      正確
    \item
      微分対象が様々な変数を持ち、出力が少ない場合に最適
    \end{itemize}
  \item
    往復1回で重みの誤差の勾配がわかる
  \end{itemize}
\item
  バックプロバケーション
\item
  1度に1個のミニバッチ（複数インスタンス）を処理（1エポック）
\item
  手順

  \begin{itemize}
  \tightlist
  \item
    全ての接続重みを無作為に初期化

    \begin{itemize}
    \tightlist
    \item
      対称性のある初期化をすると、ニューロンが少ないNNのように表現力が下がる
    \end{itemize}
  \item
    前進パス

    \begin{itemize}
    \tightlist
    \item
      処理結果を記録しつつ下位層から順に処理
    \end{itemize}
  \item
    後退パス

    \begin{itemize}
    \tightlist
    \item
      出力誤差をコスト関数で評価
    \item
      連鎖律で個々の出力接続部の誤差を計算
    \end{itemize}
  \item
    勾配降下ステップ

    \begin{itemize}
    \tightlist
    \item
      誤差が小さくなるように接続重みを調節
    \end{itemize}
  \end{itemize}
\item
  バックプロバケーションのための改良
\item
  活性化関数の改良

  \begin{itemize}
  \tightlist
  \item
    ステップ関数（カクカク） -\textgreater{}
    ロジスティック関数（滑らか）

    \begin{itemize}
    \tightlist
    \item
      連鎖律のために勾配が取れるように
    \item
      生物学的ニューロンに近しい
    \end{itemize}
  \item
    双曲線正接

    \begin{itemize}
    \tightlist
    \item
      tanh(z) = 2σ(2z)-１
    \item
      -1 \textless{} tanh(z) \textless{} 1
      のため、出力が0を中心として散らばり収束が早まる
    \end{itemize}
  \item
    ReLU関数

    \begin{itemize}
    \tightlist
    \item
      ReLU(z) = max(0, z)
    \item
      勾配が z = 0 で発散し、z \textless{} 0
      で0となるが、高速で最も利用される
    \item
      出力の最大値がないため、勾配消失問題を解決する（11章で詳解）
    \end{itemize}
  \item
    そもそも活性化関数は、線形関数の組み合わせで線形問題しか解けなかった問題を解消するための非線形関数
  \end{itemize}
\end{itemize}

\hypertarget{ux56deux5e30mlp}{%
\subsection{10.1.5 回帰MLP}\label{ux56deux5e30mlp}}

\begin{itemize}
\item
  MLPによる回帰タスク
\item
  予測の数 = 出力ニューロンの数
\item
  出力ニューロンには活性化関数は用いないことが多い

  \begin{itemize}
  \tightlist
  \item
    回帰では任意の値を出力させるため
  \end{itemize}
\item
  正の数に限定するのであればReLU関数やsoftplus関数を用いることも

  \begin{itemize}
  \tightlist
  \item
    softplus(z) = ln(1 + exp(z))
  \end{itemize}
\item
  予測値の範囲を制限するには、ロジスティック関数や双極正接関数に通してスケーリング
\item
  回帰MLPのコスト関数
\item
  平均二乗誤差

  \begin{itemize}
  \tightlist
  \item
    デファクトスタンダード
  \end{itemize}
\item
  平均絶対誤差

  \begin{itemize}
  \tightlist
  \item
    2乗しないので外れ値に強い
  \end{itemize}
\item
  フーバー損失関数

  \begin{itemize}
  \tightlist
  \item
    誤差が閾値（一般に1）以下のとき

    \begin{itemize}
    \tightlist
    \item
      平均二乗誤差に近い、高速で正確な式
    \end{itemize}
  \item
    誤差が閾値より大きいのとき

    \begin{itemize}
    \tightlist
    \item
      平均絶対誤差に近い、外れ値に近い式
    \end{itemize}
  \item
    https://qiita.com/Hatomugi/items/d00c1a7df07e0e3925a8
  \end{itemize}
\end{itemize}

\hypertarget{ux5206ux985emlp}{%
\subsection{10.1.6 分類MLP}\label{ux5206ux985emlp}}

\begin{itemize}
\tightlist
\item
  MLPによる分類タスク

  \begin{itemize}
  \tightlist
  \item
    表を挿入
  \item
    ロジスティック関数は合計が1とは限らない

    \begin{itemize}
    \tightlist
    \item
      重複があるため
    \end{itemize}
  \item
    ソフトマックス関数は合計が1になる

    \begin{itemize}
    \tightlist
    \item
      重複がないため
    \end{itemize}
  \item
    例
  \item
    二項分類

    \begin{itemize}
    \tightlist
    \item
      0～1のロジスティック活性化関数で推定確率を表現
    \end{itemize}
  \item
    多クラス分類
  \end{itemize}
\end{itemize}

\hypertarget{nnux306eux30cfux30a4ux30d1ux30e9ux306eux5faeux8abfux6574}{%
\section{10.3
NNのハイパラの微調整}\label{nnux306eux30cfux30a4ux30d1ux30e9ux306eux5faeux8abfux6574}}

\begin{itemize}
\item
  単純なMLPの調整要素
\item
  層の数
\item
  層ごとのニューロン
\item
  活性化関数
\item
  重みの初期かロジック
\item
  etc.
\item
  ANNの調整の方法1
\item
  手動で調節して交差検証で良いものを選択

  \begin{itemize}
  \tightlist
  \item
    scikit-learn風に使うためにラップ
  \item
    損失ではなくスコアを用いることに注意
  \item
    ハイパーパラメータが多いのでグリッドサーチではなくランダムサーチで一部を評価
  \item
    コード
  \end{itemize}
\item
  広範囲のハイパーパラメータ-\textgreater 最良のハイパーパラメータ付近

  \begin{itemize}
  \tightlist
  \item
    時間がかかる
  \end{itemize}
\item
  ある領域が良いとわかったらズームイン
\item
  ズームインするライブラリ1

  \begin{itemize}
  \tightlist
  \item
    Hyperopt

    \begin{itemize}
    \tightlist
    \item
      あらゆるタイプの複雑な探索空間に対応する広く使われるライブラリ
    \end{itemize}
  \item
    Hyperas

    \begin{itemize}
    \tightlist
    \item
      Kerasの最適化ライブラリ
    \end{itemize}
  \item
    kopt

    \begin{itemize}
    \tightlist
    \item
      Kerasの最適化ライブラリ
    \end{itemize}
  \item
    Talos

    \begin{itemize}
    \tightlist
    \item
      Kerasの最適化ライブラリ
    \end{itemize}
  \item
    Keras Tuner

    \begin{itemize}
    \tightlist
    \item
      Kerasの最適化ライブラリ
    \item
      可視化と解析も可能
    \end{itemize}
  \end{itemize}
\item
  ズームインするライブラリ2

  \begin{itemize}
  \tightlist
  \item
    Scikit-Optimize

    \begin{itemize}
    \tightlist
    \item
      GridSearchCVクラスと似たベイズ最適化が可能
    \end{itemize}
  \item
    Spearmint

    \begin{itemize}
    \tightlist
    \item
      ベイズ最適化ライブラリ
    \end{itemize}
  \item
    Hyperband

    \begin{itemize}
    \tightlist
    \item
      高速
    \end{itemize}
  \item
    Sklearn

    \begin{itemize}
    \tightlist
    \item
      進化的アルゴリズム
    \item
      GridSearchCV風
    \end{itemize}
  \end{itemize}
\item
  ズームインするツール
\item
  Google Cloud API のハイパーパラメータ調整サービス
\item
  Arimo
\item
  SigOpt
\item
  CallDeskのOscar
\item
  DeepMind (2017) ``Population Based Training of Neural Networks''
\item
  GoogleのAutoMLスイート
\end{itemize}

\hypertarget{ux96a0ux308cux5c64ux306eux6570}{%
\subsection{10.3.1 隠れ層の数}\label{ux96a0ux308cux5c64ux306eux6570}}

\begin{itemize}
\tightlist
\item
  浅いNN

  \begin{itemize}
  \tightlist
  \item
    複雑でなければ隠れ層は1つで十分なことが多い
  \item
    MNISTで97\%
  \end{itemize}
\item
  深いNW

  \begin{itemize}
  \tightlist
  \item
    森を描くときに枝葉や木をコピペできるようなイメージ
  \item
    指数的に少ないニューロンで済む
  \item
    収束が早い
  \item
    汎化能力が高い

    \begin{itemize}
    \tightlist
    \item
      髪の訓練に頭部の分類器の下位層を使いまわせる
    \item
      転移学習という
    \end{itemize}
  \item
    過学習の手前まで層を増やす
  \end{itemize}
\end{itemize}

\hypertarget{ux96a0ux308cux5c64ux3042ux305fux308aux306eux30cbux30e5ux30fcux30edux30f3ux6570}{%
\subsection{10.3.2
隠れ層あたりのニューロン数}\label{ux96a0ux308cux5c64ux3042ux305fux308aux306eux30cbux30e5ux30fcux30edux30f3ux6570}}

\begin{itemize}
\tightlist
\item
  層ごとのニューロン数
\item
  上位層に向けて萎むパターン

  \begin{itemize}
  \tightlist
  \item
    従来手法
  \end{itemize}
\item
  全ての層で同じ数にするパターン

  \begin{itemize}
  \tightlist
  \item
    多くの場合、性能は同じか向上
  \item
    ハイパーパラメータがn層分から1つに統一可能
  \end{itemize}
\item
  過学習の手前まで数を増やすパターン
\item
  ストレッチパンツアプローチ

  \begin{itemize}
  \tightlist
  \item
    必要以上に多く取り、早期打ち切りや正則化で過学習対策
  \item
    ボトルネックとなる層が発生しない
  \end{itemize}
\item
  ニューロン数より層の数を増やした方が効果は得やすい
\end{itemize}

\hypertarget{ux5b66ux7fd2ux7387ux30d0ux30c3ux30c1ux30b5ux30a4ux30baux305dux306eux4ed6ux306eux30cfux30a4ux30d1ux30e9}{%
\subsection{10.3.3
学習率、バッチサイズ、その他のハイパラ}\label{ux5b66ux7fd2ux7387ux30d0ux30c3ux30c1ux30b5ux30a4ux30baux305dux306eux4ed6ux306eux30cfux30a4ux30d1ux30e9}}

\begin{itemize}
\tightlist
\item
  M- LPの重要なハイパーパラメータ1
\item
  学習率

  \begin{itemize}
  \tightlist
  \item
    一般に、出力値が発散し始める値の半分が最適
  \item
    実装例：10\^{}-5から数百回イテレートして10へ
  \item
    対数軸を取って損失をグラフ化する
  \item
    一般に、折り返しの1/10手前が最適

    \begin{itemize}
    \tightlist
    \item
      過学習を恐れて？
    \end{itemize}
  \end{itemize}
\item
  オプティマイザ

  \begin{itemize}
  \tightlist
  \item
    ミニバッチ勾配降下法の代替法
  \item
    11章で詳解
  \end{itemize}
\item
  MLPの重要なハイパーパラメータ2
\item
  バッチサイズ

  \begin{itemize}
  \tightlist
  \item
    大きいほどGPUが同時に沢山処理
  \item
    RAMの制限を受ける
  \item
    大きすぎると訓練初期に不安定＆汎化しない

    \begin{itemize}
    \tightlist
    \item
      2～32が推奨されたりされなかったり
    \item
      段々と大きくしていく手法も存在
    \end{itemize}
  \end{itemize}
\item
  活性化関数

  \begin{itemize}
  \tightlist
  \item
    出力層以外はReLU関数でよい
  \end{itemize}
\item
  イテレーション数

  \begin{itemize}
  \tightlist
  \item
    ほぼ操作することはないが、早期打ち切りで意識することがある
  \item
    NNのハイパーパラメータ調整のベストプラクティス
  \end{itemize}
\end{itemize}
