<!-- tex script for md -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# é€±æ¬¡å ±å‘Šæ›¸ 2021å¹´12æœˆ08æ—¥
AL18036 ç‰‡å²¡ å‡ª

## 1. ä»Šå›ã®å ±å‘Šä¼šã¾ã§ã«å®Ÿæ–½ã™ã‚‹äºˆå®šã ã£ãŸã“ã¨
- å’è«–ã®ç›®æ¬¡ä½œæˆ
- IBMã®å‰å‡¦ç†
- åˆ†é¡çµæœã‚’åŸå‘³
    - å‰å‡¦ç†ã®æœ‰ç„¡
    - ~~ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¤‰ãˆã¦~~
- ~~ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®å®Ÿè£…~~
    - å‡ºæ¥äº‹ã®æ–‡ç« ã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
    - ä¸»å¼µã®æ–‡ã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
    - 3ã‚«å›½ã®è¨˜äº‹ã‚’çµåˆã—ã¦ä¿å­˜
    - [e-feature-array]ã‚’åŸºã«è¨˜äº‹ï¼ˆè¡Œï¼‰ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    - å‡ºæ¥äº‹ã®è¨˜äº‹ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰nation-name;article-n;sentence-nã§è¨˜äº‹ã‚’ç‰¹å®š
    - æŒ‡å®šã—ãŸè¨˜äº‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡ã‚’ã€cã®[feature-array]ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    - ä¸»å¼µã®æ–‡ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰nation-name;article-n;sentence-nã§æ–‡ã¨ãã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ç‰¹å®š
- ~~è©•ä¾¡æ–¹æ³•ã®æ¤œè¨~~

## 2. å®Ÿæ–½å†…å®¹

### ç›®æ¬¡
- 2.1 IBM Debater Datasertã‚’å‰å‡¦ç†ã—ã¦æ¯”è¼ƒ
- 2.2 ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ï¼ˆåŠã°æ–­å¿µï¼‰
- 2.3 ç›®æ¬¡ã®ä½œæˆ

### 2.1 IBM Debater Datasertã‚’å‰å‡¦ç†ã—ã¦æ¯”è¼ƒ

å‰å›å ±å‘Šã—ãŸå‡ºæ¥äº‹ã¨ä¸»å¼µã®åˆ†é¡ã§ã¯ã€å‰å‡¦ç†ã—ã¦ã„ãªã„IBM Debater Datasertã§åˆ†é¡å™¨ã‚’å­¦ç¿’ã—ã€å‰å‡¦ç†ã—ãŸcovid-19-articlesã‚’åˆ†é¡ã—ã¦ã„ãŸã€‚
ã“ã®å‰å‡¦ç†ã®æœ‰ç„¡ã«ã‚ˆã£ã¦ç²¾åº¦ãŒä¸‹ãŒã£ã¦ã„ã‚‹ã¨è€ƒãˆã€IBM Debater Datasertã«ã‚‚covid-19-articlesã¨ã»ã¼åŒæ§˜ã®å‰å‡¦ç†ã‚’è¡Œã„ã€åˆ†é¡ã‚’è¡Œã£ãŸã€‚
IBM Debater Datasertã«é™ã‚Šã€[REF]ã‚¿ã‚°ã®é™¤å»ã‚„æ–‡æœ«ã®ãƒ”ãƒªã‚ªãƒ‰ã®è¿½åŠ ã‚’è¡Œã£ã¦ã„ã‚‹ã€‚

<!-- åˆ†é¡ã®çµæœã€å‰å‡¦ç†ã—ãªã„åˆ†é¡ã§ã¯å°‘ãªã‹ã£ãŸã€Œä¸»å¼µã¨åˆ†é¡ã™ã‚‹ä¾‹ã€ãŒ6ä»¶å¢—åŠ ã—ã€ãã®ä»–ã®åˆ†é¡çµæœã¯ä¸€è‡´ã—ãŸã€‚ -->
<!-- 6ä»¶ã®ä¸»å¼µã®åˆ†é¡ã®ã†ã¡2ä»¶ã¯æ­£ç­”ã—ã¦ãŠã‚Š -->
<!-- ä¸»å¼µã¨ã‚‚è¨€ãˆãªãã‚‚ãªã„æ–‡ã¯ã‚ã£ãŸã‹ -->

åˆ†é¡ã®çµæœã€2ä»¶ã®èª¤åˆ†é¡ãŒå¢—åŠ ã—ã€æ­£ã—ã„åˆ†é¡ã¯39ä»¶ä¸­26ä»¶ï¼ˆ66.7%ï¼‰ã¨ãªã£ãŸã€‚
åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®é©åˆç‡ãŒ99.8%ã§ã‚ã‚Šã€éå­¦ç¿’ãŒç–‘ã‚ã‚Œã‚‹ãŸã‚ã€ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æ¸›ã‚‰ã™äºˆå®šã§ã‚ã‚‹ã€‚

â€»å‰å›ã®åˆ†é¡ã§39ä»¶ä¸­21ä»¶ï¼ˆç´„53.8ï¼…ï¼‰ã®ã‚¨ãƒ©ãƒ¼ï¼ˆæ­£è§£ã¯46.2%ï¼‰ã¨ãŠä¼ãˆã—ã¦ã—ã¾ã„ã¾ã—ãŸãŒã€æ­£ã—ãã¯39ä»¶ä¸­11ä»¶ï¼ˆç´„28.2ï¼…ï¼‰ã®ã‚¨ãƒ©ãƒ¼ï¼ˆæ­£è§£ã¯71.8%ï¼‰ã§ã—ãŸã€‚


è¨˜äº‹ID | æ¨æ¸¬ | æœªå‡¦ç† 10ã‚¨ãƒ | 10ã‚¨ãƒ | 5ã‚¨ãƒ | 3ã‚¨ãƒ |
|:----:|:----:|:----:|:----:|:----:|:----:|
| IN-1 | E | E | E |  |  |
| IN-2 | E | E | C |  |  |
| IN-3 | E | E | E |  |  |
| IN-4 | E | E | E |  |  |
| IN-5 | E | E | E |  |  |
| IN-6 | E | E | C |  |  |
| IN-7 | E | E | E |  |  |
| IN-8 | E | E | E |  |  |
| IN-9 | E | E | E |  |  |
| IN-10 | C | E | E |  |  |
| JP-1 | E | E | E |  |  |
| JP-2 | E | E | E |  |  |
| JP-3 | E | E | E |  |  |
| JP-4 | C | E | C |  |  |
| JP-5 | E | E | C |  |  |
| JP-6 | E | E | E |  |  |
| JP-7 | E | E | E |  |  |
| JP-8 | E | C | C |  |  |
| JP-9 | E | E | E |  |  |
| JP-10 | E | E | C |  |  |
| JP-11 | E | E | E |  |  |
| JP-12 | E | E | E |  |  |
| JP-13 | C | E | E |  |  |
| JP-14 | E | E | E |  |  |
| JP-15 | E | C | C |  |  |
| JP-16 | E | E | E |  |  |
| JP-17 | E | E | E |  |  |
| KR-1 | E | E | E |  |  |
| KR-2 | E | E | E |  |  |
| KR-3 | C | E | E |  |  |
| KR-4 | C | E | E |  |  |
| KR-5 | E | E | E |  |  |
| KR-6 | E | E | E |  |  |
| KR-7 | C | E | E |  |  |
| KR-8 | C | E | C |  |  |
| KR-9 | C | E | E |  |  |
| KR-10 | E | E | E |  |  |
| KR-11 | E | E | E |  |  |
| KR-12 | C | E | E |  |  |
| æ­£è§£æ•° | 39 | **28** | 26 |  |  |
| æ­£è§£ç‡ | 100 | 71.8 | 66.7 |  |  |
| acc | - | 99.6 | 99.5 |  |  |
| prc | - | 99.996 | 99.8 |  |  |


### 2.2 ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ï¼ˆåŠã°æ–­å¿µï¼‰

ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å°‘ãªãã™ã‚‹ã“ã¨ã‚’è€ƒãˆãŸéš›ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã‚ˆã£ã¦è‡ªå‹•ã§æœ€é©åŒ–ã§ããªã„ã‹ã¨è€ƒãˆãŸã€‚
è»½ãèª¿æŸ»ã‚’è¡Œã£ãŸã¨ã“ã‚ã€ç”¨ã„ãŸTransformerãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«é©ã—ãŸ[Weightsï¼†Biases(wandb)](https://note.com/npaka/n/n298f269c2275)ã¨ã„ã†ã‚µãƒ¼ãƒ“ã‚¹ã‚’è¦‹ã¤ã‘ãŸã€‚
wandbã§ã¯ã€ã‚¨ãƒãƒƒã‚¯æ•°ã ã‘ã§ãªãæœ€é©ãªå­¦ç¿’ç‡ã¾ã§å®šã‚ã‚‹ã“ã¨ãŒã§ãã€åˆ†é¡ç²¾åº¦ã®å‘ä¸ŠãŒæœŸå¾…ã§ãã‚‹ã€‚

çŸ­ã„ã‚³ãƒ¼ãƒ‰ã§ã™ãã«å®Ÿè£…ã§ããŸãŒã€ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã€‚
ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã®éš›ã«å¤šæ•°ã®é‡ã¿ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚„ã€ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã§è¨“ç·´ã—ã¦ã„ãªã„ï¼ˆï¼Ÿï¼‰ã“ã¨ã‚„ã€DataFrameå‹ã®å¼•æ•°æŒ‡å®šãŒæ­£ã—ããªã„ãªã©ã®ã‚¨ãƒ©ãƒ¼ã§ã‚ã‚‹ã€‚

ã„ãã¤ã‹ã®æŠ€è¡“è¨˜äº‹ã‚„å…¬å¼ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã§è¨˜è¿°ã®å·®ãŒæ¿€ã—ã„äº‹ã‚„ã€è¦‹æ…£ã‚Œãªã„ã‚¨ãƒ©ãƒ¼ãŒå¤šã„ã“ã¨ã‹ã‚‰ã€ä¿®æ­£ã«ã¯é•·ã„æ™‚é–“ãŒã‹ã‹ã‚‹ã¨åˆ¤æ–­ã—ãŸã€‚
å ±å‘Šä¼šã‚„ç›¸è«‡ä¼šã§ã™ãã«è§£æ±ºã—ãªã„å ´åˆã€å®Ÿè£…ã¯å¾Œå›ã—ã«ã—ã€5, 3 (, 7)ã‚¨ãƒãƒƒã‚¯ã‚’è©¦ã™äºˆå®šã§ã‚ã‚‹ã€‚
7ã‚¨ãƒãƒƒã‚¯ã¯ã€5ã‚¨ãƒãƒƒã‚¯ã‹ã‚‰3ã‚¨ãƒãƒƒã‚¯ã«ã—ãŸéš›ã«ç²¾åº¦ãŒä½æ¸›ã—ãŸå ´åˆã«å®Ÿè¡Œã™ã‚‹ã€‚

```
*** train & evaluate model ***
Create sweep with ID: gb4a5zpd
Sweep URL: https://wandb.ai/kataoka-nagi/RTE%20-%20Hyperparameter%20Optimization/sweeps/gb4a5zpd
NUM_EPOCHS: 10
MODEL_SEED: 2021
CLASSIFICATION_MODEL_TYPE: roberta
CLASSIFICATION_MODEL_NAME: roberta-base
wandb: Tracking run with wandb version 0.12.7
wandb: Syncing run wild-durian-1
wandb: â­ï¸ View project at https://wandb.ai/kataoka-nagi/RTE%20-%20Hyperparameter%20Optimization
wandb: ğŸš€ View run at https://wandb.ai/kataoka-nagi/RTE%20-%20Hyperparameter%20Optimization/runs/25olfk2s
wandb: Run data is saved locally in /home/nagi/Documents/git/data-engineering-lab/experiment/wandb/run-20211206_083148-25olfk2s
wandb: Run `wandb offline` to turn off syncing.
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_model.py:586: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.
  "Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels."
INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.

Traceback (most recent call last):
  File "process_01_train_classifier_as_claim_or_evidence.py", line 253, in <module>
    main()
  File "process_01_train_classifier_as_claim_or_evidence.py", line 214, in main
    wandb.agent(sweep_id, train(train_df, eval_df, model_args))
  File "process_01_train_classifier_as_claim_or_evidence.py", line 243, in train
    overwrite_output_dir=True)
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_model.py", line 593, in train_model
    train_examples, verbose=verbose
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_model.py", line 1806, in load_and_cache_examples
    no_cache=no_cache,
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_utils.py", line 258, in __init__
    data, tokenizer, args, mode, multi_label, output_mode, no_cache
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_utils.py", line 197, in build_classification_dataset
    labels = [args.labels_map[label] for label in labels]
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_utils.py", line 197, in <listcomp>
    labels = [args.labels_map[label] for label in labels]
KeyError: 1.0
Traceback (most recent call last):
  File "process_01_train_classifier_as_claim_or_evidence.py", line 253, in <module>
    main()
  File "process_01_train_classifier_as_claim_or_evidence.py", line 214, in main
    wandb.agent(sweep_id, train(train_df, eval_df, model_args))
  File "process_01_train_classifier_as_claim_or_evidence.py", line 243, in train
    overwrite_output_dir=True)
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_model.py", line 593, in train_model
    train_examples, verbose=verbose
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_model.py", line 1806, in load_and_cache_examples
    no_cache=no_cache,
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_utils.py", line 258, in __init__
    data, tokenizer, args, mode, multi_label, output_mode, no_cache
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_utils.py", line 197, in build_classification_dataset
    labels = [args.labels_map[label] for label in labels]
  File "/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_utils.py", line 197, in <listcomp>
    labels = [args.labels_map[label] for label in labels]
KeyError: 1.0
wandb: Waiting for W&B process to finish, PID 6393... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)
wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)
wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)
wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)
wandb: - 0.00MB of 0.01MB uploaded (0.00MB deduped)
wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb:                                                                                
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced wild-durian-1: https://wandb.ai/kataoka-nagi/RTE%20-%20Hyperparameter%20Optimization/runs/25olfk2s
wandb: Find logs at: ./wandb/run-20211206_083148-25olfk2s/logs/debug.log
wandb: 
```

ï¼ˆâ†“ä¸»è¦ãªã‚¨ãƒ©ãƒ¼ã®DeepLç¿»è¨³ï¼‰
```
roberta-baseã®ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã„ãã¤ã‹ã®é‡ã¿ãŒã€RobertaForSequenceClassificationã®åˆæœŸåŒ–æ™‚ã«ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight'] ã€‚
- ã“ã‚Œã¯ã€ä»–ã®ã‚¿ã‚¹ã‚¯ã‚„ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰RobertaForSequenceClassificationã‚’åˆæœŸåŒ–ã—ã¦ã„ã‚‹å ´åˆã«æœŸå¾…ã•ã‚Œã¾ã™ï¼ˆä¾‹ãˆã°ã€BertForPreTrainingãƒ¢ãƒ‡ãƒ«ã‹ã‚‰BertForSequenceClassificationãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹å ´åˆï¼‰ã€‚
- ã“ã‚Œã¯ã€å…¨ãåŒä¸€ã§ã‚ã‚‹ã¨äºˆæƒ³ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰RobertaForSequenceClassificationã‚’åˆæœŸåŒ–ã—ã¦ã„ã‚‹å ´åˆï¼ˆBertForSequenceClassificationãƒ¢ãƒ‡ãƒ«ã‹ã‚‰BertForSequenceClassificationãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ã„ã‚‹å ´åˆï¼‰ã«ã¯æœŸå¾…ã§ãã¾ã›ã‚“ã€‚
RobertaForSequenceClassificationã®ã„ãã¤ã‹ã®é‡ã¿ã¯ã€roberta-baseã®ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã¯åˆæœŸåŒ–ã•ã‚Œãšã€æ–°ãŸã«åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias'] ã€‚
ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’äºˆæ¸¬ã‚„æ¨è«–ã«ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ã«ã¯ã€ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã§TRAINã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã§ã—ã‚‡ã†ã€‚
/home/nagi/Documents/git/data-engineering-lab/experiment/venv/lib/python3.6/site-packages/simpletransformers/classification/classification_model.py:586: UserWarningã§ã™ã€‚Dataframeã®ãƒ˜ãƒƒãƒ€ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚0åˆ—ç›®ã‚’ãƒ†ã‚­ã‚¹ãƒˆã€1åˆ—ç›®ã‚’ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã«æˆ»ã‚Šã¾ã™ã€‚
  "Dataframe headers not specified. åˆ— 0 ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ã€åˆ— 1 ã‚’ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã«æˆ»ã‚Šã¾ã™ã€‚"
INFO:simpletransformers.classification.classification_utils: æ©Ÿèƒ½ã¸ã®å¤‰æ›ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚
```



### 2.3 ç›®æ¬¡ã®ä½œæˆ
å’æ¥­è«–æ–‡ã®ç›®æ¬¡ã®è‰æ¡ˆã‚’ä½œæˆã—ãŸã€‚
ä½œæˆã«ã‚ãŸã‚Šã€å ±å‘Šæ›¸ã‚„å ±å‘Šä¼šã®è­°äº‹éŒ²ã€ä¸­æ‘å…ˆè¼©ã¨åŠ ç€¬å…ˆè¼©ã¨ç–‹ç”°å…ˆè¼©ã®å’è«–ã€äº•å°»å…ˆç”Ÿã®å’è«–ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å‚ç…§ã—ãŸã€‚

â€»æ‹¬å¼§å†…ã¯ãƒ¡ãƒ¢ã§ã™ï¼ˆç¯€ã‚¿ã‚¤ãƒˆãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰
â€»i -> n.1, a -> n.m.1 ã¨èª­ã¿æ›¿ãˆã¦ãã ã•ã„

1.  ç›®æ¬¡
2.  åºè«–
    1.  ç ”ç©¶èƒŒæ™¯
    2.  ç ”ç©¶ç›®çš„
    3.  æœ¬è«–æ–‡ã®æ§‹æˆ
3.  æœ¬ç ”ç©¶ã§ç”¨ã„ã‚‹çŸ¥è­˜ãƒ»æŠ€è¡“
    1.  ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
    2.  ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãŒç”Ÿã‚€ãƒã‚¤ã‚¢ã‚¹
        1.  ã‚¨ã‚³ãƒ¼ãƒã‚§ãƒ³ãƒãƒ¼å•é¡Œ
        2.  ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ–ãƒ«å•é¡Œ
    3.  æ©Ÿæ¢°å­¦ç¿’
        1.  ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        2.  æ•™å¸«ã‚ã‚Šå­¦ç¿’
        3.  Attentionï¼ˆLSTMã«è§¦ã‚Œã‚‹ï¼‰
        4.  Transformerï¼ˆæ•™å¸«ã‚ã‚Šåˆ†é¡ã«ã‚‚è§¦ã‚Œã‚‹ï¼‰
        5.  BERT
        6.  RoBERTaï¼ˆBERTã¨ã®æ¯”è¼ƒãªã©ï¼‰
    4.  æ–‡ç« åˆ†é¡
        1.  ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        2.  å˜èªåŸ‹ã‚è¾¼ã¿
        3.  Transformeråˆ†é¡å™¨
        4.  åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ï¼ˆacc, prcãªã©ã®è­°è«–ï¼‰
    5.  æ–‡ç« ã®é¡ä¼¼åº¦ã®ç®—å‡º
        1.  Sentence-BERT
        2.  ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    6.  ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        1.  ééšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        2.  éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        3.  Wardæ³• 
        4.  ï¼ˆãã®ä»–ä½¿ç”¨ã—ãŸæ‰‹æ³•ï¼‰
        5.  t-SNE (ä½¿ã†ã‹ã‚‚)
4.  é–¢é€£ç ”ç©¶
    1.  ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãƒã‚¤ã‚¢ã‚¹ã®è§£æ±º
        1.  Breaking the filter bubble: democracy and design
            1.  ï¼ˆUIã§ãƒãƒ–ãƒ«ã®å¯è¦–åŒ–ï¼‰ï¼ˆçµå±€è¡¨ç¤ºã•ã‚Œã‚‹ã‚‚ã®ã«ãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã‚‹ã€è¡Œå‹•ã«ç¹‹ãŒã‚‰ãªã„ï¼‰
            2.  ï¼ˆãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã€LSI, LDAã®è­°è«–ï¼‰
    2.  è©±é¡Œã®å®šé‡åŒ–ã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦æ‰‹æ³•
        1.  ãƒˆãƒ”ãƒƒã‚¯ãƒãƒƒãƒ—
        2.  Labeled Bilingual Topic Model for Cross-Lingual Text Classification and Label Recommendation
            1.  ï¼ˆLDAã®åˆ©ç”¨ï¼‰
        <!-- 2.  LSI -->
    1.  ï¼ˆè¦è¿½åŠ èª¿æŸ»ï¼šæ¯”è¼ƒè©•ä¾¡ã§ãã‚‹æ¨è–¦æ‰‹æ³•ï¼‰ï¼ˆå‡ºæ¥äº‹ã€ä¸»å¼µã«ç€ç›®ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãªã©ï¼‰
        1.  Investigating COVID-19 News Across Four Nations A Topic Modeling and Sentiment Analysis Approach
            1.  ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã€top2vec, roberta
    <!-- 6.  tf-idf -->
    <!-- SCDV -->
5.  ææ¡ˆæ‰‹æ³•
    1.  ä½¿ç”¨ã™ã‚‹èªå½™ã¨åŸºæº–ã®å®šç¾©
        1.  æ–‡ã¨æ–‡ç« 
        2.  å‡ºæ¥äº‹ã®æ–‡
        3.  ä¸»å¼µã®æ–‡
        4.  æ–‡ãŒç¤ºã™è©±é¡Œã®é¡ä¼¼åº¦
    2.  è¨˜äº‹ã®å‡ºæ¥äº‹ã¨ä¸»å¼µã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ç”¨ã„ãŸå¤šè¨€èªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦
        1.  å†…å®¹
        2.  ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®é †åºã®æ¤œè¨
6.  å®Ÿè£…
    1.  ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆæŒ‡é‡ï¼ˆå…¥å‡ºåŠ›ã€ä½¿ã„æ–¹ãªã©ï¼‰
    2.  ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®èª¬æ˜ï¼‰
    3.  å®Ÿè£…ç’°å¢ƒï¼ˆPCã‚¹ãƒšãƒƒã‚¯ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ãªã©ï¼‰
    4.  ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        1. è‡ªç„¶è¨€èªå‡¦ç†ã®ãŸã‚ã®ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ï¼ˆå‰å‡¦ç†ã®ç¨®é¡ã€ãªãœå‰å‡¦ç†ã™ã‚‹ã®ã‹ã€awkã®æ­£è¦è¡¨ç¾ãªã©ã®è­°è«–ï¼‰
        2. çœç•¥ã®ãƒ”ãƒªã‚ªãƒ‰ã«æ³¨æ„ã—ãŸæ–‡ç« ã®åˆ†å‰²ï¼ˆStanza, spacyã®è­°è«–ï¼‰
    5.  å‡ºæ¥äº‹ã®æ–‡ã¨ä¸»å¼µã®æ–‡ã®åˆ†é¡
        1. Simple Transformer
    6.  å‡ºæ¥äº‹ã®æ–‡ç« ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        1.  (è¦æ¤œè¨)
    7.  ä¸»å¼µã®æ–‡ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        1.  (è¦æ¤œè¨)
7.  å®Ÿé¨“
    1.  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é¸å®š
        1.  å‡ºæ¥äº‹ã®æ–‡ã¨ä¸»å¼µã®æ–‡ã®åˆ†é¡å™¨ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆIBM Debater Datasetã®è­°è«–ï¼‰
        2.  åˆ†é¡ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†ãƒ‡ãƒ¼ã‚¿ï¼ˆcovid-19-articlesã®è­°è«–ï¼‰
            <!-- 1.  Japanese fakenews dataset -->
    2.  å‡ºæ¥äº‹ã®æ–‡ã¨ä¸»å¼µã®æ–‡ã®åˆ†é¡
        1.  å®Ÿé¨“æ–¹æ³•
        2.  å®Ÿé¨“çµæœ
        3.  ï¼ˆè©¦è¡ŒéŒ¯èª¤ï¼‰
    3.  å‡ºæ¥äº‹ã®æ–‡ç« ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        1.  å®Ÿé¨“æ–¹æ³•
        2.  å®Ÿé¨“çµæœ
        3.  ï¼ˆè©¦è¡ŒéŒ¯èª¤ï¼‰
    4.  ä¸»å¼µã®æ–‡ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        1.  å®Ÿé¨“æ–¹æ³•ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ã®éšå±¤ã®åŸºæº–ã”ã¨ã®è©•ä¾¡ï¼‰
        2.  å®Ÿé¨“çµæœ
        3.  ï¼ˆè©¦è¡ŒéŒ¯èª¤ï¼‰
    5.  ï¼ˆä»–ã®ç ”ç©¶ã¨ã®æ¯”è¼ƒå®Ÿé¨“ï¼‰
        1.  å®Ÿé¨“æ–¹æ³•
        2.  å®Ÿé¨“çµæœ
        3.  ï¼ˆè©¦è¡ŒéŒ¯èª¤ï¼‰
8.  çµæœã¨è€ƒå¯Ÿ
    1.  æ—¢å­˜æ‰‹æ³•ã¨ã®æ¯”è¼ƒ
    2.  ææ¡ˆæ‰‹æ³•ã®å®Ÿç”¨æ€§
        1.  å…¥åŠ›ã¨å‡ºåŠ›ã®å¦¥å½“æ€§
        2.  å‡¦ç†é€Ÿåº¦ï¼ˆåˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã®è­°è«–ã‚‚ï¼‰
    3.  ï¼ˆçµæœã‚’åŸºã«æ¤œè¨ï¼‰
9.  ã¾ã¨ã‚ã¨å±•æœ›
    1.  ï¼ˆçµæœã‚’åŸºã«æ¤œè¨ï¼‰
    2.  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç›¸æ€§ï¼ˆãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆã¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰
10. è¬è¾
11. å‚è€ƒæ–‡çŒ®


## 3. æ¬¡å›ã¾ã§ã«å®Ÿæ–½äºˆå®šã§ã‚ã‚‹ã“ã¨
- çŸ¥äººï¼†LINEæ¢ã—
- åˆ†é¡
    - Epocæ•°ã®å¤‰æ›´ã¨åˆ†æ
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®å®Ÿè£…
    - å‡ºæ¥äº‹ã®æ–‡ç« ã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
    - ä¸»å¼µã®æ–‡ã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
    - 3ã‚«å›½ã®è¨˜äº‹ã‚’çµåˆã—ã¦ä¿å­˜
    - [e-feature-array]ã‚’åŸºã«è¨˜äº‹ï¼ˆè¡Œï¼‰ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    - å‡ºæ¥äº‹ã®è¨˜äº‹ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰nation-name;article-n;sentence-nã§è¨˜äº‹ã‚’ç‰¹å®š
    - æŒ‡å®šã—ãŸè¨˜äº‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡ã‚’ã€cã®[feature-array]ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    - ä¸»å¼µã®æ–‡ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰nation-name;article-n;sentence-nã§æ–‡ã¨ãã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ç‰¹å®š
- è©•ä¾¡æ–¹æ³•ã®æ¤œè¨

## 4. ãƒ¡ãƒ¢
- IBMã®å‰å‡¦ç†
    - shã‚’ä½œæˆ
    - awkã‚’ä½œæˆ
        - covidã¨ã®å·®åˆ†
            - [REF ã‚„ [REF] ãªã©ã®é™¤å»
            - æœ«å°¾ã«ãƒ”ãƒªã‚ªãƒ‰ã‚’è¿½åŠ 
    - å­¦ç¿’
        - 10-epoc
            - {'mcc': 0.9885047024316609, 'tp': 444, 'tn': 946, 'fp': 5, 'fn': 2, 'auroc': 0.9993704997807358, 'auprc': 0.9981418495989224, 'eval_loss': 0.04455929614907031, 'acc': 0.9949892627057981}
    - å­¦ç¿’ã®æœ€é©åŒ–
        - Weightsï¼†Biases(wandb)ã®åˆ©ç”¨
            - https://note.com/npaka/n/n298f269c2275
- ç›®æ¬¡
    - å…ˆè¼©ã®è«–æ–‡ã‚’å‚ç…§
        - tmp/senior-thesis-idx
    - å ±å‘Šæ›¸ã¨è­°äº‹éŒ²ã‚’æŒ¯ã‚Šè¿”ã‚‹
- 104ã®å½¼ã¨é–“æ¥çš„ã«é€£çµ¡
    - ~~105~~
    - ~~107~~
    - è‡ªå·±ç´¹ä»‹ã§ã‚µãƒ¼ã‚¯ãƒ«ã¨ã‹è¨€ã£ã¦ãŸã£ã‘
