<!-- tex script for md -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 論文要約 『UNBERT: User-News Matching BERT for News Recommendation』

- [1]Q. Zhangほか, 「UNBERT: User-News Matching BERT for News Recommendation」, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, Montreal, Canada, 8月 2021, pp. 3356–3362. doi: 10.24963/ijcai.2021/462.
- 20220128参照

<!-- -------------------- -->

## 概要
- ニュースレコメンデーション
    - ユーザーが自分の興味のあるニュースにアクセスするためのチャネル
    - 目的
        - テキストコンテンツを表現
        - ユーザの興味と候補となるニュースを的確にマッチング
    - 既存
        - 単にドメイン内のニュースデータからテキスト表現を学習するだけ
        - 新しいニュースに対する汎化能力に限界
            - コールドスタートシナリオにありがち
        - 過去に閲覧したニュースを一つのベクトルに集約して各ユーザを表現
            - 候補ニュースベクトルを用いてマッチングスコアを計算
            - 低レベルのマッチングシグナルが失われる可能性
    - 本論文
        - 自然言語処理における成功したBERT事前学習技術をニュース推薦に利用
        - UNBERT
            - BERTベース
            - ユーザー-ニュースマッチングモデル
            - 豊富な言語知識を持つ事前学習モデルを活用
                - テキスト表現を強化可能
            - 単語レベルとニュースレベルの両方で多階調のユーザー-ニュースマッチング信号を捕捉
            - マイクロソフトニュースデータセット(MIND)で常に最先端

<!-- -------------------- -->

## 図表
- ネガティブな例
    - フロリダ、アメリカ、カリフォルニアの単語に反応して日本を推薦
- UNBERTの入力
    - 和
        - トークン埋め込み
        - セグメンテーション埋め込み
        - 位置埋め込み
        - **ニュースセグメンテーション埋め込み**
        - CLS、新規ニュース、SEP、閲覧済みニュース、NSEP、閲覧済みニュース、NSEP、、、
- UNBERTのアーキテクチャ
    - 出力Cをクリック予測に渡す
    - C以外の出力を別のBERTに渡す
    - 別のBERTの出力を同じクリック予測に渡す
    - 単語レベルの学習とニュースレベルの学習
- MINDベンチマークでトップ
    - LibFM
    - DeepFM
    - DKN
    - NPA
    - NAML
    - LSTUR
    - NRMS
    - FIM
- アグリゲータを変えて比較
- 各メソッドの日ごとのパフォーマンス推移
- 

<!-- -------------------- -->

## はじめに
- 従来の推薦手法
    - ニュースコンテントのモデル化
        - カテゴリ特徴量（ニュースID、ニュースカテゴリなど）
        - Bag-of-Words（トークンやN-gram）
        - 代表的な例
            - 因数分解マシンに基づく古典的な推薦モデルである
                - LibFM [Rendle, 2012]
                - DeepFM [Guo et al., 2017]
- 最近の多くの研究
    - [Wang et al., 2018; An et al., 2019; Wu et al., 2019a; Wu et al., 2019c; Wang et al., 2020]
    - ニューラルニュース推薦
    - エンドツーエンド方式でニュース表現を学習
    - 単語を低次元の埋め込みに
    - 一般的なネットワークアーキテクチャで隠されたニュース表現を学習
        - CNN
        - 注意メカニズム
        - 性能が向上
        - だが、コールドスタート問題
            - ニュース記事は更新が速く、時間も短い
            - 最近のいくつかの研究
                -  [Wu et al., 2019c; Wang et al., 2020] 
                -  モデルの埋め込み層を初期化
                -  事前に学習した単語埋め込みの仕様
                    -  例えば，Word2Vec [Mikolov et al., 2013] 
                -  Glove [Pennington et al., 2014] ）の使用
                    -  しかし、文脈に依存しない
                    -  ran-domlyに初期化された下流モデルでの学習時には、その効果がさらに弱まる
                        -  事前学習されたBERTモデル[Devlin et al., 2018]を適用
                        -  最も成功した事前学習済み言語モデル
                            - 広く利用されている
- BERTベースのモデル
    - 一般的なドメイン知識が詰まったウェブスケールのテキストデータ上で単語の埋め込みとモデルパラメータの両方が事前に学習されている
        - コールドスタート問題をより良く軽減するためにニュースの深い意味的モデリングを強化することができる
    - pretain-finetune戦略
- ユーザの興味と候補となるニュースをいかに正確にマッチングさせるか
    - DKN [Wang et al., 2018]
    - NPA [Wu et al., 2019b]
        - 候補ニュースと以前にクリックしたニュースとの類似性
        - ユーザー表現を学習
    - LSTUR [An et al., 2019]
        - GRU
        - クリックされたニュースから短期的および長期的なユーザーの興味をモデル化
    - NAML [Wu et al., 2019a] 
    - NRMS [Wu et al., 2019c] 
        - ユーザー表現の学習
        - アテンションネットワーク
    - これらのモデル
        - 各ユーザーベクトルとアイテムベクトルを別々に取得
        - 2つのベクトルをマッチング
            - クリック確率を予測
        - 各ニュースを全体のベクトル
            - ユーザーとアイテムを別々に符号化
            - ユーザーの興味と候補となるニュースの間にある低レベルのマッチング信号（例えば、単語レベルの関係）を無視する可能性
                - 図1
                    - 1番目と3番目
                    - ニュースレベル
                        - すべて映画と関連づけ
                        - 強い意味的類似性
                    - 単語レベル
                        - "Florida"
                        - "American"
                            - "Japan "が一致しない
                            - 実際にユーザはそのニュースをクリックしない
            - BERTはユーザ-ニュースマッチング信号をニュースレベルおよび単語レベルの両方で捕らえることができる

<!-- -------------------- -->

## 関連研究
- 従来の方法
    - 人手による特徴工学を利用
        - [Son et al., 2013; Bansal et al., 2015]
        - マッチングのため
        - ニュースやユーザーを表現するため
    - CCTM [Bansal et al., 2015]
        - トピックモデリングによる記事とコメント内容
        - 協調フィルタリングによるユーザーの共同コメントパターン
        - ニュースとユーザーの表現を構築
- 深層学習手法
    - 優れたパフォーマンス
    - DKN [Wang et al., 2018]
    - NPA [Wu et al., 2019b]
        - CNN
        - 個人化された注意メカニズム
        - ニュース表現を学習
    - LSTUR [An et al., 2019]
    - NAML [Wu et al., 2019a] 
    - NRMS [Wu et al., 2019c]
        - ニュース表現に注意ネットワーク
        - ユーザーの閲覧したニュースを集約
        - ユーザー表現を学習
- マッチングベースの手法
    - FIM [Wang et al., 2020]
        - 閲覧した各ニュースのセグメントペアと各意味レベルの候補ニュースとの間できめ細かいマッチングを形成
        - 積層拡張畳み込み
- 本手法
    - 領域外の知識を導入
    - コールドスタートの問題を軽減するために、事前に学習可能なBERTモデルを導入し、自己注意を介して単語レベルとニュースレベルの両方でマッチング表現を学習し、多階層のユーザー-ニュースマッチング信号を捕捉します。

<!-- -------------------- -->

## 提案手法
- 問題の定式化
- モデルアーキテクチャの説明

### 問題定義
- ユーザu
- ニュース候補Vu
- uに対するi番目のvuのスコア

### 入力と出力
- ニュース表現
    - 単語列をニュース文に
- ユーザ表現
    - クリックした単語列
- 各ニュースの冒頭に[NSEP]トークン
- nju
    - uがクリックしたj番目のニュース

### モデルアーキテクチャ
- 

<!-- -------------------- -->

## 実験

### データセット
- MIND5 [Wu et al., 2020]
    - MSN News6のログから収集した実世界のニュース推薦データセット
    - MIND-large
    - MIND-small
        - MIND-largeから毎日の行動ログを等確率でランダムにサンプリング

### 評価指標
- AUC、MRR、nDCG@K（K = 5、10）
    - すべてのインプレッションログに対する平均値
    - MIND-largeのテストセットのラベルは提供されていない
    - テスト性能はMIND News Recommendation Competi-tion7に投稿して得られたもの

### モデルと訓練の詳細
- MIND-smallデータセットを用いてパラメータ設定を決定
- smallとlargeの両方
- bert-base-uncased
    - 単語レベルモジュールを初期化するための事前学習モデルとして
- モデルの最適化
    - 他のベースラインとの整合性と学習効率を考慮
    - ネガティブサンプリング
    - Adam [Kingma and Ba, 2014]
- バッチサイズを128
- 学習率を2e-5に設定
- 2エポックを学習
- 全てのハイパーパラメータは検証セットでチューニング

### 性能評価
- LibFM [Rendle, 2012]
    - ユーザの閲覧したニュースと候補のニュースからTF-IDF [Beel et al.16]を入力
- DeepFM [Guo et al., 2017]
    - LibFMと同じ特徴を持つ深層ファクトリゼーションマシン
- DKN [Wang et al., 2018]
    - 知識認識CNNに基づく深層ニュース推薦法
- NPA [Wu et al, 2019b]
    - 人に合わせた注意メカニズムを持つニューラルニュース推薦手法
- NAML [Wu et al., 2019a]
    - 気配り型マルチビュー学習を持つニューラルニュース推薦手法
- LSTUR [An et al, 2019]
    - GRUを用いてユーザーレプレゼンテーションを学習するニューラルニュース推薦法
- NRMS [Wu et al., 2019c], マルチヘッド自己注意を用いてユーザーおよびニュース表現を学習するニューラルニュースrec-ommendation法、 
- FIM [Wang et al., 2020],
    - 細粒度の興味マッチングのニューラルニュースrec-推薦法、
- UNBERT
    - 提案手法
- LibFMとDeepFM
    - 全てのニュースのタイトルから抽出されたTF-IDF特徴量のみ
    - 公平に比較
- 表2
    - DKN、NPA、NAML、LSTUR、NRMS
        - エンドツーエンドで単語表現を学習
        - 手動で特徴を作るニューラル推薦手法よりも優れている
            - LibFMやDeepFM
        - 手動特徴工学に基づく手法よりもニュース表現学習に適している
    - NPA、NAML、LSTUR、NRMSなど
        - 注目メカニズムを用いたニュース表現学習法
        - DKNよりも性能が優れている
        - アテンション機構が単語間の相互作用をモデル化
            - 相互作用の相対的重要度を捉える
            - より正確にニュース表現を学習できる
        - 特に、NRMSはユーザの閲覧したニュースと候補ニュースとの関連性をニュースレベルで捉えるために有用な多頭自己注意を採用
    - FIM
        - ペアワイズ・マルチレベル・マッチング・アーキテクチャ
        - ニュースレベルのマッチング情報だけでなく
        - きめ細かいマッチングシグナルを検出できる
        - 他の手法より優れている
    - UNBERT
        - 全ての指標において、他のベースライン手法を一貫して上回る
        - アウトドメインデータを活用することで豊富な言語知識を導入
        - テキスト表現を強化できる
        - 自己注意を介して単語レベルとニュースレベルの両方で、多階調のユーザー-ニュースマッチング信号の利点を検証している

### WLMとNLMのアブレーションスタディ
- 単語レベルモジュールとニュースレベルモジュールの有効性を検討
    - ある部分を無効にして別の部分を評価
    - 単語レベル信号のUNBERT（UNBERTword）
    - ニュースレベル信号のUNBERT（UNBERTnews）
    - 完全UNBERT
    - MIND-smallで実験
        - MIND-largeでは投稿制限
    - UNBERTwordとUNBERTnewsは、完全版UNBERTに近いかなり良い性能
    - 2つのレベルマッチング信号の有効性が確認
    - UNBERTnewsは、UNBERTwordを上回っている
        - ニュース構造の捕捉に関する弱点である単語レベルが不十分
    - UNBERTのフルバージョンが最も良い結果
        - 多階調のマッチング信号がニュース推薦に必要

### アグリゲータタイプのインパクト
- UNBERTのNLMにおけるアグリゲータの種類の違いが最終的なランキング性能に与える影響
- Attention Aggregatorが最も良い性能
    - Mean Aggregatorがそれに続く
    - NSEG Aggrega-torは最も悪い性能
    - であることがわかった。NSEG Aggregatorは単語のem-beddingからある程度ニュースを表現できる特別なトークンの埋め込みを用いるが、Mean Aggregatorは単語表現を明示的に平均化することによってニュース表現に集約するため、Mean Ag-gregatorの性能が良くなっていることがわかる。Attention Aggregatorはアテンション機構を適用し、さらに性能を向上させる。このことは、単語間の関連性がニュース表現に不可欠な要素であることを示している。

### コールドスタート問題の評価
- 未閲覧のニュース
- 日替わりのテストセットでUNBERTを評価
- MIND-smallデータセットを日ごとに7つのグループに
    - 最初の3日間はモデルの学習と検証
    - 残りの4日間は各日でモデルの検証
- 異なる日において常に大きなマージンで他の手法を上回る
- 時間とともに寒冷なニュースが増えていく
- 特に2019/11/13から2019/11/14
- アウトドメイン知識の優位性
- 未見のニュースの割合が増加する一方で、全体の性能が増加している
    - トークン分布がこの2日間であまり変化していないことが原因

### 
- 

### 
- 

<!-- -------------------- -->

## おわりに
- コールドスタート問題が改善

<!-- -------------------- -->

## 片岡所感
- 

<!-- -------------------- -->

## 重要ピックアップ
- 
