<!-- tex script for md -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 論文要約 『AutoClustering: A Feed-Forward Neural Network Based Clustering Algorithm』

- 2018
- 20220108参照
- [1]M. Kimura, 「AutoClustering: A Feed-Forward Neural Network Based Clustering Algorithm」, 2018 IEEE International Conference on Data Mining Workshops (ICDMW), 11月 2018, pp. 659–666. doi: 10.1109/ICDMW.2018.00102.

<!-- -------------------- -->

## 概要
- クラスタリング
    - データとクラスタラベルの写像
        - FFNNを使えそう
- 提案手法
    - FFNNのみでクラスタリング
    - 自己組織化マップや成長型ニューラル・ガスネットワークとは異なる
        - DLと互換性
    - 構成
        - レコードからクラスタへの写像（エンコーダ）
        - クラスタからその模範例への写像（デコーダ）
        - レコードと模範例の位置的近接度を測定する損失関数
    - 高速化の工夫
        - エンコーダのソフトマックス関数を連続的にマックス関数に移行
    - クラスタ数を自然に与える
        - 結果として得られるユニークな一注目ベクトルの数として
    - 損失関数のローカルミニマムの存在とクラスタとの関係について議論

<!-- -------------------- -->

## 図表
- AutoClusteringフレームワーク
- AutoClusteringのネットワーク構造
- ポイント1、2、3はクラスタ1に属し、ポイント4、5、6はクラスタ2に属し、ポイント7、8、9はクラスタ3に属す
- 青丸がターゲットデータ、赤丸がエグザンプルデータであり、表1に示すベクトルcを持つデータの分布。
- 評価用データセット
    - Irisなど
- データセットの均質性と完全性。
- ガウス混合モデル(gmm)とk-meansモデル(kmeans)を用いて得られたf値
    - 真のクラスタ番号を使用
- オートクラスタリングで得られたf値
- 親和性伝播による f 値 (減衰)

<!-- -------------------- -->

## はじめに
- 

### 
- 

### 
- 

### 
- 

<!-- -------------------- -->

## 関連研究
- 

### 
- 

### 
- 

### 
- 

<!-- -------------------- -->

## 提案手法
- 

### 
- 

### 
- 

### 
- 

<!-- -------------------- -->

## 実験
- 

### データセット
- 

### 性能指標
- homo-geneity と completeness
    - Rosenberg と Hirschberg[41] が提唱
    - 均質性(h)はクラスタ内のクラスのエントロピー
    - 完全性(c)はクラスタ内のクラスタのエントロピー

### 実装
- 

### 結果
- 代表的なセントロイドモデルとも比較
    - ガウス混合モデル
        - k-meansの拡張
        - クラスタ数が必要
            - 手動かベイズ情報量規準
    - アフィニティプロパゲーション
        - メッセージの受け渡し
        - クラスタ数は不要
        - 減衰係数、プリファレンスが必要
    - 同質性 h と完全性 c の調和平均で与えられる F 値

<!-- -------------------- -->

## 結論
- 着想
    - SOMやGNGが、深層学習ニューラルネットワークと適切に組み合わせられる構造を持っていないこと
- 今後
    - 大規模データに適用する予定
    - optimal Tとレコード数の関係の検討
    - 最適な（潜在的な）模範解答の数について議論する必要
        - 不要な模範解答があると性能が低下するため
    - より一般的な模範解答に拡張
        - 曲線など
    - 他の深層学習ネットワークとの組み合わせ
    - クラスタ機構が有用なアプリケーションの追究

### 
- 

### 
- 

### 
- 

<!-- -------------------- -->

## 片岡所感
- 出来事
    - 同じ出来事を集めたい
        - 別の出来事とは異なり具合は二の次
        - homo-geneity
- 主張
    - 違う主張を見たい
        - 同じ主張が集まっているかは二の次
        - 多様性のベンチマーク
        - completeness
- covidに限定したデータであるため、一般のデータと単純比較はできない
    - covidで別の手法を行い、多様性を評価
        - そんな時間はない
- 人間の評価は必要

<!-- -------------------- -->

## 重要ピックアップ
- homo-geneity と completeness
    - Rosenberg と Hirschberg[41] が提唱
    - 均質性(h)はクラスタ内のクラスのエントロピー
    - 完全性(c)はクラスタ内のクラスタのエントロピー
