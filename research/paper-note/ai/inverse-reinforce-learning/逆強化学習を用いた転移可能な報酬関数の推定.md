<!-- tex script for md -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 論文要約 『逆強化学習を用いた転移可能な報酬関数の推定』
- 20210426
- [1]勇樹北里と幸代荒井, 「逆強化学習を用いた転移可能な報酬関数の推定」, 人工知能学会全国大会論文集, vol. JSAI2016, p. 3D3OS30a2-3D3OS30a2, 2016, doi: 10.11517/pjsai.JSAI2016.0_3D3OS30a2.

<!-- -------------------- -->

## はじめに
- 転移学習
    - 解決が容易なタスクを難しいタスク用に転移
    - Fernandoの手法
        - 確率的なバイアス
            - 現在の方策
            - 新しい状態を探索
            - 過去の方策
    - Matthewのルールトランスファー
        - 元タスクと目標タスクが大きく異なる場合に対応
        - 元タスクをDBで修正
- 逆強化学習
    - 方策ではなく報酬を転移
    - エージェント（エキスパート）の行動軌跡と状態遷移確率から報酬関数を推定
    - Monicaの手法
        - 別目的の行動軌跡から同じエキスパートをEMアルゴリズムでクラスタリング
        - 各クラスタで報酬関数を推定
        - 新たな軌跡にベイズルールからクラスタリングと報酬関数う割り当て
    - Jaedeugの手法
        - ディリクレ過程混合モデルでノンパラメトリックベイジアンアプローチによる報酬関数の推定
            - 事前のクラスタ数の設定が不要
            - 以前の学習結果から推定
- 逆強化学習を用いた転移学習
    - 必要なエキスパートの行動軌跡によって適用範囲が限定
- 本研究
    - 迷路問題
    - Abbeelの逆強化学習による元タスクの報酬関数
    - 目標タスクの教師あり学習
    - 計算機実験

### Abbeel の逆強化学習
1. ランダムに選んだ方策 $\pi^{0}$ から特徵期待值 $\mu^{0}=\mu\left(\pi^{0}\right)$ を計算する
2. 各特徴期待値に対する重み $w^{1}=\mu_{E}-\mu^{0}$ を計算し, $i=1$ とする
3. 終了条件 $t^{i} \leq \tau$ を満たすまで以下を繰り返す：
    1. 報酬関数 $\bar{R}=w^{i} \cdot \phi$ と強化学習を用いて，最適方策 $\pi^{i}$ を求める
    2. 最適方策 $\pi^{i}$ から特徵期待值 $\mu^{(i)}=\mu\left(\pi^{i}\right)$ を計算し, $i=i+1$ とする
    3. エキスパートの特徴期待値への射影ベクトル $\bar{\mu}^{i-1}$を計算する
    4. $w^{(i)}=\mu_{E}-\bar{\mu}^{(i-1)}, t^{(i)}=\left\|\mu_{E}-\bar{\mu}^{(i-1)}\right\|_{2}$ とする

- エキスパート
    - 各状態で最適な行動をとるエージェント
- エキスパートと同様な行動軌跡を生む報酬関数をR
- 特徴期待値 $E[\sum_{t=0}^{\infty}\gamma^t\phi(s_t)|\pi]\in R^k$ ($\phi(s_t)$は状態$S \rightarrow [0,1]^k$で定義される)
- 類似手法
    - max-margin法
        - QPsolver
    - projection法
        - 正射影ベクトル
        - 僅かに良い

### 提案手法
1. 強化学習で元タスクを用意
    - スタート、ゴール、障害物の座標
    - 各迷路の最適方策を求める
    - 最適方策からエキスパートの行動軌跡を求める
2. Abbeelの逆強化学習による各状態の報酬関数の推定
3. NNによる目標タスクへの報酬の転移
    - 入力は元タスクの環境情報
    - 出力は求めた報酬関数
    - バックプロパゲーション
    - 目標タスクを入力して報酬関数を推定

### 計算機実験
- 5*5マスの10個の迷路問題
- 障害物はランダムに3-10個
    - なぜ
    - ゴールに辿り着けないケースは除外
- 逆強化学習
    - 割引率は0.9、学習率は0.03、繰り返し回数の上限は100
        - なぜ
    - エキスパートと一致したら終了
- NN
    - 学習率0.1
    - 初期重み$[-0.1,0.1]$でランダム
    - 誤差逆伝播1000回
- 右回りの最短経路のみが学習できた
    - 何をもって
    - なぜ右回り
- 100試行の学習曲線の平均値を図示
- 結果出ず
    - 訓練数不足？
    - パラメータの調整不足
    - 学習効率を改善する報酬関数ではなく、完全に一致するものを見つけている

<!-- -------------------- -->

## 所感
- 逆強化学習と転移学習を合わせるつもりだったが、逆強化学習⊂転移学習？
- 成功か失敗かを先に見ておくべきだった
