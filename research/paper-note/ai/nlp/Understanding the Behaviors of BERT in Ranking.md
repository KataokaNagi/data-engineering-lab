<!-- tex script for md -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 論文要約 『Understanding the Behaviors of BERT in Ranking』

- [1]Y. Qiao, C. Xiong, Z. LiuとZ. Liu, 「Understanding the Behaviors of BERT in Ranking」, arXiv:1904.07531 [cs], 4月 2019, 参照: 10月 11, 2021. [Online]. Available at: http://arxiv.org/abs/1904.07531

<!-- -------------------- -->

## 概要
- ランキング課題のBERTの性能および動作
- fine-tuningの複数の手法
- Transformer層でクエリ・文書トークン間の注意をどう配分するか
- パラフレーズ・トークン間の意味的マッチをどうするか
- クリック学習されたニューラル・ランカーによって学習されたソフトマッチ・パターンとどのように異なるか
    - ？

<!-- -------------------- -->

## 図表
- ランキング性能
    - LeToRと比較
- BERT（Last-Int）におけるマーカー、ストップワード、規則的な単語への注目度
    - X軸は層の深さ
    - Y軸は各グループに「More than Average」または「Majority」の注目を送るトークンの数
- MS MARCO での BERT (Last-Int) と Conv-KNRM で規則的な用語を削除した場合の影響
    - 各ポイントは、ランダムな規則的な用語が削除されたクエリとパッセージのペアに対応
    - X軸は元のランキングスコア
    - Y軸は用語削除後のスコア
- BERTおよびConv-KNRMにおけるMS MARCOの文章で最も影響力のある用語の例

<!-- -------------------- -->

## はじめに
- neural information retrieval (Neu-IR) 
    - 相互作用ベースのニューラルランカー
        - クエリとドキュメントの用語の相互作用を利用
        - ソフトマッチング
    - 表現ベースの埋め込み
    - 大容量ネットワーク
        - 大規模なランキングラベル
        - 関連性パターンを学習
    - BERT
        - 深層双方向トランスフォーマー
        - 多くのseq2seq
        - 既存のNeu-IRモデルを凌駕
- 本論文
    - BERTの性能と特性
    - ランキングにおけるBERTの使用方法
    - ベンチマーク
        - MS MARCO passage ranking task
            - 質問に対する回答文をランク付け
        - TREC Web Track ad hoc task
            - キーワードクエリに対するClueWeb文書をランク付け

<!-- -------------------- -->

## BERTに基づくランカー

### BERTの特筆すべき特性
- 大容量
    - 標準的なTransformerアーキテクチャを非常に深く使用
- 事前学習
    - Google News
    - Wikipedia のコーパス
    - 文の残りの部分を用いてランダムな欠落語（15%）を予測（Mask-LM）
    - 2つの文が隣り合っているかどうかを予測
        - SEPで分割
- 微調整
    - 冒頭のCLSトークン
        - CLS埋め込みにタスク固有の層を追加して微調整

### BERT による順位付け
- 4 つの BERT に基づくランキングモデルを実験
    - BERT（Rep）
        - CLSのコサイン類似度
    - BERT（Last-Int）
        - CLSを照合特徴として使用
            - 重み w で線形結合
            - 推奨方法
            - クエリと文書の間のすべての用語ペアの相互作用を含む
    - BERT（Mult-Int）
        - BERT（Last-Int）にすべての層からマッチング特徴 qd_{cls}^k を追加
        - 異なる層が異なる情報を提供するか研究
    - BERT（Term-Trans）
        - ニューラルランキングネットワークを追加
            - 組み合わせの性能を研究
    - BERT（Term-Trans）
        - クエリとドキュメントの間の翻訳行列を、文脈上のエンベッディングの投影間のコサイン類似度を用いて構築
        - Mean-Pooling と線形結合ですべての層からの翻訳行列を結合
    - クラス分け損失を使用
        - クエリと文書のペアが関連しているか
    - ペアワイズ・ランキング・ロスを実験した
        - 違いなし
- 問い合わせ q
- 文書 d
- 2つの qd の連結の表現

<!-- -------------------- -->

## 実験的方法論
- データセット
    - MS MARCO
        - Bingの検索ログからサンプリングした質問に似たクエリ
        - 1,010,916個の学習用クエリ
        - 100万人の専門家が注釈をつけた回答文の関連性ラベル
        - Train Triples Smallを使用して微調整
        - 先行研究と同じ設定
    - ClueWeb
        - ClueWeb09-B の文書
        - TREC Web Track ad hoc retrieval task 2009-2012 のクエリ
            - 関連性を判断した 200 のクエリ
        - 先行研究と同じ設定、処理済みデータ
        - TRECラベルだけでは微調整や他のニューラル・ランカーを訓練してSDMを凌駕するには不十分
            - MS MARCO上ですべてのニューラル手法を事前に学習
            - その後ClueWeb上で微調整することに
- 評価指標
    - MS MARCO
        - MRR@10
            - 開発セットの結果
                - BM25のトップ100を再ランク化したもの
            - 評価セットの結果
                - 主催者から入手
                - BM25の上位1000位までを再評価
    - ClueWeb
        - NDCG@20とERR@20
    - 統計的有意性
        - p < 0.05の順列検定
            - クエリごとのスコアがリーダーボードで返されないMS MARCO Evalを除く
- 比較手法
    - Base
        - リランクの候補となる文書を提供するベースとなる検索モデル
        - MS MARCOではBM25
        - ClueWebではGalago-SDM
    - LeToR
        - 特徴ベースの学習でランク付け
        - MS MARCOではRankSVM
        - ClueWebではCoordinate Ascent
    - K-NRM
        - カーネルベースのインタラクションベースのニューラルランカー
    - Conv-KNRM
        - K-NRMのn-gramバージョン
    - ClueWebにおけるK-NRMおよびConv-KNRMの結果
        - 我々の実装
        - MS MARCOで事前に学習
        - Conv-KNRM (Bing)は先行研究によりBingのクリックで事前学習
        - 残りのベースライン
            - 先行研究からの既存の結果を再利用
- 実装の詳細
    - すべての BERT ランカー
        - Adam オプティマイザー
        - 学習率 3e-6
            - Term-Trans は学習率 0.002で投影層を学習
        - バッチサイズ
            - 最大でも1
        - 収束の判定
            - 少量の検証データに対する損失（MS MARCO）
                - または検証フォールド（ClueWeb）
        - MS MARCO
            - すべてのランカーが収束するのに約5%のトレーニングトリプル

<!-- -------------------- -->

## 評価および分析

### 総合性能
- BERT ベースのランカー
    - MS MARCO は効果的
        - 以前の Conv-KNRM を 30%～50% 向上
        - Transformer からのクロスクエリ・ドキュメントの注目点のおかげ
    - BERT（Rep）
        - クエリとドキュメントに個別に BERT を適用
        - クロスシーケンスの相互作用を破棄
        - 性能はランダムに近い
    - 表現モデルとして使用することは推奨されていない
    - Multi-Int および Term-Trans
        - BERT（Last-Int）よりも性能が劣る
        - 微調整において、事前に訓練されたBERTを劇的に修正することは困難
        - エンドツーエンドの訓練は効果的にするかも
    - ClueWeb でかなり異なる動作
        - どの BERT モデルもLeToR を有意に上回らない
        - Conv-KNRM (Bing)は、ClueWeb上で最も良い性能
    - MARCO の通路ランキング
        - 質問応答に焦点を当てている
            - seq2seq タスクに近い
                - BERT の周辺コンテキストに基づく事前訓練が優れている
    - TREC のアドホックタスク
        - 文脈以外の異なる信号を必要
        - クリックに関する事前学習が効果的

### 学習された注意
- 性能の高いMS MARCO および BERT（Last-Int）
- MS MARCO Dev から 100 件のクエリを無作為に抽出
- 候補となる用語を3つに分けた
    - マーカー（「[CLS]」および「[SEP]」）
    - ストップワード
    - 通常の単語
- マーカーは最も注目される
    - 削除すると、MRR が 15% 減少
    - マーカーで2 つのテキストシーケンスを区別
- ストップワードは、ノンストップワードと同様に注目
    - しかし削除してもMRR 性能に影響はない
    - ストップワードが有用ではないと学習し、それらに冗長な注目重み
- ネットワークが深くなるにつれ
    - 他のトークンの大部分の注意を受けるトークンが少なくなる
    - 注意がシーケンス 全体に広がる
    - 埋め込みが文脈化される
    - 必ずしもよりグローバルなマッチング決定につながるわけではない

### 学習された用語のマッチング
- BERT（Last-Int）の学習されたマッチングパターンとConv-KNRMを比較
- 前回の実験と同じ MS MARCO Dev サンプルを使用
- 用語を含む文書と含まない文書のランキングスコアを比較
    - 用語の影響を調べる
- 各クエリとパッセージのペアについて、ノンストップの単語をランダムに削除
- BERT (Last-Int) または Conv-KNRM を使用してランキングスコアを計算
- それを元のランキングスコアと比較
- 図2
    - Conv-KNRMのランキングスコアがより均一に分布しているのに対し
        - BERTではほとんどのペアが1に近いか0のランキングスコア
    - 各文書には、BERT のランキング得点の大部分を決定する数個の用語
        - 削除すると、 ランク付けの得点が 1 から 0 に近づくまで大きく変化
        - BERT では大部分の用語を削除してもあまり問題にならない
            - ほとんどの得点が角に集まる
                - BERT が大規模な事前訓練からよく訓練されていることを示す
        - Conv-KNRM では、用語の寄与はより均等
            - 単一の用語を削除すると、Conv-KNRM のランキングスコアがある程度変化することが多い
        - 最も影響力のある用語を手動で調査
            - 完全に一致する用語は、BERT（Last-Int）において重要な役割
            - BERT
                - 質問またはそれに近い言い換え
            - Conv-KNRM
                - 検索におけるクエリとより緩やかに関連する用語
            - MS MARCO
                - マキアートにおけるミルクの役割（"visible mark"）、ショーとシンドバッドが演じた役（"Cosby "と "Coach Walter"）、およびパーソナルミーティング ID に関連するタスク（"schedule"
            - BERTが周囲の文脈を事前に学習することで、意味的に近いテキスト列のペアを好むことを示唆
            - このような周囲のコンテキ ストで訓練されたモデルは、キーワードクエリに対する TREC 形式のアドホックな文書ランキングでは、 それほど効果的ではない

<!-- -------------------- -->

## おわりに
- BERT に基づくランカー
    - 質問応答に焦点を当てた MS MARCO 通過ランキング課題
        - 良好な結果
        - 文脈
    - TREC のアドホック文書ランキング
        - 良好ではなかった
        - クリックが文脈よりも優れた事前訓練信号
- BERT
    - テキスト配列を効果的に照合する相互作用に基づくseq2seq モデル
    - コンテクスト全体に注意を大局的に分散
    - クエリと文書のペアに極端なマッチングスコア
    - ほとんどのペアは 1 つまたはゼロのランキングスコア
        - 事前訓練によってよく調整されている
    - 周辺の文脈について事前に学習
        - 意味的に近いテキスト対を好む
            - 文脈よりクリックを好むTREC形式のアドホック・ランキングに有効でない
                - クリック用により深いネットワークを訓練する必要
            - 性を示唆しています。将来的には、関連性ベースのラベルで訓練されたときに、BERTのように非常に深いモデルが、浅いニューラルランカーと比較してどのように機能するかを研究することは興味深いことです。

<!-- -------------------- -->

## 片岡所感
- 前処理が知りたくて読んだが、あまり記載なし
    - RoBERTaでどうかも調査が必要
- ランキングの専門外の内容が多い印象

<!-- -------------------- -->

## 重要ピックアップ
- BERT
    - 文脈に強い
    - クリックタスクに弱い
- CLSトークンはベクトル
- ストップワードの除去は不要
