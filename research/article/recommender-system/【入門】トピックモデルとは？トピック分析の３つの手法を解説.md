<!-- tex script for md -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 記事要約 『【入門】トピックモデルとは？トピック分析の３つの手法を解説』

- https://spjai.com/topic-model/
- SPJ
- 20200104
- 20210711参照

<!-- -------------------- -->

## あらすじ
- トピック分析
    - NLP, MLのひとつ
    - 文章理解に利用
    - 背後のトピックの把握

<!-- -------------------- -->

## トピックモデル
- トピック
    - Weblio
        - 話題、主題、題目
        - 陳述される中心的対象
        - 明示された場合は話題語（主題語）
        - **日本語では話題が前の文と同じである場合には省略することが多い**
            - （主題役割というときの「主題」とは直接関係はない）
    - 話の抽象度を最もあげた時の概念的なもの
        - **どの抽象度レイヤを適用するかが研究になりそう**
- トピックモデル
    - トピックの判断材料
    - 文章、画像、音楽などに利用

<!-- -------------------- -->

## 3つのトピック分析
- 行列の次元圧縮なども必要

### LSI (Latent Semantic Index; 潜在的意味解析)
- LSI
    - 自動車、車などの同義語をグルーピング
    - 文の情報量を凝縮
    - 要点を強める
- 前処理
    - 各文章の単語を文章matrixに変換
        - wheel automobile car cat dog rabbitという単語カテゴリ
        - 文章ごとに01のvec
            - **この要素をどう取るかが肝**
        - 膨大すぎる
            - 圧縮するのがLSI
    - 必要なら形態素解析
- LSIの数学
    - 特異値分解 $M = U \Sigma V^*$
        - 行列を圧縮
        - U：文章ベクトル空間の基底
        - $\Sigma$ ：特異値$\alpha$を降順に並べた対角行列
        - V：単語ベクトル空間の基底
        - vehicleなどの潜在軸を見つけたい
            - $D = UW (W=\Sigma V^T)$
            - $W = \left[ \sigma_1 \bm{v}_1, \cdots, \sigma_r \bm{v}_r \right]^T$
            - 小さく重要でない$\alpha$を削除
- 2種のLSI
    - rankの削減
    - 文章ベクトルの次元数の削減
- 効果
    - 次の対策
        - 単語カテゴリが多いとほぼ異なる文章ベクトルと認識されてしまう
- 類似文章検索等によく用いられる

### Probabilistic Latent Semantic Indexing
- LSI & 確率論
    - $P(w, d) = \Sigma_c P(c) P(d|c) P(w|c) = P(d) \Sigma_c P(c|d) P(w|c)$
        - 文書dの単語wに着目
        - P(d|c)=P(c|d)：文章dのトピックがcである確率（逆？）
        - P(w|c)：単語wの、トピックcにおける出現のしやすさ
- パラメータ推定
    - EMアルゴリズム（Expectation Maximumxation; 期待値最大法）
- 文章毎に複数のトピックをもつ可能性がある
- トピック数は事前に明確である必要あり
- トピック毎に、異なる単語の生成分布を持ち得る
    - ？？
- 制約
    - 文章dは学習時のトレーニングコーパスのもの
        - 新規の文章を自然に扱う事ができない
        - パラメータ調整で過学習になりやすい
- 応用
    - 情報検索分野
    - NLP全般

### LDA（Latent Dirichlet Allocation）
- 潜在的なトピックを推定
- 応用
    - 文章分類
    - 文章ベクトルの次元削減など
    - に用いられる技術です。
- PLSIをベイズ化したもの
- ベイズの定理
    - $P(B|A) = \frac{P(A|B) P(B)}{P(A)}$
- ベイズ推定
    - ベイズの公式を使用
    - 観測された事象から、何らかの事項を推定
- LDAの式をベイズ推定
    - 式は文章d orトピックcの確率分布を表す
        - これを生成する確率分布をベイズ推定で考える
            - **文章dに直接依存しない、トピックの確率分布を推定可能**
                - 学習データにない、新規の文章も利用可能
            - dの代わりに、dで使用される単語集合（w = w1, w2… wN）で文章を代表させる
    - トピック毎の単語の分布、文章毎のトピックの分布は、ディレクトリ分布に従うものと仮定
        - $\phi ~ p(\phi | \beta)$
    - 上記は、各トピック毎に単語分布を生成
        - $\theta ~ p(\theta | \alpha)$
    - 上記は、各文章毎にトピックの分布を生成
        - $z ~ p(z | \theta)$
    - 以上より、単語のトピックに該当する単語分布を選び、単語を生成
        - $w ~ p(w | \phi_z)$

<!-- -------------------- -->

## 片岡所感
- 研究の肝になりそうな点
    - どの抽象度レイヤをトピックとして用いるか
    - 文章ベクトルの単語カテゴリの選び方

<!-- -------------------- -->

## 重要ピックアップ
- トピック分析
    - 次元圧縮が重要
        - LSI
            - 同義語を圧縮
            - 文章行列を特異値分解
            - rankや文章ベクトルの次元を削除
                - 小さい$\alpha$を削除
        - PLSI
            - LSIと確率論
            - 期待値の最大化
            - コーパスが必要
        - LDA
            - PLSIとベイズ推定
            - 新規データに利用可能
